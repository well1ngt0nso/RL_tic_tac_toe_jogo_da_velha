{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install keras\n",
        "!pip install keras-rl2\n",
        "!pip install tensorflow==2.14.0\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten\n",
        "\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from tensorflow.keras.optimizers.legacy import Adam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sy4oesOGxaAz",
        "outputId": "c6f7d846-bb55-48a6-87a0-0a0f80696fd3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.10/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from keras-rl2) (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (24.12.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.69.0)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->keras-rl2) (3.2.2)\n",
            "Requirement already satisfied: tensorflow==2.14.0 in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (24.12.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (1.69.0)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.14.0) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFDPXlK-t0XL",
        "outputId": "a069dc40-2d36-47b6-f818-020d3dc1ce27",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0 0\n",
            "0 0 0\n",
            "0 0 1\n",
            "\n",
            "0 0 0\n",
            "0 0 0\n",
            "2 0 1\n",
            "\n",
            "0 0 0\n",
            "1 0 0\n",
            "2 0 1\n",
            "\n",
            "0 0 0\n",
            "1 0 0\n",
            "2 2 1\n",
            "\n",
            "0 0 0\n",
            "1 0 0\n",
            "2 2 1\n",
            "\n",
            "Fim do episódio 1\n",
            "0 0 0\n",
            "0 1 0\n",
            "0 0 0\n",
            "\n",
            "2 0 0\n",
            "0 1 0\n",
            "0 0 0\n",
            "\n",
            "2 0 0\n",
            "0 1 0\n",
            "1 0 0\n",
            "\n",
            "2 0 2\n",
            "0 1 0\n",
            "1 0 0\n",
            "\n",
            "2 0 2\n",
            "0 1 0\n",
            "1 0 1\n",
            "\n",
            "2 2 2\n",
            "0 1 0\n",
            "1 0 1\n",
            "\n",
            "Fim do episódio 2\n",
            "0 0 1\n",
            "0 0 0\n",
            "0 0 0\n",
            "\n",
            "0 0 1\n",
            "2 0 0\n",
            "0 0 0\n",
            "\n",
            "0 0 1\n",
            "2 0 1\n",
            "0 0 0\n",
            "\n",
            "0 0 1\n",
            "2 0 1\n",
            "0 0 0\n",
            "\n",
            "Fim do episódio 3\n",
            "1 0 0\n",
            "0 0 0\n",
            "0 0 0\n",
            "\n",
            "1 0 0\n",
            "0 2 0\n",
            "0 0 0\n",
            "\n",
            "1 0 0\n",
            "0 2 0\n",
            "0 0 0\n",
            "\n",
            "Fim do episódio 4\n",
            "0 0 0\n",
            "0 0 0\n",
            "0 0 1\n",
            "\n",
            "0 0 0\n",
            "0 0 0\n",
            "2 0 1\n",
            "\n",
            "0 0 0\n",
            "0 0 0\n",
            "2 0 1\n",
            "\n",
            "Fim do episódio 5\n",
            "0 0 0\n",
            "0 0 1\n",
            "0 0 0\n",
            "\n",
            "0 0 0\n",
            "0 0 1\n",
            "0 2 0\n",
            "\n",
            "0 0 0\n",
            "0 0 1\n",
            "1 2 0\n",
            "\n",
            "0 0 0\n",
            "2 0 1\n",
            "1 2 0\n",
            "\n",
            "0 0 0\n",
            "2 0 1\n",
            "1 2 1\n",
            "\n",
            "0 0 0\n",
            "2 2 1\n",
            "1 2 1\n",
            "\n",
            "0 1 0\n",
            "2 2 1\n",
            "1 2 1\n",
            "\n",
            "0 1 0\n",
            "2 2 1\n",
            "1 2 1\n",
            "\n",
            "Fim do episódio 6\n",
            "0 0 1\n",
            "0 0 0\n",
            "0 0 0\n",
            "\n",
            "0 0 1\n",
            "0 2 0\n",
            "0 0 0\n",
            "\n",
            "1 0 1\n",
            "0 2 0\n",
            "0 0 0\n",
            "\n",
            "1 0 1\n",
            "0 2 0\n",
            "0 0 0\n",
            "\n",
            "Fim do episódio 7\n",
            "0 0 0\n",
            "0 0 1\n",
            "0 0 0\n",
            "\n",
            "2 0 0\n",
            "0 0 1\n",
            "0 0 0\n",
            "\n",
            "2 0 0\n",
            "0 0 1\n",
            "1 0 0\n",
            "\n",
            "2 0 0\n",
            "2 0 1\n",
            "1 0 0\n",
            "\n",
            "2 1 0\n",
            "2 0 1\n",
            "1 0 0\n",
            "\n",
            "2 1 0\n",
            "2 0 1\n",
            "1 2 0\n",
            "\n",
            "2 1 0\n",
            "2 0 1\n",
            "1 2 1\n",
            "\n",
            "2 1 0\n",
            "2 0 1\n",
            "1 2 1\n",
            "\n",
            "Fim do episódio 8\n",
            "0 0 1\n",
            "0 0 0\n",
            "0 0 0\n",
            "\n",
            "0 0 1\n",
            "0 0 0\n",
            "0 2 0\n",
            "\n",
            "0 0 1\n",
            "0 0 0\n",
            "0 2 0\n",
            "\n",
            "Fim do episódio 9\n",
            "0 0 0\n",
            "0 0 0\n",
            "1 0 0\n",
            "\n",
            "0 0 2\n",
            "0 0 0\n",
            "1 0 0\n",
            "\n",
            "0 1 2\n",
            "0 0 0\n",
            "1 0 0\n",
            "\n",
            "2 1 2\n",
            "0 0 0\n",
            "1 0 0\n",
            "\n",
            "2 1 2\n",
            "0 0 0\n",
            "1 0 0\n",
            "\n",
            "Fim do episódio 10\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from gym import Env, spaces\n",
        "\n",
        "class TicTacToeEnv(Env):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.action_space = spaces.Discrete(9)  # 9 células do tabuleiro\n",
        "        self.observation_space = spaces.Box(0, 2, (9,), dtype=int)  # Tabuleiro 3x3 achatado com algumas configurações legais, recomendo a documentação\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = np.zeros(9, dtype=int)  # Tabuleiro 1D (array)\n",
        "        self.done = False\n",
        "        self.current_player = 1\n",
        "        return self.board\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.done or self.board[action] != 0:\n",
        "            return self.board, -10, True, {}  # Penalidade por jogada inválida\n",
        "\n",
        "        self.board[action] = self.current_player # vai na posição escolhina e coloca o número\n",
        "        if self.check_winner(self.current_player):\n",
        "            return self.board, 10, True, {}  # Vitória\n",
        "        if not (self.board == 0).any():\n",
        "            return self.board, 1, True, {}  # Empate\n",
        "\n",
        "        # Alternando o jogador\n",
        "        self.current_player = 3 - self.current_player #alterna entre 1 e 2\n",
        "        return self.board, 0, False, {}\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        board = self.board.reshape(3, 3)\n",
        "        print(\"\\n\".join([\" \".join(map(str, row)) for row in board]))\n",
        "        print()\n",
        "\n",
        "    def check_winner(self, player):\n",
        "        board = self.board.reshape(3, 3)\n",
        "        return any(\n",
        "            all(board[row, :] == player) or\n",
        "            all(board[:, col] == player) or\n",
        "            all(np.diag(board) == player) or\n",
        "            all(np.diag(np.fliplr(board)) == player)\n",
        "            for row in range(3) for col in range(3)\n",
        "        )\n",
        "\n",
        "# Criar o ambiente\n",
        "env = TicTacToeEnv()\n",
        "\n",
        "# Testando o agente em 10 episódios\n",
        "for episode in range(10):\n",
        "    state = env.reset()  # Reiniciar o ambiente a cada episódio\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = env.action_space.sample()  # Ação aleatória para o jogador\n",
        "        next_state, reward, done, info = env.step(action)  # Passo no ambiente\n",
        "        env.render()  # Mostrar estado\n",
        "        if done:\n",
        "            print(f\"Fim do episódio {episode + 1}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Construção do modelo\n",
        "def build_model(input_shape, action_space):\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(1,) + input_shape),  # Entrada achatada, para o ESP, necessário corrigir isso\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(action_space, activation='linear')  # Saída Q(s, a)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = build_model(env.observation_space.shape, env.action_space.n)\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZu_ZbHwuPH2",
        "outputId": "45fd4238-a727-4b43-9fad-23292def979c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 9)                 0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                640       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 9)                 585       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5385 (21.04 KB)\n",
            "Trainable params: 5385 (21.04 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar memória e política\n",
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "policy = EpsGreedyQPolicy()\n",
        "\n",
        "# Criar o agente DQN\n",
        "dqn = DQNAgent(model=model, nb_actions=env.action_space.n, memory=memory,\n",
        "               nb_steps_warmup=10, target_model_update=1e-2, policy=policy)\n",
        "dqn.compile(optimizer=Adam(), metrics=['mae'])\n"
      ],
      "metadata": {
        "id": "cEfrK7vb0XEZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinar o agente\n",
        "dqn.fit(env, nb_steps=150000, visualize=False, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaE0ok-jt2Xp",
        "outputId": "780cf57b-fb9f-451e-b049-49b01c4848a3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for 150000 steps ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      5/150000: episode: 1, duration: 0.278s, episode steps:   5, steps per second:  18, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.800 [0.000, 8.000],  loss: --, mae: --, mean_q: --\n",
            "     10/150000: episode: 2, duration: 0.009s, episode steps:   5, steps per second: 561, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.800 [0.000, 8.000],  loss: --, mae: --, mean_q: --\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 12 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 13 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 14 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 15 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 16 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 17 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 18 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 19 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 20 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 21 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 22 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 23 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 24 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 25 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 26 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 27 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 28 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     14/150000: episode: 3, duration: 1.283s, episode steps:   4, steps per second:   3, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.000 [0.000, 8.000],  loss: 9.314233, mae: 0.333115, mean_q: 0.352299\n",
            "     17/150000: episode: 4, duration: 0.073s, episode steps:   3, steps per second:  41, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 5.333 [4.000, 8.000],  loss: 10.518156, mae: 0.334749, mean_q: 0.280068\n",
            "     20/150000: episode: 5, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 5.333 [4.000, 8.000],  loss: 8.910376, mae: 0.301563, mean_q: 0.239767\n",
            "     25/150000: episode: 6, duration: 0.078s, episode steps:   5, steps per second:  64, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.800 [0.000, 8.000],  loss: 13.008311, mae: 0.392807, mean_q: 0.221060\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 29 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 30 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.10/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 31 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mA saída de streaming foi truncada nas últimas 5000 linhas.\u001b[0m\n",
            " 115581/150000: episode: 16847, duration: 0.047s, episode steps:   5, steps per second: 106, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.800 [0.000, 5.000],  loss: 0.218422, mae: 13.676763, mean_q: 20.203373\n",
            " 115587/150000: episode: 16848, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.315888, mae: 13.819595, mean_q: 20.111082\n",
            " 115594/150000: episode: 16849, duration: 0.063s, episode steps:   7, steps per second: 112, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.286 [0.000, 7.000],  loss: 0.316160, mae: 14.097364, mean_q: 20.033827\n",
            " 115598/150000: episode: 16850, duration: 0.044s, episode steps:   4, steps per second:  91, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.250 [0.000, 7.000],  loss: 0.298836, mae: 13.731829, mean_q: 20.055283\n",
            " 115605/150000: episode: 16851, duration: 0.088s, episode steps:   7, steps per second:  80, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.411553, mae: 13.828464, mean_q: 20.169062\n",
            " 115608/150000: episode: 16852, duration: 0.033s, episode steps:   3, steps per second:  91, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 3.667 [3.000, 5.000],  loss: 0.484828, mae: 13.349473, mean_q: 19.633821\n",
            " 115611/150000: episode: 16853, duration: 0.031s, episode steps:   3, steps per second:  96, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 5.667 [3.000, 7.000],  loss: 0.360639, mae: 13.999026, mean_q: 19.914570\n",
            " 115618/150000: episode: 16854, duration: 0.073s, episode steps:   7, steps per second:  96, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.376606, mae: 13.619430, mean_q: 20.294033\n",
            " 115627/150000: episode: 16855, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.425885, mae: 13.988037, mean_q: 20.070282\n",
            " 115634/150000: episode: 16856, duration: 0.069s, episode steps:   7, steps per second: 102, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.357597, mae: 13.894753, mean_q: 20.122150\n",
            " 115637/150000: episode: 16857, duration: 0.036s, episode steps:   3, steps per second:  84, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 5.333 [5.000, 6.000],  loss: 0.266536, mae: 14.208221, mean_q: 20.276550\n",
            " 115643/150000: episode: 16858, duration: 0.074s, episode steps:   6, steps per second:  81, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 8.000],  loss: 0.256808, mae: 14.010765, mean_q: 20.058107\n",
            " 115649/150000: episode: 16859, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [0.000, 8.000],  loss: 0.463790, mae: 13.599445, mean_q: 20.167202\n",
            " 115651/150000: episode: 16860, duration: 0.025s, episode steps:   2, steps per second:  79, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 0.000 [0.000, 0.000],  loss: 0.264820, mae: 13.773491, mean_q: 20.155222\n",
            " 115659/150000: episode: 16861, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.465522, mae: 13.706809, mean_q: 20.238922\n",
            " 115668/150000: episode: 16862, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.654031, mae: 13.936046, mean_q: 20.075970\n",
            " 115676/150000: episode: 16863, duration: 0.077s, episode steps:   8, steps per second: 105, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.676043, mae: 14.083000, mean_q: 19.964102\n",
            " 115684/150000: episode: 16864, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.708099, mae: 14.288835, mean_q: 20.165689\n",
            " 115692/150000: episode: 16865, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.728807, mae: 13.690588, mean_q: 19.895632\n",
            " 115699/150000: episode: 16866, duration: 0.073s, episode steps:   7, steps per second:  95, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.269160, mae: 13.994020, mean_q: 20.302835\n",
            " 115703/150000: episode: 16867, duration: 0.042s, episode steps:   4, steps per second:  96, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 6.000 [4.000, 7.000],  loss: 0.379038, mae: 14.190306, mean_q: 19.960789\n",
            " 115711/150000: episode: 16868, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.395195, mae: 13.867369, mean_q: 20.218586\n",
            " 115719/150000: episode: 16869, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.408339, mae: 13.663268, mean_q: 20.021946\n",
            " 115727/150000: episode: 16870, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.471534, mae: 13.622445, mean_q: 20.108885\n",
            " 115736/150000: episode: 16871, duration: 0.082s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.554061, mae: 13.660071, mean_q: 19.963678\n",
            " 115744/150000: episode: 16872, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.298746, mae: 13.933280, mean_q: 20.122879\n",
            " 115753/150000: episode: 16873, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.312674, mae: 13.849941, mean_q: 20.058470\n",
            " 115758/150000: episode: 16874, duration: 0.053s, episode steps:   5, steps per second:  95, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.200 [0.000, 5.000],  loss: 0.329192, mae: 13.711992, mean_q: 20.190268\n",
            " 115765/150000: episode: 16875, duration: 0.061s, episode steps:   7, steps per second: 114, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 5.000 [0.000, 8.000],  loss: 0.289616, mae: 13.817977, mean_q: 20.080351\n",
            " 115773/150000: episode: 16876, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.298723, mae: 13.738850, mean_q: 20.066309\n",
            " 115780/150000: episode: 16877, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 7.000],  loss: 0.481899, mae: 13.887420, mean_q: 20.051184\n",
            " 115787/150000: episode: 16878, duration: 0.068s, episode steps:   7, steps per second: 104, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 5.286 [2.000, 8.000],  loss: 0.381284, mae: 13.745645, mean_q: 20.047607\n",
            " 115794/150000: episode: 16879, duration: 0.065s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.418852, mae: 13.801887, mean_q: 20.076193\n",
            " 115800/150000: episode: 16880, duration: 0.070s, episode steps:   6, steps per second:  86, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.487418, mae: 14.049858, mean_q: 19.938181\n",
            " 115809/150000: episode: 16881, duration: 0.094s, episode steps:   9, steps per second:  95, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.377659, mae: 13.851678, mean_q: 20.133835\n",
            " 115812/150000: episode: 16882, duration: 0.032s, episode steps:   3, steps per second:  94, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 6.000 [5.000, 8.000],  loss: 0.631535, mae: 13.849500, mean_q: 19.959421\n",
            " 115818/150000: episode: 16883, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.365784, mae: 13.731872, mean_q: 20.020901\n",
            " 115825/150000: episode: 16884, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.000 [0.000, 8.000],  loss: 0.595373, mae: 13.737008, mean_q: 20.232145\n",
            " 115833/150000: episode: 16885, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.406181, mae: 13.942942, mean_q: 20.022724\n",
            " 115839/150000: episode: 16886, duration: 0.059s, episode steps:   6, steps per second: 101, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [1.000, 7.000],  loss: 0.308813, mae: 13.702770, mean_q: 20.009386\n",
            " 115846/150000: episode: 16887, duration: 0.078s, episode steps:   7, steps per second:  90, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.494167, mae: 14.071225, mean_q: 20.135244\n",
            " 115854/150000: episode: 16888, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.539164, mae: 13.733110, mean_q: 19.914490\n",
            " 115858/150000: episode: 16889, duration: 0.039s, episode steps:   4, steps per second: 103, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.250 [1.000, 5.000],  loss: 0.342856, mae: 13.938890, mean_q: 19.920258\n",
            " 115867/150000: episode: 16890, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.586909, mae: 13.837288, mean_q: 20.003344\n",
            " 115872/150000: episode: 16891, duration: 0.056s, episode steps:   5, steps per second:  90, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.600 [1.000, 8.000],  loss: 0.293082, mae: 13.470743, mean_q: 20.010448\n",
            " 115880/150000: episode: 16892, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.384210, mae: 13.782129, mean_q: 20.307522\n",
            " 115888/150000: episode: 16893, duration: 0.072s, episode steps:   8, steps per second: 112, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.367838, mae: 13.824684, mean_q: 20.017227\n",
            " 115895/150000: episode: 16894, duration: 0.074s, episode steps:   7, steps per second:  94, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.143 [0.000, 8.000],  loss: 0.270527, mae: 14.196806, mean_q: 20.281799\n",
            " 115900/150000: episode: 16895, duration: 0.048s, episode steps:   5, steps per second: 105, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.800 [0.000, 8.000],  loss: 0.429015, mae: 13.833033, mean_q: 20.037951\n",
            " 115908/150000: episode: 16896, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.125 [0.000, 8.000],  loss: 0.349451, mae: 14.263338, mean_q: 19.977184\n",
            " 115916/150000: episode: 16897, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.534707, mae: 14.007523, mean_q: 20.212576\n",
            " 115919/150000: episode: 16898, duration: 0.041s, episode steps:   3, steps per second:  74, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 4.667 [4.000, 5.000],  loss: 0.662267, mae: 13.596096, mean_q: 19.933676\n",
            " 115928/150000: episode: 16899, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.393149, mae: 14.027096, mean_q: 20.078815\n",
            " 115935/150000: episode: 16900, duration: 0.062s, episode steps:   7, steps per second: 113, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.401115, mae: 13.596601, mean_q: 20.043386\n",
            " 115944/150000: episode: 16901, duration: 0.096s, episode steps:   9, steps per second:  93, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.222 [0.000, 8.000],  loss: 0.320655, mae: 13.819387, mean_q: 20.027079\n",
            " 115949/150000: episode: 16902, duration: 0.048s, episode steps:   5, steps per second: 103, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.172252, mae: 13.973272, mean_q: 20.199467\n",
            " 115957/150000: episode: 16903, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.252090, mae: 13.791755, mean_q: 20.115013\n",
            " 115965/150000: episode: 16904, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.389629, mae: 13.931736, mean_q: 19.914768\n",
            " 115971/150000: episode: 16905, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 7.000],  loss: 0.457670, mae: 14.107562, mean_q: 20.094683\n",
            " 115978/150000: episode: 16906, duration: 0.067s, episode steps:   7, steps per second: 104, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.377255, mae: 14.117507, mean_q: 20.066393\n",
            " 115984/150000: episode: 16907, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.500 [0.000, 8.000],  loss: 0.852903, mae: 13.565089, mean_q: 20.132601\n",
            " 115992/150000: episode: 16908, duration: 0.117s, episode steps:   8, steps per second:  68, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.474874, mae: 13.978293, mean_q: 20.001595\n",
            " 116000/150000: episode: 16909, duration: 0.119s, episode steps:   8, steps per second:  67, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.250 [0.000, 8.000],  loss: 0.344724, mae: 14.078856, mean_q: 20.210882\n",
            " 116005/150000: episode: 16910, duration: 0.073s, episode steps:   5, steps per second:  68, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.400 [0.000, 8.000],  loss: 0.502979, mae: 13.726746, mean_q: 20.133144\n",
            " 116013/150000: episode: 16911, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.910628, mae: 13.844741, mean_q: 20.031723\n",
            " 116019/150000: episode: 16912, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.390287, mae: 13.617841, mean_q: 20.267864\n",
            " 116028/150000: episode: 16913, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.570786, mae: 13.757137, mean_q: 20.099295\n",
            " 116037/150000: episode: 16914, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.573257, mae: 13.715557, mean_q: 20.021271\n",
            " 116046/150000: episode: 16915, duration: 0.130s, episode steps:   9, steps per second:  69, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.520892, mae: 13.831839, mean_q: 20.056011\n",
            " 116053/150000: episode: 16916, duration: 0.088s, episode steps:   7, steps per second:  79, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.353291, mae: 13.693251, mean_q: 20.057919\n",
            " 116061/150000: episode: 16917, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.350097, mae: 13.531666, mean_q: 20.324585\n",
            " 116067/150000: episode: 16918, duration: 0.085s, episode steps:   6, steps per second:  71, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [0.000, 8.000],  loss: 0.246522, mae: 14.021194, mean_q: 20.196886\n",
            " 116073/150000: episode: 16919, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2.833 [0.000, 6.000],  loss: 0.226639, mae: 13.796290, mean_q: 20.256060\n",
            " 116078/150000: episode: 16920, duration: 0.064s, episode steps:   5, steps per second:  78, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.400 [2.000, 8.000],  loss: 0.296756, mae: 13.721415, mean_q: 20.101912\n",
            " 116085/150000: episode: 16921, duration: 0.111s, episode steps:   7, steps per second:  63, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.857 [1.000, 8.000],  loss: 0.305845, mae: 13.585999, mean_q: 20.016928\n",
            " 116093/150000: episode: 16922, duration: 0.117s, episode steps:   8, steps per second:  69, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.276182, mae: 13.783489, mean_q: 20.193239\n",
            " 116101/150000: episode: 16923, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.440002, mae: 13.504556, mean_q: 20.028463\n",
            " 116107/150000: episode: 16924, duration: 0.085s, episode steps:   6, steps per second:  71, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [1.000, 7.000],  loss: 0.296601, mae: 13.643506, mean_q: 19.919390\n",
            " 116116/150000: episode: 16925, duration: 0.131s, episode steps:   9, steps per second:  69, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.310932, mae: 13.935187, mean_q: 20.020054\n",
            " 116123/150000: episode: 16926, duration: 0.100s, episode steps:   7, steps per second:  70, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.340230, mae: 13.815033, mean_q: 20.220251\n",
            " 116131/150000: episode: 16927, duration: 0.110s, episode steps:   8, steps per second:  72, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.265480, mae: 13.907097, mean_q: 20.134983\n",
            " 116139/150000: episode: 16928, duration: 0.121s, episode steps:   8, steps per second:  66, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.524179, mae: 13.445836, mean_q: 20.215652\n",
            " 116146/150000: episode: 16929, duration: 0.122s, episode steps:   7, steps per second:  57, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.714 [1.000, 8.000],  loss: 0.387723, mae: 13.628630, mean_q: 20.160791\n",
            " 116155/150000: episode: 16930, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.593160, mae: 13.783731, mean_q: 20.069075\n",
            " 116162/150000: episode: 16931, duration: 0.106s, episode steps:   7, steps per second:  66, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.498245, mae: 13.994562, mean_q: 20.180897\n",
            " 116171/150000: episode: 16932, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.534680, mae: 13.734365, mean_q: 20.156340\n",
            " 116179/150000: episode: 16933, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.425490, mae: 13.640809, mean_q: 19.993380\n",
            " 116187/150000: episode: 16934, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.125 [0.000, 8.000],  loss: 0.392125, mae: 13.999269, mean_q: 20.045042\n",
            " 116189/150000: episode: 16935, duration: 0.037s, episode steps:   2, steps per second:  55, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.256802, mae: 14.041245, mean_q: 20.173885\n",
            " 116196/150000: episode: 16936, duration: 0.100s, episode steps:   7, steps per second:  70, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.286 [0.000, 6.000],  loss: 0.447353, mae: 13.658484, mean_q: 19.996229\n",
            " 116204/150000: episode: 16937, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.342714, mae: 13.923622, mean_q: 20.038395\n",
            " 116211/150000: episode: 16938, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.277497, mae: 13.993188, mean_q: 20.056623\n",
            " 116218/150000: episode: 16939, duration: 0.110s, episode steps:   7, steps per second:  63, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.318871, mae: 13.962535, mean_q: 20.238983\n",
            " 116225/150000: episode: 16940, duration: 0.102s, episode steps:   7, steps per second:  69, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.571 [2.000, 8.000],  loss: 0.213364, mae: 13.875367, mean_q: 20.062012\n",
            " 116234/150000: episode: 16941, duration: 0.128s, episode steps:   9, steps per second:  71, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.229461, mae: 13.843173, mean_q: 20.100023\n",
            " 116242/150000: episode: 16942, duration: 0.149s, episode steps:   8, steps per second:  54, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 5.000 [2.000, 8.000],  loss: 0.643207, mae: 13.929413, mean_q: 19.941429\n",
            " 116249/150000: episode: 16943, duration: 0.079s, episode steps:   7, steps per second:  89, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 2.714 [0.000, 8.000],  loss: 0.302017, mae: 13.639954, mean_q: 20.015879\n",
            " 116255/150000: episode: 16944, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.432195, mae: 13.835270, mean_q: 20.338270\n",
            " 116258/150000: episode: 16945, duration: 0.032s, episode steps:   3, steps per second:  94, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 4.667 [3.000, 8.000],  loss: 0.231485, mae: 13.731341, mean_q: 19.928587\n",
            " 116266/150000: episode: 16946, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.354475, mae: 13.857806, mean_q: 19.981710\n",
            " 116273/150000: episode: 16947, duration: 0.066s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.338383, mae: 14.125540, mean_q: 20.040762\n",
            " 116280/150000: episode: 16948, duration: 0.066s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.207304, mae: 14.067260, mean_q: 20.067318\n",
            " 116287/150000: episode: 16949, duration: 0.065s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.429 [0.000, 7.000],  loss: 0.184789, mae: 13.880565, mean_q: 20.167833\n",
            " 116295/150000: episode: 16950, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.262043, mae: 13.999300, mean_q: 20.008373\n",
            " 116298/150000: episode: 16951, duration: 0.033s, episode steps:   3, steps per second:  91, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 5.667 [5.000, 6.000],  loss: 0.295767, mae: 14.107442, mean_q: 20.462858\n",
            " 116306/150000: episode: 16952, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.352080, mae: 14.005259, mean_q: 20.250217\n",
            " 116314/150000: episode: 16953, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.875 [0.000, 8.000],  loss: 0.270891, mae: 13.961277, mean_q: 20.252380\n",
            " 116323/150000: episode: 16954, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.596238, mae: 13.710120, mean_q: 19.994944\n",
            " 116329/150000: episode: 16955, duration: 0.062s, episode steps:   6, steps per second:  97, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.706836, mae: 13.866389, mean_q: 20.005228\n",
            " 116334/150000: episode: 16956, duration: 0.048s, episode steps:   5, steps per second: 103, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.600 [2.000, 8.000],  loss: 0.308587, mae: 14.281436, mean_q: 20.175457\n",
            " 116343/150000: episode: 16957, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.464689, mae: 13.839159, mean_q: 20.150740\n",
            " 116349/150000: episode: 16958, duration: 0.066s, episode steps:   6, steps per second:  91, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.251368, mae: 13.739152, mean_q: 19.953333\n",
            " 116355/150000: episode: 16959, duration: 0.057s, episode steps:   6, steps per second: 106, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 7.000],  loss: 0.249465, mae: 14.154861, mean_q: 20.332081\n",
            " 116364/150000: episode: 16960, duration: 0.078s, episode steps:   9, steps per second: 116, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 1.159252, mae: 13.570374, mean_q: 20.004110\n",
            " 116367/150000: episode: 16961, duration: 0.035s, episode steps:   3, steps per second:  87, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 6.000 [2.000, 8.000],  loss: 0.596700, mae: 13.881905, mean_q: 19.813158\n",
            " 116375/150000: episode: 16962, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.337473, mae: 14.217979, mean_q: 20.352043\n",
            " 116384/150000: episode: 16963, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.111 [0.000, 8.000],  loss: 0.332028, mae: 13.719374, mean_q: 20.175322\n",
            " 116390/150000: episode: 16964, duration: 0.062s, episode steps:   6, steps per second:  97, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.333 [0.000, 8.000],  loss: 0.812315, mae: 13.784488, mean_q: 20.083590\n",
            " 116397/150000: episode: 16965, duration: 0.077s, episode steps:   7, steps per second:  91, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [1.000, 8.000],  loss: 0.323029, mae: 13.807539, mean_q: 20.134762\n",
            " 116401/150000: episode: 16966, duration: 0.046s, episode steps:   4, steps per second:  87, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.500 [1.000, 7.000],  loss: 0.270758, mae: 13.776096, mean_q: 20.454823\n",
            " 116409/150000: episode: 16967, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.125 [0.000, 7.000],  loss: 0.303989, mae: 13.810453, mean_q: 20.040607\n",
            " 116417/150000: episode: 16968, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.329197, mae: 13.766244, mean_q: 20.220440\n",
            " 116422/150000: episode: 16969, duration: 0.062s, episode steps:   5, steps per second:  81, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.800 [3.000, 7.000],  loss: 0.204025, mae: 13.886095, mean_q: 19.975067\n",
            " 116427/150000: episode: 16970, duration: 0.053s, episode steps:   5, steps per second:  94, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.800 [1.000, 6.000],  loss: 0.605326, mae: 13.810346, mean_q: 20.116001\n",
            " 116435/150000: episode: 16971, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.555621, mae: 13.898329, mean_q: 19.844643\n",
            " 116442/150000: episode: 16972, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.571 [0.000, 8.000],  loss: 0.261036, mae: 13.754728, mean_q: 20.307957\n",
            " 116450/150000: episode: 16973, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.815745, mae: 13.931284, mean_q: 20.116793\n",
            " 116457/150000: episode: 16974, duration: 0.076s, episode steps:   7, steps per second:  92, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.380032, mae: 14.122238, mean_q: 20.230671\n",
            " 116465/150000: episode: 16975, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.336833, mae: 14.124535, mean_q: 20.257957\n",
            " 116474/150000: episode: 16976, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.331566, mae: 13.908632, mean_q: 19.932060\n",
            " 116480/150000: episode: 16977, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.167 [0.000, 8.000],  loss: 0.301557, mae: 13.818150, mean_q: 20.276541\n",
            " 116487/150000: episode: 16978, duration: 0.070s, episode steps:   7, steps per second:  99, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.368138, mae: 13.856069, mean_q: 20.006460\n",
            " 116493/150000: episode: 16979, duration: 0.065s, episode steps:   6, steps per second:  93, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.537901, mae: 13.885970, mean_q: 19.968487\n",
            " 116501/150000: episode: 16980, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.394411, mae: 13.810495, mean_q: 20.215252\n",
            " 116507/150000: episode: 16981, duration: 0.066s, episode steps:   6, steps per second:  92, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.463318, mae: 14.147580, mean_q: 19.889006\n",
            " 116516/150000: episode: 16982, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.269243, mae: 13.877666, mean_q: 20.231852\n",
            " 116522/150000: episode: 16983, duration: 0.067s, episode steps:   6, steps per second:  90, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.434965, mae: 14.024750, mean_q: 19.985344\n",
            " 116527/150000: episode: 16984, duration: 0.058s, episode steps:   5, steps per second:  86, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.200 [2.000, 7.000],  loss: 0.689541, mae: 13.711046, mean_q: 19.788309\n",
            " 116533/150000: episode: 16985, duration: 0.068s, episode steps:   6, steps per second:  89, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.328672, mae: 13.958969, mean_q: 20.430349\n",
            " 116541/150000: episode: 16986, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.363483, mae: 13.882927, mean_q: 20.075325\n",
            " 116545/150000: episode: 16987, duration: 0.043s, episode steps:   4, steps per second:  94, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.250 [4.000, 6.000],  loss: 0.302625, mae: 13.967546, mean_q: 20.234283\n",
            " 116552/150000: episode: 16988, duration: 0.077s, episode steps:   7, steps per second:  91, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 7.000],  loss: 0.218140, mae: 13.794389, mean_q: 20.191729\n",
            " 116561/150000: episode: 16989, duration: 0.082s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.283054, mae: 13.889019, mean_q: 20.062889\n",
            " 116567/150000: episode: 16990, duration: 0.056s, episode steps:   6, steps per second: 108, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.375602, mae: 13.766205, mean_q: 20.241739\n",
            " 116574/150000: episode: 16991, duration: 0.070s, episode steps:   7, steps per second: 100, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.291869, mae: 13.876528, mean_q: 20.188076\n",
            " 116580/150000: episode: 16992, duration: 0.064s, episode steps:   6, steps per second:  93, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 7.000],  loss: 0.475960, mae: 13.948750, mean_q: 20.097586\n",
            " 116585/150000: episode: 16993, duration: 0.047s, episode steps:   5, steps per second: 107, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.317479, mae: 13.945070, mean_q: 19.977308\n",
            " 116593/150000: episode: 16994, duration: 0.071s, episode steps:   8, steps per second: 112, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.496341, mae: 14.125216, mean_q: 20.120758\n",
            " 116602/150000: episode: 16995, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.507116, mae: 13.635470, mean_q: 19.957924\n",
            " 116607/150000: episode: 16996, duration: 0.062s, episode steps:   5, steps per second:  81, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.362256, mae: 13.621313, mean_q: 20.107098\n",
            " 116612/150000: episode: 16997, duration: 0.049s, episode steps:   5, steps per second: 101, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.400 [1.000, 5.000],  loss: 0.228885, mae: 13.939456, mean_q: 20.152908\n",
            " 116621/150000: episode: 16998, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.222 [0.000, 8.000],  loss: 0.713824, mae: 13.874706, mean_q: 20.079115\n",
            " 116629/150000: episode: 16999, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.339605, mae: 13.905962, mean_q: 20.075909\n",
            " 116638/150000: episode: 17000, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.433358, mae: 13.838243, mean_q: 19.915718\n",
            " 116647/150000: episode: 17001, duration: 0.078s, episode steps:   9, steps per second: 115, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.520688, mae: 14.237462, mean_q: 20.317595\n",
            " 116654/150000: episode: 17002, duration: 0.074s, episode steps:   7, steps per second:  95, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.857 [0.000, 8.000],  loss: 0.399476, mae: 13.688993, mean_q: 20.005108\n",
            " 116661/150000: episode: 17003, duration: 0.067s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.218681, mae: 14.110864, mean_q: 20.188124\n",
            " 116669/150000: episode: 17004, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.238625, mae: 13.752081, mean_q: 20.133261\n",
            " 116675/150000: episode: 17005, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.000 [2.000, 7.000],  loss: 0.379369, mae: 14.288154, mean_q: 20.018061\n",
            " 116683/150000: episode: 17006, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.258764, mae: 13.694239, mean_q: 20.104279\n",
            " 116690/150000: episode: 17007, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.000 [0.000, 7.000],  loss: 0.239233, mae: 14.092727, mean_q: 20.236151\n",
            " 116694/150000: episode: 17008, duration: 0.043s, episode steps:   4, steps per second:  94, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.000 [1.000, 7.000],  loss: 0.686714, mae: 14.354095, mean_q: 19.883896\n",
            " 116700/150000: episode: 17009, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 7.000],  loss: 0.365466, mae: 13.791202, mean_q: 19.909691\n",
            " 116709/150000: episode: 17010, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.178992, mae: 14.200111, mean_q: 20.301338\n",
            " 116713/150000: episode: 17011, duration: 0.041s, episode steps:   4, steps per second:  98, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.641212, mae: 13.917383, mean_q: 19.776587\n",
            " 116722/150000: episode: 17012, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.306963, mae: 14.116286, mean_q: 19.975073\n",
            " 116731/150000: episode: 17013, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.497887, mae: 14.068751, mean_q: 20.203238\n",
            " 116736/150000: episode: 17014, duration: 0.060s, episode steps:   5, steps per second:  83, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.200 [0.000, 5.000],  loss: 0.654596, mae: 13.950360, mean_q: 19.923513\n",
            " 116743/150000: episode: 17015, duration: 0.068s, episode steps:   7, steps per second: 102, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.000 [0.000, 7.000],  loss: 0.701349, mae: 13.799083, mean_q: 20.128290\n",
            " 116748/150000: episode: 17016, duration: 0.048s, episode steps:   5, steps per second: 105, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.400 [2.000, 8.000],  loss: 0.436483, mae: 14.068972, mean_q: 20.322968\n",
            " 116754/150000: episode: 17017, duration: 0.071s, episode steps:   6, steps per second:  84, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.428100, mae: 13.760234, mean_q: 20.067459\n",
            " 116762/150000: episode: 17018, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.370235, mae: 13.918549, mean_q: 20.350685\n",
            " 116770/150000: episode: 17019, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.125 [0.000, 8.000],  loss: 0.201226, mae: 13.995064, mean_q: 20.239853\n",
            " 116779/150000: episode: 17020, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.356123, mae: 13.760594, mean_q: 20.077892\n",
            " 116785/150000: episode: 17021, duration: 0.059s, episode steps:   6, steps per second: 101, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.373131, mae: 14.323619, mean_q: 20.259819\n",
            " 116793/150000: episode: 17022, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.486796, mae: 13.747554, mean_q: 19.898712\n",
            " 116802/150000: episode: 17023, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.515538, mae: 14.153373, mean_q: 20.125740\n",
            " 116807/150000: episode: 17024, duration: 0.053s, episode steps:   5, steps per second:  95, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.501101, mae: 13.883380, mean_q: 19.884373\n",
            " 116812/150000: episode: 17025, duration: 0.063s, episode steps:   5, steps per second:  79, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.200 [0.000, 8.000],  loss: 0.335078, mae: 13.873614, mean_q: 19.992674\n",
            " 116817/150000: episode: 17026, duration: 0.053s, episode steps:   5, steps per second:  94, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 8.000],  loss: 0.348193, mae: 14.168149, mean_q: 20.321121\n",
            " 116825/150000: episode: 17027, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.000 [0.000, 8.000],  loss: 0.605748, mae: 13.859888, mean_q: 20.045897\n",
            " 116834/150000: episode: 17028, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.287580, mae: 14.187012, mean_q: 20.162252\n",
            " 116843/150000: episode: 17029, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.394475, mae: 13.873414, mean_q: 20.031237\n",
            " 116851/150000: episode: 17030, duration: 0.086s, episode steps:   8, steps per second:  94, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.483306, mae: 13.906709, mean_q: 20.117088\n",
            " 116857/150000: episode: 17031, duration: 0.059s, episode steps:   6, steps per second: 101, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.393364, mae: 13.782426, mean_q: 20.431267\n",
            " 116863/150000: episode: 17032, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.488698, mae: 13.706253, mean_q: 19.929979\n",
            " 116870/150000: episode: 17033, duration: 0.067s, episode steps:   7, steps per second: 104, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.360779, mae: 13.968491, mean_q: 20.192072\n",
            " 116878/150000: episode: 17034, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.437185, mae: 13.982123, mean_q: 20.305664\n",
            " 116884/150000: episode: 17035, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.248764, mae: 13.794904, mean_q: 20.112379\n",
            " 116893/150000: episode: 17036, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.276301, mae: 13.823402, mean_q: 20.275457\n",
            " 116902/150000: episode: 17037, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.551984, mae: 13.631927, mean_q: 20.050293\n",
            " 116911/150000: episode: 17038, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.111 [0.000, 8.000],  loss: 0.390885, mae: 13.855706, mean_q: 20.130148\n",
            " 116917/150000: episode: 17039, duration: 0.068s, episode steps:   6, steps per second:  88, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 7.000],  loss: 0.599333, mae: 13.460361, mean_q: 20.183752\n",
            " 116926/150000: episode: 17040, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.405750, mae: 13.398441, mean_q: 20.095490\n",
            " 116934/150000: episode: 17041, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.648770, mae: 13.768244, mean_q: 20.178478\n",
            " 116941/150000: episode: 17042, duration: 0.066s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.476601, mae: 13.655929, mean_q: 19.731405\n",
            " 116946/150000: episode: 17043, duration: 0.048s, episode steps:   5, steps per second: 104, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 8.000],  loss: 0.269746, mae: 14.102757, mean_q: 20.023037\n",
            " 116953/150000: episode: 17044, duration: 0.078s, episode steps:   7, steps per second:  90, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.429 [0.000, 8.000],  loss: 0.249430, mae: 13.633220, mean_q: 20.078991\n",
            " 116960/150000: episode: 17045, duration: 0.067s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.268510, mae: 14.099222, mean_q: 20.163403\n",
            " 116967/150000: episode: 17046, duration: 0.069s, episode steps:   7, steps per second: 101, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.201827, mae: 13.697050, mean_q: 20.111410\n",
            " 116973/150000: episode: 17047, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.249564, mae: 13.941841, mean_q: 20.116087\n",
            " 116981/150000: episode: 17048, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.750 [0.000, 8.000],  loss: 0.351747, mae: 13.852703, mean_q: 20.030142\n",
            " 116989/150000: episode: 17049, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.351821, mae: 13.895294, mean_q: 20.195984\n",
            " 116995/150000: episode: 17050, duration: 0.063s, episode steps:   6, steps per second:  96, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.462899, mae: 13.653071, mean_q: 19.940863\n",
            " 117000/150000: episode: 17051, duration: 0.048s, episode steps:   5, steps per second: 104, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.200 [0.000, 5.000],  loss: 0.363610, mae: 13.723536, mean_q: 20.085756\n",
            " 117008/150000: episode: 17052, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.267579, mae: 13.519152, mean_q: 20.198511\n",
            " 117016/150000: episode: 17053, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.369519, mae: 14.061252, mean_q: 19.915197\n",
            " 117023/150000: episode: 17054, duration: 0.061s, episode steps:   7, steps per second: 115, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.143 [0.000, 8.000],  loss: 0.303710, mae: 13.894516, mean_q: 20.099487\n",
            " 117032/150000: episode: 17055, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.456099, mae: 13.931945, mean_q: 20.110373\n",
            " 117038/150000: episode: 17056, duration: 0.056s, episode steps:   6, steps per second: 108, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.735797, mae: 14.264694, mean_q: 19.914167\n",
            " 117045/150000: episode: 17057, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.484039, mae: 13.604291, mean_q: 20.175493\n",
            " 117054/150000: episode: 17058, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.462099, mae: 14.109189, mean_q: 19.937859\n",
            " 117063/150000: episode: 17059, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.688991, mae: 14.017835, mean_q: 20.037861\n",
            " 117066/150000: episode: 17060, duration: 0.032s, episode steps:   3, steps per second:  93, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 2.667 [0.000, 8.000],  loss: 0.325660, mae: 13.945763, mean_q: 20.182531\n",
            " 117075/150000: episode: 17061, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.656887, mae: 13.794192, mean_q: 19.961264\n",
            " 117082/150000: episode: 17062, duration: 0.074s, episode steps:   7, steps per second:  95, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.431847, mae: 13.437355, mean_q: 20.011322\n",
            " 117088/150000: episode: 17063, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.344012, mae: 13.961669, mean_q: 20.392836\n",
            " 117093/150000: episode: 17064, duration: 0.049s, episode steps:   5, steps per second: 103, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.800 [0.000, 8.000],  loss: 0.268436, mae: 14.126780, mean_q: 20.195217\n",
            " 117100/150000: episode: 17065, duration: 0.077s, episode steps:   7, steps per second:  91, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.398318, mae: 13.997676, mean_q: 19.894077\n",
            " 117106/150000: episode: 17066, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.167 [2.000, 8.000],  loss: 0.262302, mae: 13.946727, mean_q: 20.223270\n",
            " 117111/150000: episode: 17067, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.000 [0.000, 5.000],  loss: 0.337378, mae: 13.802037, mean_q: 20.196085\n",
            " 117117/150000: episode: 17068, duration: 0.074s, episode steps:   6, steps per second:  81, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.833 [0.000, 8.000],  loss: 0.796854, mae: 14.059905, mean_q: 20.008802\n",
            " 117125/150000: episode: 17069, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.125 [0.000, 8.000],  loss: 0.332458, mae: 14.114693, mean_q: 20.324997\n",
            " 117131/150000: episode: 17070, duration: 0.057s, episode steps:   6, steps per second: 106, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.327331, mae: 13.954133, mean_q: 19.974463\n",
            " 117139/150000: episode: 17071, duration: 0.070s, episode steps:   8, steps per second: 115, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.230374, mae: 13.504297, mean_q: 20.087631\n",
            " 117144/150000: episode: 17072, duration: 0.048s, episode steps:   5, steps per second: 105, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 6.000 [4.000, 8.000],  loss: 0.896289, mae: 14.109879, mean_q: 20.057137\n",
            " 117150/150000: episode: 17073, duration: 0.068s, episode steps:   6, steps per second:  88, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2.833 [0.000, 7.000],  loss: 0.351105, mae: 13.662910, mean_q: 19.921728\n",
            " 117155/150000: episode: 17074, duration: 0.050s, episode steps:   5, steps per second:  99, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.600 [3.000, 8.000],  loss: 0.414737, mae: 13.809225, mean_q: 20.193686\n",
            " 117161/150000: episode: 17075, duration: 0.057s, episode steps:   6, steps per second: 106, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.000 [0.000, 8.000],  loss: 0.886235, mae: 14.054223, mean_q: 20.301027\n",
            " 117167/150000: episode: 17076, duration: 0.054s, episode steps:   6, steps per second: 111, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [1.000, 8.000],  loss: 0.955085, mae: 13.708984, mean_q: 19.851610\n",
            " 117176/150000: episode: 17077, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.262648, mae: 13.961758, mean_q: 20.346449\n",
            " 117185/150000: episode: 17078, duration: 0.078s, episode steps:   9, steps per second: 115, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.398654, mae: 13.381268, mean_q: 19.957829\n",
            " 117187/150000: episode: 17079, duration: 0.025s, episode steps:   2, steps per second:  81, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.171700, mae: 14.145471, mean_q: 20.004004\n",
            " 117195/150000: episode: 17080, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.289860, mae: 13.956461, mean_q: 20.099640\n",
            " 117204/150000: episode: 17081, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.319400, mae: 14.092052, mean_q: 19.982258\n",
            " 117212/150000: episode: 17082, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.769941, mae: 13.815770, mean_q: 20.042713\n",
            " 117217/150000: episode: 17083, duration: 0.057s, episode steps:   5, steps per second:  88, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.600 [0.000, 8.000],  loss: 0.501882, mae: 13.924227, mean_q: 19.968304\n",
            " 117220/150000: episode: 17084, duration: 0.036s, episode steps:   3, steps per second:  83, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 2.000 [0.000, 6.000],  loss: 0.460382, mae: 14.165801, mean_q: 20.278675\n",
            " 117225/150000: episode: 17085, duration: 0.058s, episode steps:   5, steps per second:  86, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.689652, mae: 14.244905, mean_q: 20.346649\n",
            " 117232/150000: episode: 17086, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [0.000, 8.000],  loss: 0.391223, mae: 13.870043, mean_q: 20.017633\n",
            " 117240/150000: episode: 17087, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.679321, mae: 14.116186, mean_q: 20.077637\n",
            " 117246/150000: episode: 17088, duration: 0.085s, episode steps:   6, steps per second:  71, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [1.000, 7.000],  loss: 0.662482, mae: 14.193990, mean_q: 20.080095\n",
            " 117253/150000: episode: 17089, duration: 0.095s, episode steps:   7, steps per second:  73, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.365356, mae: 13.913467, mean_q: 20.272879\n",
            " 117256/150000: episode: 17090, duration: 0.050s, episode steps:   3, steps per second:  60, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 3.667 [3.000, 5.000],  loss: 0.408221, mae: 13.521329, mean_q: 20.349442\n",
            " 117263/150000: episode: 17091, duration: 0.094s, episode steps:   7, steps per second:  75, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.363494, mae: 13.582397, mean_q: 20.270098\n",
            " 117269/150000: episode: 17092, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [0.000, 7.000],  loss: 1.188051, mae: 13.748008, mean_q: 19.961847\n",
            " 117276/150000: episode: 17093, duration: 0.118s, episode steps:   7, steps per second:  59, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.388428, mae: 14.045477, mean_q: 19.834772\n",
            " 117283/150000: episode: 17094, duration: 0.088s, episode steps:   7, steps per second:  80, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.459636, mae: 14.073934, mean_q: 20.392117\n",
            " 117289/150000: episode: 17095, duration: 0.086s, episode steps:   6, steps per second:  70, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [2.000, 8.000],  loss: 0.393002, mae: 14.045493, mean_q: 20.219528\n",
            " 117298/150000: episode: 17096, duration: 0.139s, episode steps:   9, steps per second:  65, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.360828, mae: 13.870449, mean_q: 20.154612\n",
            " 117306/150000: episode: 17097, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.483091, mae: 13.970694, mean_q: 20.048361\n",
            " 117313/150000: episode: 17098, duration: 0.090s, episode steps:   7, steps per second:  77, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.275240, mae: 13.780379, mean_q: 20.109030\n",
            " 117321/150000: episode: 17099, duration: 0.112s, episode steps:   8, steps per second:  71, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.325308, mae: 13.387657, mean_q: 20.194279\n",
            " 117327/150000: episode: 17100, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [0.000, 8.000],  loss: 0.546972, mae: 13.817939, mean_q: 20.072998\n",
            " 117333/150000: episode: 17101, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.333 [2.000, 7.000],  loss: 1.135134, mae: 13.978073, mean_q: 20.050148\n",
            " 117341/150000: episode: 17102, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.413941, mae: 13.774035, mean_q: 20.024282\n",
            " 117347/150000: episode: 17103, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [2.000, 7.000],  loss: 0.854256, mae: 13.942677, mean_q: 20.278942\n",
            " 117355/150000: episode: 17104, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.435208, mae: 13.950478, mean_q: 20.241819\n",
            " 117364/150000: episode: 17105, duration: 0.141s, episode steps:   9, steps per second:  64, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.298314, mae: 13.713708, mean_q: 20.244642\n",
            " 117372/150000: episode: 17106, duration: 0.123s, episode steps:   8, steps per second:  65, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.225927, mae: 13.997841, mean_q: 20.163177\n",
            " 117378/150000: episode: 17107, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.667 [2.000, 7.000],  loss: 0.226163, mae: 14.263104, mean_q: 20.095772\n",
            " 117386/150000: episode: 17108, duration: 0.130s, episode steps:   8, steps per second:  61, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.308875, mae: 13.807304, mean_q: 20.178043\n",
            " 117389/150000: episode: 17109, duration: 0.051s, episode steps:   3, steps per second:  59, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 2.667 [0.000, 8.000],  loss: 0.429738, mae: 13.834054, mean_q: 19.778681\n",
            " 117394/150000: episode: 17110, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.635936, mae: 13.715584, mean_q: 19.892017\n",
            " 117400/150000: episode: 17111, duration: 0.094s, episode steps:   6, steps per second:  64, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 7.000],  loss: 0.265419, mae: 14.127309, mean_q: 20.338104\n",
            " 117408/150000: episode: 17112, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.437553, mae: 13.843792, mean_q: 20.197676\n",
            " 117416/150000: episode: 17113, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.496521, mae: 13.916918, mean_q: 19.979099\n",
            " 117425/150000: episode: 17114, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.277217, mae: 14.000111, mean_q: 20.061926\n",
            " 117431/150000: episode: 17115, duration: 0.085s, episode steps:   6, steps per second:  71, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.319360, mae: 13.842875, mean_q: 20.319990\n",
            " 117439/150000: episode: 17116, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.579479, mae: 13.704596, mean_q: 19.820652\n",
            " 117445/150000: episode: 17117, duration: 0.099s, episode steps:   6, steps per second:  60, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.253425, mae: 13.950005, mean_q: 20.286592\n",
            " 117453/150000: episode: 17118, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.347120, mae: 14.040706, mean_q: 20.150158\n",
            " 117458/150000: episode: 17119, duration: 0.074s, episode steps:   5, steps per second:  68, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.800 [1.000, 6.000],  loss: 0.352097, mae: 13.767845, mean_q: 20.070782\n",
            " 117462/150000: episode: 17120, duration: 0.062s, episode steps:   4, steps per second:  64, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.500 [2.000, 7.000],  loss: 0.246991, mae: 14.091335, mean_q: 20.031181\n",
            " 117470/150000: episode: 17121, duration: 0.115s, episode steps:   8, steps per second:  69, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.670036, mae: 13.671448, mean_q: 20.066391\n",
            " 117476/150000: episode: 17122, duration: 0.093s, episode steps:   6, steps per second:  65, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [2.000, 8.000],  loss: 0.514511, mae: 13.797280, mean_q: 19.877703\n",
            " 117483/150000: episode: 17123, duration: 0.109s, episode steps:   7, steps per second:  64, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.533809, mae: 13.644232, mean_q: 20.033781\n",
            " 117492/150000: episode: 17124, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.375577, mae: 13.668868, mean_q: 20.194611\n",
            " 117501/150000: episode: 17125, duration: 0.102s, episode steps:   9, steps per second:  89, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.229709, mae: 14.163019, mean_q: 20.091600\n",
            " 117507/150000: episode: 17126, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.242268, mae: 13.799042, mean_q: 20.091736\n",
            " 117515/150000: episode: 17127, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.250 [0.000, 8.000],  loss: 0.323528, mae: 13.675409, mean_q: 20.002899\n",
            " 117524/150000: episode: 17128, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.426814, mae: 13.948775, mean_q: 20.074755\n",
            " 117530/150000: episode: 17129, duration: 0.073s, episode steps:   6, steps per second:  82, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 8.000],  loss: 0.657242, mae: 14.266701, mean_q: 19.976021\n",
            " 117535/150000: episode: 17130, duration: 0.054s, episode steps:   5, steps per second:  93, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.200 [2.000, 7.000],  loss: 0.517448, mae: 13.967352, mean_q: 19.860296\n",
            " 117542/150000: episode: 17131, duration: 0.066s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.343720, mae: 13.405787, mean_q: 20.273899\n",
            " 117549/150000: episode: 17132, duration: 0.074s, episode steps:   7, steps per second:  94, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.453802, mae: 13.858193, mean_q: 19.898787\n",
            " 117557/150000: episode: 17133, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.659582, mae: 13.518581, mean_q: 20.165417\n",
            " 117566/150000: episode: 17134, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.462518, mae: 13.586923, mean_q: 19.916719\n",
            " 117573/150000: episode: 17135, duration: 0.077s, episode steps:   7, steps per second:  91, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.643001, mae: 13.673076, mean_q: 20.162584\n",
            " 117579/150000: episode: 17136, duration: 0.060s, episode steps:   6, steps per second: 101, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.310792, mae: 13.889809, mean_q: 19.781876\n",
            " 117588/150000: episode: 17137, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.467245, mae: 13.922100, mean_q: 20.364906\n",
            " 117594/150000: episode: 17138, duration: 0.069s, episode steps:   6, steps per second:  87, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 7.000],  loss: 0.309201, mae: 13.905079, mean_q: 19.921602\n",
            " 117601/150000: episode: 17139, duration: 0.071s, episode steps:   7, steps per second:  99, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.714 [1.000, 7.000],  loss: 0.270569, mae: 14.016588, mean_q: 20.202524\n",
            " 117609/150000: episode: 17140, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.516085, mae: 13.460608, mean_q: 19.967598\n",
            " 117617/150000: episode: 17141, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.260863, mae: 13.967207, mean_q: 20.080099\n",
            " 117623/150000: episode: 17142, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [2.000, 8.000],  loss: 0.206982, mae: 13.988152, mean_q: 20.223337\n",
            " 117631/150000: episode: 17143, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.511771, mae: 14.060565, mean_q: 20.177761\n",
            " 117637/150000: episode: 17144, duration: 0.064s, episode steps:   6, steps per second:  94, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.360839, mae: 13.736557, mean_q: 19.860842\n",
            " 117644/150000: episode: 17145, duration: 0.074s, episode steps:   7, steps per second:  95, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.234856, mae: 13.799087, mean_q: 20.178263\n",
            " 117653/150000: episode: 17146, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.343362, mae: 13.853812, mean_q: 19.981718\n",
            " 117658/150000: episode: 17147, duration: 0.051s, episode steps:   5, steps per second:  98, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 8.000],  loss: 0.241446, mae: 13.974197, mean_q: 19.845753\n",
            " 117663/150000: episode: 17148, duration: 0.059s, episode steps:   5, steps per second:  85, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 8.000],  loss: 0.243831, mae: 13.752817, mean_q: 20.029333\n",
            " 117672/150000: episode: 17149, duration: 0.081s, episode steps:   9, steps per second: 112, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.462133, mae: 13.719316, mean_q: 19.813759\n",
            " 117677/150000: episode: 17150, duration: 0.054s, episode steps:   5, steps per second:  93, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.600 [2.000, 8.000],  loss: 0.354072, mae: 13.757828, mean_q: 20.139866\n",
            " 117684/150000: episode: 17151, duration: 0.077s, episode steps:   7, steps per second:  91, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.273320, mae: 14.227087, mean_q: 20.150003\n",
            " 117690/150000: episode: 17152, duration: 0.064s, episode steps:   6, steps per second:  94, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 7.000],  loss: 0.435483, mae: 13.437720, mean_q: 19.849619\n",
            " 117694/150000: episode: 17153, duration: 0.053s, episode steps:   4, steps per second:  76, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.750 [0.000, 7.000],  loss: 0.802536, mae: 13.980616, mean_q: 20.109829\n",
            " 117701/150000: episode: 17154, duration: 0.063s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.441222, mae: 13.670362, mean_q: 19.828783\n",
            " 117708/150000: episode: 17155, duration: 0.081s, episode steps:   7, steps per second:  87, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.373396, mae: 13.819459, mean_q: 20.205187\n",
            " 117714/150000: episode: 17156, duration: 0.057s, episode steps:   6, steps per second: 106, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.322061, mae: 14.021144, mean_q: 19.944410\n",
            " 117719/150000: episode: 17157, duration: 0.048s, episode steps:   5, steps per second: 103, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.600 [0.000, 6.000],  loss: 1.077975, mae: 14.004126, mean_q: 19.903160\n",
            " 117726/150000: episode: 17158, duration: 0.083s, episode steps:   7, steps per second:  84, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.000 [0.000, 7.000],  loss: 0.629268, mae: 13.943435, mean_q: 20.083197\n",
            " 117734/150000: episode: 17159, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.733437, mae: 13.700768, mean_q: 20.125134\n",
            " 117739/150000: episode: 17160, duration: 0.049s, episode steps:   5, steps per second: 103, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.403176, mae: 14.180628, mean_q: 20.187571\n",
            " 117746/150000: episode: 17161, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.336636, mae: 14.138219, mean_q: 20.102129\n",
            " 117750/150000: episode: 17162, duration: 0.044s, episode steps:   4, steps per second:  90, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.000 [0.000, 7.000],  loss: 0.209647, mae: 14.345111, mean_q: 20.309593\n",
            " 117755/150000: episode: 17163, duration: 0.059s, episode steps:   5, steps per second:  84, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.200 [0.000, 8.000],  loss: 0.782506, mae: 13.736673, mean_q: 20.010834\n",
            " 117763/150000: episode: 17164, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.302097, mae: 13.814055, mean_q: 20.241976\n",
            " 117771/150000: episode: 17165, duration: 0.074s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.333407, mae: 13.811281, mean_q: 20.039858\n",
            " 117780/150000: episode: 17166, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.225530, mae: 13.848351, mean_q: 20.107262\n",
            " 117785/150000: episode: 17167, duration: 0.056s, episode steps:   5, steps per second:  89, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 6.000 [3.000, 8.000],  loss: 0.355302, mae: 14.162638, mean_q: 19.844185\n",
            " 117791/150000: episode: 17168, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.259784, mae: 14.244514, mean_q: 20.134398\n",
            " 117796/150000: episode: 17169, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 8.000],  loss: 0.235751, mae: 14.145444, mean_q: 20.052761\n",
            " 117804/150000: episode: 17170, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.257222, mae: 13.706068, mean_q: 19.912937\n",
            " 117807/150000: episode: 17171, duration: 0.033s, episode steps:   3, steps per second:  92, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.284756, mae: 14.011563, mean_q: 20.287714\n",
            " 117816/150000: episode: 17172, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.426534, mae: 13.937138, mean_q: 19.871609\n",
            " 117825/150000: episode: 17173, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.306045, mae: 13.632333, mean_q: 20.182970\n",
            " 117830/150000: episode: 17174, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.200 [3.000, 7.000],  loss: 0.440636, mae: 13.742663, mean_q: 19.987736\n",
            " 117838/150000: episode: 17175, duration: 0.074s, episode steps:   8, steps per second: 107, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.250 [1.000, 8.000],  loss: 0.424022, mae: 13.526363, mean_q: 19.832542\n",
            " 117842/150000: episode: 17176, duration: 0.041s, episode steps:   4, steps per second:  97, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.000 [0.000, 7.000],  loss: 0.254747, mae: 13.700788, mean_q: 20.340252\n",
            " 117851/150000: episode: 17177, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.275577, mae: 13.617699, mean_q: 20.176048\n",
            " 117859/150000: episode: 17178, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.335310, mae: 13.992115, mean_q: 19.872391\n",
            " 117865/150000: episode: 17179, duration: 0.059s, episode steps:   6, steps per second: 101, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.263899, mae: 13.842659, mean_q: 20.039276\n",
            " 117870/150000: episode: 17180, duration: 0.050s, episode steps:   5, steps per second: 101, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.258679, mae: 13.931226, mean_q: 19.897182\n",
            " 117876/150000: episode: 17181, duration: 0.071s, episode steps:   6, steps per second:  84, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [1.000, 8.000],  loss: 0.246742, mae: 13.818500, mean_q: 19.880306\n",
            " 117882/150000: episode: 17182, duration: 0.067s, episode steps:   6, steps per second:  90, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [1.000, 7.000],  loss: 0.387690, mae: 13.963371, mean_q: 20.107725\n",
            " 117888/150000: episode: 17183, duration: 0.057s, episode steps:   6, steps per second: 104, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.000 [0.000, 8.000],  loss: 0.249927, mae: 13.719456, mean_q: 20.203560\n",
            " 117891/150000: episode: 17184, duration: 0.032s, episode steps:   3, steps per second:  94, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 3.667 [2.000, 7.000],  loss: 0.196948, mae: 13.876483, mean_q: 19.774012\n",
            " 117899/150000: episode: 17185, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.244098, mae: 13.906211, mean_q: 20.052711\n",
            " 117908/150000: episode: 17186, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.667 [0.000, 8.000],  loss: 0.410089, mae: 13.714746, mean_q: 19.962982\n",
            " 117916/150000: episode: 17187, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.329136, mae: 13.903673, mean_q: 19.990704\n",
            " 117924/150000: episode: 17188, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.399890, mae: 13.698231, mean_q: 19.981190\n",
            " 117932/150000: episode: 17189, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.317042, mae: 14.003744, mean_q: 20.060562\n",
            " 117941/150000: episode: 17190, duration: 0.084s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.304008, mae: 13.870457, mean_q: 20.251780\n",
            " 117947/150000: episode: 17191, duration: 0.073s, episode steps:   6, steps per second:  82, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [0.000, 7.000],  loss: 0.434569, mae: 13.725178, mean_q: 20.037485\n",
            " 117955/150000: episode: 17192, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.708142, mae: 13.800482, mean_q: 20.079514\n",
            " 117961/150000: episode: 17193, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 6.000],  loss: 0.794518, mae: 13.879173, mean_q: 20.260111\n",
            " 117967/150000: episode: 17194, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.319224, mae: 13.906260, mean_q: 20.160995\n",
            " 117974/150000: episode: 17195, duration: 0.073s, episode steps:   7, steps per second:  96, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.307378, mae: 14.222852, mean_q: 20.270464\n",
            " 117982/150000: episode: 17196, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.387360, mae: 13.986675, mean_q: 20.064075\n",
            " 117990/150000: episode: 17197, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.500 [0.000, 8.000],  loss: 0.302435, mae: 14.195345, mean_q: 20.173088\n",
            " 117999/150000: episode: 17198, duration: 0.095s, episode steps:   9, steps per second:  94, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.111 [0.000, 8.000],  loss: 0.290085, mae: 14.044353, mean_q: 20.088154\n",
            " 118006/150000: episode: 17199, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.624100, mae: 14.049212, mean_q: 19.978701\n",
            " 118013/150000: episode: 17200, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.857 [1.000, 8.000],  loss: 0.466437, mae: 13.620269, mean_q: 20.024279\n",
            " 118022/150000: episode: 17201, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.387854, mae: 14.006476, mean_q: 20.231310\n",
            " 118030/150000: episode: 17202, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.505388, mae: 13.787420, mean_q: 19.747810\n",
            " 118036/150000: episode: 17203, duration: 0.054s, episode steps:   6, steps per second: 111, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.262717, mae: 14.278140, mean_q: 20.493723\n",
            " 118045/150000: episode: 17204, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.428618, mae: 13.834424, mean_q: 19.917362\n",
            " 118051/150000: episode: 17205, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [2.000, 8.000],  loss: 0.390459, mae: 13.941163, mean_q: 20.014971\n",
            " 118057/150000: episode: 17206, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 7.000],  loss: 0.285058, mae: 14.373679, mean_q: 20.082048\n",
            " 118062/150000: episode: 17207, duration: 0.049s, episode steps:   5, steps per second: 102, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 6.800 [5.000, 8.000],  loss: 0.370722, mae: 13.680962, mean_q: 19.967207\n",
            " 118068/150000: episode: 17208, duration: 0.070s, episode steps:   6, steps per second:  86, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [2.000, 7.000],  loss: 0.268800, mae: 14.233371, mean_q: 20.135386\n",
            " 118076/150000: episode: 17209, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.283945, mae: 13.766579, mean_q: 19.965408\n",
            " 118081/150000: episode: 17210, duration: 0.054s, episode steps:   5, steps per second:  92, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.600 [2.000, 8.000],  loss: 0.291942, mae: 14.198013, mean_q: 20.265356\n",
            " 118089/150000: episode: 17211, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.222656, mae: 14.332924, mean_q: 20.089928\n",
            " 118095/150000: episode: 17212, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.167 [0.000, 6.000],  loss: 0.329959, mae: 13.662964, mean_q: 19.964455\n",
            " 118101/150000: episode: 17213, duration: 0.062s, episode steps:   6, steps per second:  96, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.833 [3.000, 8.000],  loss: 0.249551, mae: 14.264815, mean_q: 19.970255\n",
            " 118103/150000: episode: 17214, duration: 0.026s, episode steps:   2, steps per second:  78, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 3.000 [3.000, 3.000],  loss: 0.242651, mae: 13.571577, mean_q: 20.087843\n",
            " 118109/150000: episode: 17215, duration: 0.071s, episode steps:   6, steps per second:  85, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [1.000, 8.000],  loss: 0.263494, mae: 13.913085, mean_q: 20.046160\n",
            " 118117/150000: episode: 17216, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.274429, mae: 13.924580, mean_q: 20.203842\n",
            " 118123/150000: episode: 17217, duration: 0.054s, episode steps:   6, steps per second: 111, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 7.000],  loss: 0.325592, mae: 13.986279, mean_q: 19.978422\n",
            " 118130/150000: episode: 17218, duration: 0.070s, episode steps:   7, steps per second: 100, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.857 [0.000, 8.000],  loss: 0.239475, mae: 13.961995, mean_q: 20.004223\n",
            " 118139/150000: episode: 17219, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.235431, mae: 14.209332, mean_q: 20.092604\n",
            " 118146/150000: episode: 17220, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [1.000, 8.000],  loss: 0.307903, mae: 14.044888, mean_q: 20.218546\n",
            " 118154/150000: episode: 17221, duration: 0.069s, episode steps:   8, steps per second: 115, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.252393, mae: 14.025084, mean_q: 20.134478\n",
            " 118162/150000: episode: 17222, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.563051, mae: 13.999264, mean_q: 20.033587\n",
            " 118169/150000: episode: 17223, duration: 0.069s, episode steps:   7, steps per second: 101, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 5.143 [2.000, 8.000],  loss: 0.530879, mae: 14.059107, mean_q: 19.846346\n",
            " 118175/150000: episode: 17224, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.167 [0.000, 7.000],  loss: 0.322532, mae: 13.948401, mean_q: 20.012430\n",
            " 118181/150000: episode: 17225, duration: 0.061s, episode steps:   6, steps per second:  99, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.411768, mae: 13.763188, mean_q: 20.200880\n",
            " 118189/150000: episode: 17226, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.257386, mae: 13.830493, mean_q: 20.004414\n",
            " 118197/150000: episode: 17227, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.614237, mae: 13.787457, mean_q: 20.049778\n",
            " 118204/150000: episode: 17228, duration: 0.062s, episode steps:   7, steps per second: 113, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.143 [0.000, 8.000],  loss: 0.274369, mae: 13.779318, mean_q: 19.957134\n",
            " 118211/150000: episode: 17229, duration: 0.063s, episode steps:   7, steps per second: 111, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.396523, mae: 13.969084, mean_q: 20.220308\n",
            " 118220/150000: episode: 17230, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.375931, mae: 13.923939, mean_q: 20.053570\n",
            " 118225/150000: episode: 17231, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.600 [2.000, 8.000],  loss: 0.312161, mae: 13.964476, mean_q: 20.038771\n",
            " 118233/150000: episode: 17232, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.750 [1.000, 8.000],  loss: 0.216960, mae: 14.048471, mean_q: 20.181202\n",
            " 118241/150000: episode: 17233, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.425157, mae: 13.978867, mean_q: 19.776842\n",
            " 118250/150000: episode: 17234, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.484100, mae: 14.083396, mean_q: 19.954611\n",
            " 118255/150000: episode: 17235, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.800 [2.000, 7.000],  loss: 0.568048, mae: 13.770475, mean_q: 19.936138\n",
            " 118263/150000: episode: 17236, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.308618, mae: 13.655911, mean_q: 20.091656\n",
            " 118272/150000: episode: 17237, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.556 [0.000, 7.000],  loss: 0.507898, mae: 13.944048, mean_q: 20.043701\n",
            " 118280/150000: episode: 17238, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.765862, mae: 13.828833, mean_q: 19.817486\n",
            " 118285/150000: episode: 17239, duration: 0.052s, episode steps:   5, steps per second:  95, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.200 [0.000, 6.000],  loss: 0.442029, mae: 13.933954, mean_q: 20.346338\n",
            " 118294/150000: episode: 17240, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.511849, mae: 13.882438, mean_q: 20.243439\n",
            " 118298/150000: episode: 17241, duration: 0.039s, episode steps:   4, steps per second: 102, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.500 [1.000, 7.000],  loss: 0.250775, mae: 13.679212, mean_q: 19.898140\n",
            " 118304/150000: episode: 17242, duration: 0.053s, episode steps:   6, steps per second: 113, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.500 [2.000, 8.000],  loss: 0.260772, mae: 14.116853, mean_q: 20.188528\n",
            " 118310/150000: episode: 17243, duration: 0.073s, episode steps:   6, steps per second:  83, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.667 [0.000, 8.000],  loss: 0.464312, mae: 13.681401, mean_q: 19.850492\n",
            " 118316/150000: episode: 17244, duration: 0.062s, episode steps:   6, steps per second:  97, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.167 [2.000, 8.000],  loss: 0.395866, mae: 13.879082, mean_q: 20.002333\n",
            " 118320/150000: episode: 17245, duration: 0.042s, episode steps:   4, steps per second:  96, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.750 [1.000, 6.000],  loss: 0.411000, mae: 14.051576, mean_q: 19.931936\n",
            " 118326/150000: episode: 17246, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.279639, mae: 13.508319, mean_q: 20.093077\n",
            " 118334/150000: episode: 17247, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.542267, mae: 13.808521, mean_q: 19.941422\n",
            " 118341/150000: episode: 17248, duration: 0.077s, episode steps:   7, steps per second:  90, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.289193, mae: 14.005738, mean_q: 20.124495\n",
            " 118347/150000: episode: 17249, duration: 0.055s, episode steps:   6, steps per second: 108, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.333 [1.000, 7.000],  loss: 0.573536, mae: 13.661386, mean_q: 20.033466\n",
            " 118353/150000: episode: 17250, duration: 0.057s, episode steps:   6, steps per second: 106, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [1.000, 7.000],  loss: 0.280445, mae: 13.921304, mean_q: 20.029612\n",
            " 118357/150000: episode: 17251, duration: 0.054s, episode steps:   4, steps per second:  73, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.250 [2.000, 7.000],  loss: 0.175632, mae: 13.679913, mean_q: 20.063719\n",
            " 118365/150000: episode: 17252, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.750 [1.000, 8.000],  loss: 0.483149, mae: 13.828685, mean_q: 20.040480\n",
            " 118372/150000: episode: 17253, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 7.000],  loss: 0.462620, mae: 13.840823, mean_q: 19.936968\n",
            " 118378/150000: episode: 17254, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.494292, mae: 13.918607, mean_q: 20.416174\n",
            " 118383/150000: episode: 17255, duration: 0.072s, episode steps:   5, steps per second:  69, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.247057, mae: 14.253687, mean_q: 20.122253\n",
            " 118392/150000: episode: 17256, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.194259, mae: 14.160583, mean_q: 20.165375\n",
            " 118401/150000: episode: 17257, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.233206, mae: 13.844990, mean_q: 19.999878\n",
            " 118407/150000: episode: 17258, duration: 0.066s, episode steps:   6, steps per second:  90, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [1.000, 7.000],  loss: 0.551530, mae: 13.749850, mean_q: 19.842968\n",
            " 118414/150000: episode: 17259, duration: 0.070s, episode steps:   7, steps per second:  99, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.209107, mae: 14.191442, mean_q: 20.124943\n",
            " 118419/150000: episode: 17260, duration: 0.050s, episode steps:   5, steps per second: 101, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.800 [3.000, 8.000],  loss: 0.328594, mae: 13.845764, mean_q: 19.923817\n",
            " 118426/150000: episode: 17261, duration: 0.067s, episode steps:   7, steps per second: 105, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.429 [0.000, 8.000],  loss: 0.276955, mae: 14.038239, mean_q: 19.989445\n",
            " 118433/150000: episode: 17262, duration: 0.080s, episode steps:   7, steps per second:  88, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.429 [0.000, 7.000],  loss: 0.876226, mae: 14.087176, mean_q: 19.911039\n",
            " 118439/150000: episode: 17263, duration: 0.066s, episode steps:   6, steps per second:  91, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [1.000, 8.000],  loss: 0.546812, mae: 13.911850, mean_q: 19.775326\n",
            " 118445/150000: episode: 17264, duration: 0.056s, episode steps:   6, steps per second: 108, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [1.000, 8.000],  loss: 0.448039, mae: 13.656387, mean_q: 20.023203\n",
            " 118452/150000: episode: 17265, duration: 0.069s, episode steps:   7, steps per second: 101, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.299956, mae: 14.103455, mean_q: 19.904409\n",
            " 118460/150000: episode: 17266, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.654126, mae: 13.890418, mean_q: 19.931355\n",
            " 118467/150000: episode: 17267, duration: 0.081s, episode steps:   7, steps per second:  87, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.571 [0.000, 8.000],  loss: 0.301399, mae: 13.866577, mean_q: 20.084093\n",
            " 118474/150000: episode: 17268, duration: 0.101s, episode steps:   7, steps per second:  69, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.362061, mae: 14.185824, mean_q: 20.048843\n",
            " 118483/150000: episode: 17269, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.284214, mae: 13.674184, mean_q: 19.999269\n",
            " 118490/150000: episode: 17270, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.382828, mae: 14.267764, mean_q: 20.012938\n",
            " 118494/150000: episode: 17271, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 6.250 [3.000, 8.000],  loss: 0.402818, mae: 13.815123, mean_q: 19.833372\n",
            " 118503/150000: episode: 17272, duration: 0.131s, episode steps:   9, steps per second:  69, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.734218, mae: 13.793927, mean_q: 20.042471\n",
            " 118511/150000: episode: 17273, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.337352, mae: 14.074332, mean_q: 20.259769\n",
            " 118519/150000: episode: 17274, duration: 0.112s, episode steps:   8, steps per second:  72, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.461579, mae: 13.877951, mean_q: 19.943050\n",
            " 118524/150000: episode: 17275, duration: 0.087s, episode steps:   5, steps per second:  57, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [2.000, 6.000],  loss: 0.315366, mae: 13.694674, mean_q: 20.303167\n",
            " 118530/150000: episode: 17276, duration: 0.090s, episode steps:   6, steps per second:  66, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.333 [2.000, 6.000],  loss: 0.390548, mae: 13.672824, mean_q: 20.094728\n",
            " 118538/150000: episode: 17277, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.388288, mae: 13.644619, mean_q: 20.071556\n",
            " 118543/150000: episode: 17278, duration: 0.078s, episode steps:   5, steps per second:  64, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.570546, mae: 13.738759, mean_q: 20.128937\n",
            " 118552/150000: episode: 17279, duration: 0.112s, episode steps:   9, steps per second:  81, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.278723, mae: 13.870690, mean_q: 20.170036\n",
            " 118559/150000: episode: 17280, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.143 [0.000, 7.000],  loss: 0.400686, mae: 14.155920, mean_q: 20.026632\n",
            " 118565/150000: episode: 17281, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [1.000, 8.000],  loss: 0.292789, mae: 13.700654, mean_q: 20.080317\n",
            " 118573/150000: episode: 17282, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.250 [0.000, 8.000],  loss: 0.252327, mae: 13.922469, mean_q: 20.253803\n",
            " 118582/150000: episode: 17283, duration: 0.124s, episode steps:   9, steps per second:  72, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.466738, mae: 13.935966, mean_q: 20.063728\n",
            " 118586/150000: episode: 17284, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.685476, mae: 13.858426, mean_q: 19.996414\n",
            " 118593/150000: episode: 17285, duration: 0.090s, episode steps:   7, steps per second:  78, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.571 [0.000, 7.000],  loss: 0.311772, mae: 13.549989, mean_q: 19.971889\n",
            " 118599/150000: episode: 17286, duration: 0.108s, episode steps:   6, steps per second:  55, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.167 [2.000, 8.000],  loss: 0.299877, mae: 13.806897, mean_q: 20.480322\n",
            " 118608/150000: episode: 17287, duration: 0.128s, episode steps:   9, steps per second:  70, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.333 [1.000, 8.000],  loss: 0.377462, mae: 13.921860, mean_q: 20.077003\n",
            " 118617/150000: episode: 17288, duration: 0.131s, episode steps:   9, steps per second:  69, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.314757, mae: 13.725511, mean_q: 20.059601\n",
            " 118626/150000: episode: 17289, duration: 0.116s, episode steps:   9, steps per second:  78, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.677139, mae: 13.972985, mean_q: 19.984617\n",
            " 118632/150000: episode: 17290, duration: 0.096s, episode steps:   6, steps per second:  63, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 7.000],  loss: 0.473339, mae: 14.128779, mean_q: 19.870245\n",
            " 118640/150000: episode: 17291, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.399676, mae: 14.185236, mean_q: 20.140049\n",
            " 118647/150000: episode: 17292, duration: 0.106s, episode steps:   7, steps per second:  66, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.671797, mae: 14.033624, mean_q: 20.100328\n",
            " 118656/150000: episode: 17293, duration: 0.121s, episode steps:   9, steps per second:  74, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.418948, mae: 13.879620, mean_q: 20.094917\n",
            " 118665/150000: episode: 17294, duration: 0.122s, episode steps:   9, steps per second:  74, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.412304, mae: 14.178175, mean_q: 20.110634\n",
            " 118674/150000: episode: 17295, duration: 0.133s, episode steps:   9, steps per second:  68, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.711654, mae: 13.815401, mean_q: 20.169760\n",
            " 118683/150000: episode: 17296, duration: 0.123s, episode steps:   9, steps per second:  73, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.406399, mae: 13.958332, mean_q: 20.021503\n",
            " 118689/150000: episode: 17297, duration: 0.091s, episode steps:   6, steps per second:  66, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.297589, mae: 13.936062, mean_q: 20.236443\n",
            " 118695/150000: episode: 17298, duration: 0.084s, episode steps:   6, steps per second:  71, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.000 [0.000, 8.000],  loss: 0.374332, mae: 14.004979, mean_q: 19.928450\n",
            " 118703/150000: episode: 17299, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.694052, mae: 13.927301, mean_q: 20.266556\n",
            " 118707/150000: episode: 17300, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.750 [1.000, 8.000],  loss: 0.632236, mae: 13.631626, mean_q: 19.750034\n",
            " 118715/150000: episode: 17301, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.487393, mae: 13.989199, mean_q: 20.198671\n",
            " 118723/150000: episode: 17302, duration: 0.126s, episode steps:   8, steps per second:  64, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.901686, mae: 14.044842, mean_q: 20.100748\n",
            " 118731/150000: episode: 17303, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.750 [0.000, 7.000],  loss: 0.393621, mae: 13.843379, mean_q: 20.052601\n",
            " 118735/150000: episode: 17304, duration: 0.040s, episode steps:   4, steps per second: 101, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.250 [0.000, 6.000],  loss: 0.327262, mae: 13.899550, mean_q: 20.309666\n",
            " 118741/150000: episode: 17305, duration: 0.060s, episode steps:   6, steps per second:  99, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [1.000, 8.000],  loss: 0.356742, mae: 14.038194, mean_q: 20.224695\n",
            " 118746/150000: episode: 17306, duration: 0.076s, episode steps:   5, steps per second:  66, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.615649, mae: 13.658055, mean_q: 19.956869\n",
            " 118752/150000: episode: 17307, duration: 0.057s, episode steps:   6, steps per second: 104, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.356810, mae: 13.800515, mean_q: 20.241074\n",
            " 118761/150000: episode: 17308, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.363359, mae: 13.832498, mean_q: 20.150047\n",
            " 118766/150000: episode: 17309, duration: 0.054s, episode steps:   5, steps per second:  92, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.000 [2.000, 7.000],  loss: 0.596726, mae: 13.939856, mean_q: 20.118664\n",
            " 118773/150000: episode: 17310, duration: 0.081s, episode steps:   7, steps per second:  86, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.344014, mae: 14.074251, mean_q: 20.125692\n",
            " 118779/150000: episode: 17311, duration: 0.061s, episode steps:   6, steps per second:  99, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.238753, mae: 13.965571, mean_q: 20.312830\n",
            " 118783/150000: episode: 17312, duration: 0.042s, episode steps:   4, steps per second:  95, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.750 [1.000, 5.000],  loss: 0.460992, mae: 14.203974, mean_q: 19.977835\n",
            " 118790/150000: episode: 17313, duration: 0.066s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.465658, mae: 14.085673, mean_q: 19.949112\n",
            " 118799/150000: episode: 17314, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.303522, mae: 13.897648, mean_q: 19.986956\n",
            " 118807/150000: episode: 17315, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.270754, mae: 13.840731, mean_q: 20.087898\n",
            " 118813/150000: episode: 17316, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.833 [0.000, 6.000],  loss: 0.216594, mae: 13.837607, mean_q: 19.989180\n",
            " 118822/150000: episode: 17317, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.466974, mae: 13.795034, mean_q: 19.956882\n",
            " 118828/150000: episode: 17318, duration: 0.063s, episode steps:   6, steps per second:  95, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.000 [0.000, 8.000],  loss: 0.306057, mae: 13.883012, mean_q: 19.959518\n",
            " 118836/150000: episode: 17319, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.125 [0.000, 8.000],  loss: 0.337423, mae: 13.950415, mean_q: 20.034237\n",
            " 118843/150000: episode: 17320, duration: 0.069s, episode steps:   7, steps per second: 102, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.356767, mae: 13.917531, mean_q: 20.017302\n",
            " 118848/150000: episode: 17321, duration: 0.074s, episode steps:   5, steps per second:  68, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.381919, mae: 13.723383, mean_q: 19.808128\n",
            " 118857/150000: episode: 17322, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.316035, mae: 13.997044, mean_q: 19.976234\n",
            " 118863/150000: episode: 17323, duration: 0.066s, episode steps:   6, steps per second:  91, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 2.667 [0.000, 5.000],  loss: 0.433640, mae: 14.108417, mean_q: 20.071867\n",
            " 118870/150000: episode: 17324, duration: 0.070s, episode steps:   7, steps per second: 101, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.232800, mae: 13.905243, mean_q: 20.083101\n",
            " 118877/150000: episode: 17325, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.429 [0.000, 6.000],  loss: 0.778686, mae: 13.641533, mean_q: 19.989147\n",
            " 118885/150000: episode: 17326, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.298108, mae: 13.630319, mean_q: 19.976299\n",
            " 118893/150000: episode: 17327, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.736703, mae: 13.870386, mean_q: 20.294792\n",
            " 118900/150000: episode: 17328, duration: 0.074s, episode steps:   7, steps per second:  95, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 5.000 [0.000, 8.000],  loss: 0.473833, mae: 13.998505, mean_q: 19.915907\n",
            " 118904/150000: episode: 17329, duration: 0.052s, episode steps:   4, steps per second:  78, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.750 [1.000, 6.000],  loss: 0.348232, mae: 13.754858, mean_q: 20.321411\n",
            " 118913/150000: episode: 17330, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.444 [0.000, 8.000],  loss: 0.347455, mae: 13.941056, mean_q: 20.345196\n",
            " 118917/150000: episode: 17331, duration: 0.038s, episode steps:   4, steps per second: 104, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.750 [2.000, 7.000],  loss: 0.208460, mae: 13.858677, mean_q: 19.928516\n",
            " 118926/150000: episode: 17332, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.289122, mae: 13.848955, mean_q: 20.096443\n",
            " 118934/150000: episode: 17333, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.251238, mae: 13.708765, mean_q: 19.941595\n",
            " 118939/150000: episode: 17334, duration: 0.053s, episode steps:   5, steps per second:  95, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.800 [0.000, 5.000],  loss: 0.225667, mae: 13.950452, mean_q: 20.015751\n",
            " 118946/150000: episode: 17335, duration: 0.086s, episode steps:   7, steps per second:  81, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.254481, mae: 14.002187, mean_q: 20.084763\n",
            " 118952/150000: episode: 17336, duration: 0.069s, episode steps:   6, steps per second:  87, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.501579, mae: 13.882339, mean_q: 20.008768\n",
            " 118957/150000: episode: 17337, duration: 0.055s, episode steps:   5, steps per second:  92, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [0.000, 8.000],  loss: 0.455691, mae: 13.721886, mean_q: 19.862738\n",
            " 118965/150000: episode: 17338, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.436706, mae: 13.693493, mean_q: 20.219814\n",
            " 118971/150000: episode: 17339, duration: 0.069s, episode steps:   6, steps per second:  87, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [1.000, 7.000],  loss: 0.293341, mae: 13.765714, mean_q: 20.052691\n",
            " 118973/150000: episode: 17340, duration: 0.025s, episode steps:   2, steps per second:  79, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 4.000 [4.000, 4.000],  loss: 0.226709, mae: 13.469994, mean_q: 20.058477\n",
            " 118980/150000: episode: 17341, duration: 0.067s, episode steps:   7, steps per second: 104, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.714 [2.000, 8.000],  loss: 0.194931, mae: 13.940413, mean_q: 20.048443\n",
            " 118989/150000: episode: 17342, duration: 0.102s, episode steps:   9, steps per second:  89, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.229536, mae: 14.079937, mean_q: 20.065432\n",
            " 118993/150000: episode: 17343, duration: 0.042s, episode steps:   4, steps per second:  95, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.250 [1.000, 5.000],  loss: 0.300661, mae: 13.935795, mean_q: 19.880932\n",
            " 119001/150000: episode: 17344, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.314162, mae: 13.958105, mean_q: 20.041328\n",
            " 119010/150000: episode: 17345, duration: 0.080s, episode steps:   9, steps per second: 112, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.290470, mae: 13.893841, mean_q: 20.035744\n",
            " 119013/150000: episode: 17346, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 4.000 [2.000, 5.000],  loss: 0.737647, mae: 14.248654, mean_q: 19.827089\n",
            " 119019/150000: episode: 17347, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.545463, mae: 13.791748, mean_q: 19.936831\n",
            " 119025/150000: episode: 17348, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.430910, mae: 13.893093, mean_q: 19.954212\n",
            " 119034/150000: episode: 17349, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.448726, mae: 14.214812, mean_q: 20.014490\n",
            " 119041/150000: episode: 17350, duration: 0.085s, episode steps:   7, steps per second:  82, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.143 [0.000, 8.000],  loss: 0.513483, mae: 13.960493, mean_q: 19.969469\n",
            " 119050/150000: episode: 17351, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.222 [1.000, 8.000],  loss: 0.360395, mae: 13.444067, mean_q: 20.203625\n",
            " 119059/150000: episode: 17352, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.357983, mae: 13.819699, mean_q: 19.825359\n",
            " 119064/150000: episode: 17353, duration: 0.050s, episode steps:   5, steps per second:  99, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 8.000],  loss: 0.362550, mae: 13.867134, mean_q: 19.905972\n",
            " 119072/150000: episode: 17354, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.247057, mae: 13.916721, mean_q: 20.043648\n",
            " 119080/150000: episode: 17355, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.209657, mae: 13.776951, mean_q: 20.040314\n",
            " 119087/150000: episode: 17356, duration: 0.081s, episode steps:   7, steps per second:  86, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.318035, mae: 13.740014, mean_q: 19.855984\n",
            " 119094/150000: episode: 17357, duration: 0.069s, episode steps:   7, steps per second: 101, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.000 [0.000, 8.000],  loss: 0.267912, mae: 13.598688, mean_q: 19.863010\n",
            " 119103/150000: episode: 17358, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.287967, mae: 13.732202, mean_q: 19.931465\n",
            " 119110/150000: episode: 17359, duration: 0.077s, episode steps:   7, steps per second:  91, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [1.000, 8.000],  loss: 0.290329, mae: 13.945177, mean_q: 19.928654\n",
            " 119119/150000: episode: 17360, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.452833, mae: 14.097694, mean_q: 19.752275\n",
            " 119125/150000: episode: 17361, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [2.000, 7.000],  loss: 0.211327, mae: 13.987817, mean_q: 20.041071\n",
            " 119132/150000: episode: 17362, duration: 0.078s, episode steps:   7, steps per second:  90, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.714 [1.000, 8.000],  loss: 0.188212, mae: 13.838243, mean_q: 19.877598\n",
            " 119140/150000: episode: 17363, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.360025, mae: 13.772760, mean_q: 19.778688\n",
            " 119146/150000: episode: 17364, duration: 0.069s, episode steps:   6, steps per second:  87, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.382830, mae: 13.987995, mean_q: 19.913795\n",
            " 119151/150000: episode: 17365, duration: 0.047s, episode steps:   5, steps per second: 107, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.600 [0.000, 6.000],  loss: 0.357662, mae: 13.923746, mean_q: 19.939590\n",
            " 119160/150000: episode: 17366, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.701180, mae: 14.059528, mean_q: 19.913635\n",
            " 119169/150000: episode: 17367, duration: 0.077s, episode steps:   9, steps per second: 118, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.281392, mae: 13.954598, mean_q: 19.965042\n",
            " 119177/150000: episode: 17368, duration: 0.069s, episode steps:   8, steps per second: 117, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.212657, mae: 13.760085, mean_q: 20.011375\n",
            " 119186/150000: episode: 17369, duration: 0.090s, episode steps:   9, steps per second:  99, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.446982, mae: 13.919971, mean_q: 19.898670\n",
            " 119195/150000: episode: 17370, duration: 0.076s, episode steps:   9, steps per second: 118, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.217904, mae: 13.854160, mean_q: 20.040638\n",
            " 119203/150000: episode: 17371, duration: 0.070s, episode steps:   8, steps per second: 115, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.875 [0.000, 8.000],  loss: 0.302543, mae: 13.994886, mean_q: 20.141022\n",
            " 119209/150000: episode: 17372, duration: 0.054s, episode steps:   6, steps per second: 110, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [1.000, 8.000],  loss: 0.192448, mae: 14.178014, mean_q: 20.089890\n",
            " 119216/150000: episode: 17373, duration: 0.072s, episode steps:   7, steps per second:  97, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.143 [0.000, 6.000],  loss: 0.616270, mae: 14.089341, mean_q: 19.964838\n",
            " 119223/150000: episode: 17374, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.418665, mae: 13.911893, mean_q: 19.885895\n",
            " 119230/150000: episode: 17375, duration: 0.063s, episode steps:   7, steps per second: 111, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.225488, mae: 14.068242, mean_q: 19.988415\n",
            " 119236/150000: episode: 17376, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.667 [0.000, 7.000],  loss: 0.232724, mae: 14.060491, mean_q: 20.045174\n",
            " 119240/150000: episode: 17377, duration: 0.044s, episode steps:   4, steps per second:  90, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.750 [3.000, 7.000],  loss: 0.252836, mae: 13.971392, mean_q: 20.283792\n",
            " 119244/150000: episode: 17378, duration: 0.045s, episode steps:   4, steps per second:  89, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.750 [3.000, 8.000],  loss: 0.309971, mae: 13.883299, mean_q: 20.175879\n",
            " 119252/150000: episode: 17379, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.250 [0.000, 7.000],  loss: 0.446910, mae: 14.250017, mean_q: 20.044680\n",
            " 119260/150000: episode: 17380, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.554769, mae: 13.710321, mean_q: 20.066109\n",
            " 119268/150000: episode: 17381, duration: 0.071s, episode steps:   8, steps per second: 112, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.414836, mae: 13.892143, mean_q: 20.080605\n",
            " 119276/150000: episode: 17382, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.361118, mae: 13.835703, mean_q: 19.865795\n",
            " 119285/150000: episode: 17383, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.364757, mae: 14.060998, mean_q: 20.182899\n",
            " 119294/150000: episode: 17384, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.266665, mae: 13.968144, mean_q: 20.079426\n",
            " 119303/150000: episode: 17385, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.647581, mae: 13.769513, mean_q: 19.955540\n",
            " 119310/150000: episode: 17386, duration: 0.076s, episode steps:   7, steps per second:  92, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.705380, mae: 13.987352, mean_q: 19.989313\n",
            " 119319/150000: episode: 17387, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.337189, mae: 13.952404, mean_q: 20.116188\n",
            " 119327/150000: episode: 17388, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.306618, mae: 13.900126, mean_q: 20.154556\n",
            " 119333/150000: episode: 17389, duration: 0.074s, episode steps:   6, steps per second:  81, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 6.000],  loss: 0.174206, mae: 13.875979, mean_q: 19.994183\n",
            " 119342/150000: episode: 17390, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.312429, mae: 14.021910, mean_q: 20.082113\n",
            " 119350/150000: episode: 17391, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.295601, mae: 14.076923, mean_q: 19.988052\n",
            " 119357/150000: episode: 17392, duration: 0.090s, episode steps:   7, steps per second:  78, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.143 [0.000, 8.000],  loss: 0.530583, mae: 14.036659, mean_q: 20.241714\n",
            " 119363/150000: episode: 17393, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.833 [0.000, 8.000],  loss: 0.396055, mae: 13.733028, mean_q: 19.968626\n",
            " 119366/150000: episode: 17394, duration: 0.034s, episode steps:   3, steps per second:  90, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 4.000 [3.000, 6.000],  loss: 0.542740, mae: 13.537323, mean_q: 20.156294\n",
            " 119373/150000: episode: 17395, duration: 0.066s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.294285, mae: 13.839751, mean_q: 20.234926\n",
            " 119380/150000: episode: 17396, duration: 0.076s, episode steps:   7, steps per second:  92, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.283369, mae: 13.975713, mean_q: 20.023014\n",
            " 119386/150000: episode: 17397, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.167 [2.000, 8.000],  loss: 0.274352, mae: 14.042205, mean_q: 20.127832\n",
            " 119392/150000: episode: 17398, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.500 [2.000, 8.000],  loss: 0.256858, mae: 13.851493, mean_q: 20.009682\n",
            " 119397/150000: episode: 17399, duration: 0.051s, episode steps:   5, steps per second:  98, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.210774, mae: 14.135510, mean_q: 20.059433\n",
            " 119404/150000: episode: 17400, duration: 0.077s, episode steps:   7, steps per second:  91, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.183327, mae: 14.095628, mean_q: 20.150822\n",
            " 119413/150000: episode: 17401, duration: 0.080s, episode steps:   9, steps per second: 112, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.556 [0.000, 8.000],  loss: 0.223696, mae: 13.978956, mean_q: 20.083097\n",
            " 119422/150000: episode: 17402, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.232872, mae: 14.013481, mean_q: 20.084833\n",
            " 119431/150000: episode: 17403, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.333 [0.000, 8.000],  loss: 0.192691, mae: 13.829729, mean_q: 20.059607\n",
            " 119434/150000: episode: 17404, duration: 0.033s, episode steps:   3, steps per second:  91, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 6.000 [5.000, 8.000],  loss: 0.141045, mae: 13.965862, mean_q: 20.179832\n",
            " 119442/150000: episode: 17405, duration: 0.069s, episode steps:   8, steps per second: 115, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.331672, mae: 14.036595, mean_q: 20.171741\n",
            " 119448/150000: episode: 17406, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2.667 [0.000, 6.000],  loss: 0.279896, mae: 13.667672, mean_q: 20.092451\n",
            " 119455/150000: episode: 17407, duration: 0.083s, episode steps:   7, steps per second:  84, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.857 [1.000, 8.000],  loss: 0.392907, mae: 13.977814, mean_q: 20.099304\n",
            " 119462/150000: episode: 17408, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [1.000, 8.000],  loss: 0.568841, mae: 13.710425, mean_q: 20.198408\n",
            " 119469/150000: episode: 17409, duration: 0.062s, episode steps:   7, steps per second: 113, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.429 [0.000, 8.000],  loss: 0.395419, mae: 13.598526, mean_q: 19.916630\n",
            " 119472/150000: episode: 17410, duration: 0.032s, episode steps:   3, steps per second:  93, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 6.333 [3.000, 8.000],  loss: 0.192732, mae: 13.957885, mean_q: 20.152590\n",
            " 119478/150000: episode: 17411, duration: 0.065s, episode steps:   6, steps per second:  92, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [0.000, 8.000],  loss: 0.202983, mae: 14.165866, mean_q: 20.444326\n",
            " 119485/150000: episode: 17412, duration: 0.072s, episode steps:   7, steps per second:  98, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.857 [0.000, 8.000],  loss: 0.673047, mae: 13.757185, mean_q: 19.919367\n",
            " 119492/150000: episode: 17413, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.231080, mae: 13.829848, mean_q: 20.093618\n",
            " 119500/150000: episode: 17414, duration: 0.070s, episode steps:   8, steps per second: 115, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.525815, mae: 13.925895, mean_q: 20.206131\n",
            " 119509/150000: episode: 17415, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.351155, mae: 13.986281, mean_q: 20.181652\n",
            " 119516/150000: episode: 17416, duration: 0.063s, episode steps:   7, steps per second: 112, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.247814, mae: 13.581084, mean_q: 19.956324\n",
            " 119524/150000: episode: 17417, duration: 0.074s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.389751, mae: 13.555159, mean_q: 20.107273\n",
            " 119532/150000: episode: 17418, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.308944, mae: 13.871483, mean_q: 19.930981\n",
            " 119541/150000: episode: 17419, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.287091, mae: 13.657064, mean_q: 19.962877\n",
            " 119548/150000: episode: 17420, duration: 0.066s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.377000, mae: 13.797093, mean_q: 19.886953\n",
            " 119556/150000: episode: 17421, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.750 [0.000, 8.000],  loss: 0.266737, mae: 14.031162, mean_q: 20.187649\n",
            " 119565/150000: episode: 17422, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.286692, mae: 13.770095, mean_q: 20.020515\n",
            " 119573/150000: episode: 17423, duration: 0.069s, episode steps:   8, steps per second: 116, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.750 [0.000, 8.000],  loss: 0.363977, mae: 13.630262, mean_q: 20.087063\n",
            " 119579/150000: episode: 17424, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.000 [0.000, 8.000],  loss: 0.212628, mae: 13.653501, mean_q: 20.154070\n",
            " 119585/150000: episode: 17425, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [1.000, 8.000],  loss: 0.399667, mae: 13.662479, mean_q: 20.101015\n",
            " 119594/150000: episode: 17426, duration: 0.078s, episode steps:   9, steps per second: 116, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.496805, mae: 13.732499, mean_q: 19.955700\n",
            " 119602/150000: episode: 17427, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.373286, mae: 14.071308, mean_q: 20.007256\n",
            " 119608/150000: episode: 17428, duration: 0.065s, episode steps:   6, steps per second:  92, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2.833 [0.000, 6.000],  loss: 0.263706, mae: 13.911929, mean_q: 20.243294\n",
            " 119614/150000: episode: 17429, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [2.000, 8.000],  loss: 0.235811, mae: 13.931026, mean_q: 19.992809\n",
            " 119619/150000: episode: 17430, duration: 0.047s, episode steps:   5, steps per second: 106, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.800 [0.000, 8.000],  loss: 0.271295, mae: 14.055349, mean_q: 20.018791\n",
            " 119625/150000: episode: 17431, duration: 0.070s, episode steps:   6, steps per second:  86, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.313992, mae: 14.148037, mean_q: 19.870039\n",
            " 119633/150000: episode: 17432, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.291830, mae: 13.723150, mean_q: 20.016354\n",
            " 119639/150000: episode: 17433, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [1.000, 7.000],  loss: 0.495935, mae: 14.068995, mean_q: 20.051144\n",
            " 119646/150000: episode: 17434, duration: 0.062s, episode steps:   7, steps per second: 112, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.420359, mae: 14.119554, mean_q: 20.019716\n",
            " 119653/150000: episode: 17435, duration: 0.082s, episode steps:   7, steps per second:  85, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.571 [0.000, 8.000],  loss: 1.015440, mae: 14.142728, mean_q: 19.897394\n",
            " 119660/150000: episode: 17436, duration: 0.068s, episode steps:   7, steps per second: 103, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.469495, mae: 13.777991, mean_q: 20.019789\n",
            " 119668/150000: episode: 17437, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.294332, mae: 13.580873, mean_q: 20.418476\n",
            " 119676/150000: episode: 17438, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.498646, mae: 13.773886, mean_q: 20.053844\n",
            " 119681/150000: episode: 17439, duration: 0.048s, episode steps:   5, steps per second: 104, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 6.000],  loss: 0.439860, mae: 13.738350, mean_q: 20.030905\n",
            " 119690/150000: episode: 17440, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.288393, mae: 14.154308, mean_q: 20.248367\n",
            " 119694/150000: episode: 17441, duration: 0.040s, episode steps:   4, steps per second: 101, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.500 [2.000, 6.000],  loss: 0.225619, mae: 13.690521, mean_q: 20.102047\n",
            " 119703/150000: episode: 17442, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.268986, mae: 13.868395, mean_q: 20.083426\n",
            " 119711/150000: episode: 17443, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.625 [0.000, 8.000],  loss: 0.282151, mae: 13.777943, mean_q: 20.137623\n",
            " 119720/150000: episode: 17444, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.375861, mae: 13.705115, mean_q: 19.972548\n",
            " 119728/150000: episode: 17445, duration: 0.122s, episode steps:   8, steps per second:  65, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.505102, mae: 13.989275, mean_q: 20.297140\n",
            " 119737/150000: episode: 17446, duration: 0.123s, episode steps:   9, steps per second:  73, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.274771, mae: 14.146674, mean_q: 19.998726\n",
            " 119746/150000: episode: 17447, duration: 0.144s, episode steps:   9, steps per second:  63, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.457623, mae: 13.841469, mean_q: 20.370924\n",
            " 119751/150000: episode: 17448, duration: 0.083s, episode steps:   5, steps per second:  60, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.435014, mae: 13.997624, mean_q: 19.874844\n",
            " 119759/150000: episode: 17449, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.625 [0.000, 8.000],  loss: 0.344879, mae: 13.881733, mean_q: 20.213158\n",
            " 119766/150000: episode: 17450, duration: 0.117s, episode steps:   7, steps per second:  60, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.633233, mae: 13.466439, mean_q: 20.058657\n",
            " 119774/150000: episode: 17451, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.213898, mae: 13.940279, mean_q: 20.181313\n",
            " 119781/150000: episode: 17452, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.356373, mae: 13.731880, mean_q: 20.342058\n",
            " 119786/150000: episode: 17453, duration: 0.082s, episode steps:   5, steps per second:  61, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.200 [0.000, 7.000],  loss: 0.288586, mae: 14.025891, mean_q: 20.027176\n",
            " 119792/150000: episode: 17454, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.230891, mae: 13.964900, mean_q: 20.313698\n",
            " 119798/150000: episode: 17455, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [1.000, 7.000],  loss: 0.617116, mae: 13.633148, mean_q: 20.209654\n",
            " 119801/150000: episode: 17456, duration: 0.056s, episode steps:   3, steps per second:  54, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.179858, mae: 14.353055, mean_q: 20.121117\n",
            " 119808/150000: episode: 17457, duration: 0.094s, episode steps:   7, steps per second:  74, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [1.000, 8.000],  loss: 0.277689, mae: 13.672915, mean_q: 20.049250\n",
            " 119813/150000: episode: 17458, duration: 0.067s, episode steps:   5, steps per second:  74, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.301307, mae: 14.050859, mean_q: 20.207182\n",
            " 119822/150000: episode: 17459, duration: 0.126s, episode steps:   9, steps per second:  72, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.271999, mae: 13.983286, mean_q: 20.172302\n",
            " 119831/150000: episode: 17460, duration: 0.123s, episode steps:   9, steps per second:  73, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.353138, mae: 14.267068, mean_q: 20.174610\n",
            " 119836/150000: episode: 17461, duration: 0.073s, episode steps:   5, steps per second:  68, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.600 [0.000, 5.000],  loss: 0.241232, mae: 14.053243, mean_q: 20.124990\n",
            " 119843/150000: episode: 17462, duration: 0.104s, episode steps:   7, steps per second:  67, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 7.000],  loss: 0.325884, mae: 13.401828, mean_q: 20.132521\n",
            " 119847/150000: episode: 17463, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.750 [2.000, 7.000],  loss: 0.149645, mae: 14.457697, mean_q: 20.228043\n",
            " 119856/150000: episode: 17464, duration: 0.123s, episode steps:   9, steps per second:  73, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.245131, mae: 14.055239, mean_q: 20.015289\n",
            " 119864/150000: episode: 17465, duration: 0.116s, episode steps:   8, steps per second:  69, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.309503, mae: 14.073765, mean_q: 20.227896\n",
            " 119870/150000: episode: 17466, duration: 0.092s, episode steps:   6, steps per second:  65, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [1.000, 8.000],  loss: 0.221969, mae: 13.894790, mean_q: 20.028028\n",
            " 119878/150000: episode: 17467, duration: 0.119s, episode steps:   8, steps per second:  67, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.521189, mae: 14.253002, mean_q: 20.046907\n",
            " 119885/150000: episode: 17468, duration: 0.102s, episode steps:   7, steps per second:  68, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.187009, mae: 13.927611, mean_q: 20.168230\n",
            " 119892/150000: episode: 17469, duration: 0.102s, episode steps:   7, steps per second:  69, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.305280, mae: 13.740716, mean_q: 20.084665\n",
            " 119897/150000: episode: 17470, duration: 0.079s, episode steps:   5, steps per second:  63, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.800 [0.000, 5.000],  loss: 0.240005, mae: 13.932753, mean_q: 20.248726\n",
            " 119905/150000: episode: 17471, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.000 [0.000, 8.000],  loss: 0.269572, mae: 13.941389, mean_q: 19.938673\n",
            " 119912/150000: episode: 17472, duration: 0.101s, episode steps:   7, steps per second:  70, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.286 [1.000, 7.000],  loss: 0.289175, mae: 13.381431, mean_q: 20.024984\n",
            " 119917/150000: episode: 17473, duration: 0.074s, episode steps:   5, steps per second:  67, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.400 [0.000, 6.000],  loss: 0.279537, mae: 13.854657, mean_q: 20.196972\n",
            " 119926/150000: episode: 17474, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.215964, mae: 13.826927, mean_q: 20.047510\n",
            " 119935/150000: episode: 17475, duration: 0.126s, episode steps:   9, steps per second:  71, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.637947, mae: 13.384203, mean_q: 20.018223\n",
            " 119944/150000: episode: 17476, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.339051, mae: 14.319319, mean_q: 20.100584\n",
            " 119949/150000: episode: 17477, duration: 0.077s, episode steps:   5, steps per second:  65, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.000 [0.000, 8.000],  loss: 0.341535, mae: 13.639908, mean_q: 20.015783\n",
            " 119952/150000: episode: 17478, duration: 0.047s, episode steps:   3, steps per second:  63, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 2.667 [2.000, 3.000],  loss: 0.438166, mae: 13.649898, mean_q: 19.911375\n",
            " 119960/150000: episode: 17479, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.243026, mae: 13.773553, mean_q: 19.980495\n",
            " 119969/150000: episode: 17480, duration: 0.142s, episode steps:   9, steps per second:  63, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.667 [1.000, 8.000],  loss: 0.309359, mae: 13.978477, mean_q: 20.013605\n",
            " 119974/150000: episode: 17481, duration: 0.084s, episode steps:   5, steps per second:  60, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 7.000],  loss: 0.314722, mae: 13.846796, mean_q: 20.197311\n",
            " 119979/150000: episode: 17482, duration: 0.065s, episode steps:   5, steps per second:  76, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.345810, mae: 14.016348, mean_q: 20.091633\n",
            " 119987/150000: episode: 17483, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.281561, mae: 14.090122, mean_q: 20.160879\n",
            " 119989/150000: episode: 17484, duration: 0.026s, episode steps:   2, steps per second:  77, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 5.000 [5.000, 5.000],  loss: 0.275016, mae: 14.019681, mean_q: 19.988216\n",
            " 119997/150000: episode: 17485, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.250 [0.000, 8.000],  loss: 0.337224, mae: 13.985957, mean_q: 20.005556\n",
            " 120002/150000: episode: 17486, duration: 0.052s, episode steps:   5, steps per second:  96, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.500302, mae: 13.567700, mean_q: 19.888165\n",
            " 120009/150000: episode: 17487, duration: 0.068s, episode steps:   7, steps per second: 103, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.253650, mae: 14.170106, mean_q: 19.903997\n",
            " 120015/150000: episode: 17488, duration: 0.059s, episode steps:   6, steps per second: 101, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.153795, mae: 14.184535, mean_q: 20.063976\n",
            " 120018/150000: episode: 17489, duration: 0.044s, episode steps:   3, steps per second:  69, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 4.000 [3.000, 6.000],  loss: 0.298728, mae: 13.730479, mean_q: 20.014471\n",
            " 120022/150000: episode: 17490, duration: 0.043s, episode steps:   4, steps per second:  92, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.750 [3.000, 7.000],  loss: 0.220731, mae: 14.055375, mean_q: 20.057568\n",
            " 120030/150000: episode: 17491, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.230908, mae: 14.033386, mean_q: 20.099558\n",
            " 120037/150000: episode: 17492, duration: 0.066s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.416941, mae: 14.283630, mean_q: 20.149021\n",
            " 120042/150000: episode: 17493, duration: 0.061s, episode steps:   5, steps per second:  82, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.800 [1.000, 5.000],  loss: 0.221045, mae: 13.870450, mean_q: 19.989628\n",
            " 120051/150000: episode: 17494, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.794597, mae: 13.804368, mean_q: 19.971880\n",
            " 120056/150000: episode: 17495, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.400 [1.000, 7.000],  loss: 0.325808, mae: 13.873735, mean_q: 20.187851\n",
            " 120065/150000: episode: 17496, duration: 0.104s, episode steps:   9, steps per second:  86, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.554767, mae: 14.010761, mean_q: 20.302132\n",
            " 120072/150000: episode: 17497, duration: 0.068s, episode steps:   7, steps per second: 103, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.143 [0.000, 8.000],  loss: 0.493089, mae: 13.803744, mean_q: 20.041988\n",
            " 120077/150000: episode: 17498, duration: 0.048s, episode steps:   5, steps per second: 104, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.275034, mae: 14.075604, mean_q: 20.308842\n",
            " 120084/150000: episode: 17499, duration: 0.066s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.143 [0.000, 8.000],  loss: 0.234357, mae: 14.240727, mean_q: 20.301012\n",
            " 120093/150000: episode: 17500, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.460674, mae: 13.937076, mean_q: 20.021427\n",
            " 120100/150000: episode: 17501, duration: 0.068s, episode steps:   7, steps per second: 103, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 7.000],  loss: 0.239171, mae: 13.933158, mean_q: 20.051771\n",
            " 120108/150000: episode: 17502, duration: 0.077s, episode steps:   8, steps per second: 103, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.269959, mae: 14.184706, mean_q: 20.242640\n",
            " 120114/150000: episode: 17503, duration: 0.068s, episode steps:   6, steps per second:  88, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.667 [0.000, 8.000],  loss: 0.556446, mae: 13.857749, mean_q: 20.075003\n",
            " 120119/150000: episode: 17504, duration: 0.055s, episode steps:   5, steps per second:  90, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.600 [0.000, 7.000],  loss: 0.325277, mae: 13.815466, mean_q: 19.783606\n",
            " 120128/150000: episode: 17505, duration: 0.088s, episode steps:   9, steps per second: 103, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.223586, mae: 13.867872, mean_q: 20.146494\n",
            " 120133/150000: episode: 17506, duration: 0.048s, episode steps:   5, steps per second: 103, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.497504, mae: 14.342389, mean_q: 19.940569\n",
            " 120141/150000: episode: 17507, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.257199, mae: 13.844769, mean_q: 20.132931\n",
            " 120146/150000: episode: 17508, duration: 0.049s, episode steps:   5, steps per second: 102, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.400 [0.000, 5.000],  loss: 0.244716, mae: 13.763318, mean_q: 20.142822\n",
            " 120153/150000: episode: 17509, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.324801, mae: 13.939254, mean_q: 20.006098\n",
            " 120155/150000: episode: 17510, duration: 0.026s, episode steps:   2, steps per second:  78, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 3.000 [3.000, 3.000],  loss: 0.366604, mae: 13.723413, mean_q: 20.130367\n",
            " 120161/150000: episode: 17511, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 7.000],  loss: 0.226506, mae: 13.691688, mean_q: 20.156843\n",
            " 120167/150000: episode: 17512, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.833 [0.000, 8.000],  loss: 0.475155, mae: 13.869874, mean_q: 19.975695\n",
            " 120174/150000: episode: 17513, duration: 0.067s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.255899, mae: 13.694531, mean_q: 20.046261\n",
            " 120179/150000: episode: 17514, duration: 0.053s, episode steps:   5, steps per second:  94, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.800 [1.000, 7.000],  loss: 0.265620, mae: 14.090307, mean_q: 20.361029\n",
            " 120187/150000: episode: 17515, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.399164, mae: 13.375384, mean_q: 19.796827\n",
            " 120192/150000: episode: 17516, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.400 [1.000, 5.000],  loss: 0.432248, mae: 13.754103, mean_q: 20.111664\n",
            " 120200/150000: episode: 17517, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.343041, mae: 13.770058, mean_q: 19.812586\n",
            " 120206/150000: episode: 17518, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 7.000],  loss: 0.287881, mae: 13.996633, mean_q: 20.233212\n",
            " 120213/150000: episode: 17519, duration: 0.072s, episode steps:   7, steps per second:  97, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.857 [1.000, 8.000],  loss: 0.588730, mae: 13.581060, mean_q: 19.906122\n",
            " 120222/150000: episode: 17520, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.365611, mae: 14.152566, mean_q: 19.969875\n",
            " 120229/150000: episode: 17521, duration: 0.066s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.223200, mae: 14.435717, mean_q: 20.174530\n",
            " 120238/150000: episode: 17522, duration: 0.095s, episode steps:   9, steps per second:  94, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.277504, mae: 14.088717, mean_q: 19.992399\n",
            " 120246/150000: episode: 17523, duration: 0.072s, episode steps:   8, steps per second: 112, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.238914, mae: 13.486414, mean_q: 20.044720\n",
            " 120252/150000: episode: 17524, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.419030, mae: 13.695705, mean_q: 19.952005\n",
            " 120260/150000: episode: 17525, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.272076, mae: 13.881031, mean_q: 20.131195\n",
            " 120263/150000: episode: 17526, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 4.667 [3.000, 8.000],  loss: 0.230569, mae: 14.614833, mean_q: 20.149591\n",
            " 120266/150000: episode: 17527, duration: 0.042s, episode steps:   3, steps per second:  71, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 4.667 [3.000, 8.000],  loss: 0.700296, mae: 13.762936, mean_q: 20.107849\n",
            " 120275/150000: episode: 17528, duration: 0.076s, episode steps:   9, steps per second: 118, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.264338, mae: 14.209042, mean_q: 20.045650\n",
            " 120282/150000: episode: 17529, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.143 [0.000, 8.000],  loss: 0.508213, mae: 13.708926, mean_q: 20.074497\n",
            " 120290/150000: episode: 17530, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.886756, mae: 14.236742, mean_q: 20.028469\n",
            " 120297/150000: episode: 17531, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.337165, mae: 14.123397, mean_q: 20.029013\n",
            " 120300/150000: episode: 17532, duration: 0.032s, episode steps:   3, steps per second:  93, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 4.000 [2.000, 8.000],  loss: 0.281125, mae: 14.083806, mean_q: 20.524170\n",
            " 120307/150000: episode: 17533, duration: 0.063s, episode steps:   7, steps per second: 112, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.857 [1.000, 7.000],  loss: 0.313625, mae: 14.019896, mean_q: 20.064867\n",
            " 120316/150000: episode: 17534, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.239390, mae: 14.026927, mean_q: 20.083853\n",
            " 120323/150000: episode: 17535, duration: 0.070s, episode steps:   7, steps per second: 100, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.226389, mae: 14.023287, mean_q: 19.989223\n",
            " 120331/150000: episode: 17536, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.397970, mae: 13.757056, mean_q: 19.966702\n",
            " 120338/150000: episode: 17537, duration: 0.063s, episode steps:   7, steps per second: 112, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.368325, mae: 13.892450, mean_q: 20.106329\n",
            " 120346/150000: episode: 17538, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.343278, mae: 13.873493, mean_q: 20.162415\n",
            " 120351/150000: episode: 17539, duration: 0.047s, episode steps:   5, steps per second: 107, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [1.000, 8.000],  loss: 0.530187, mae: 13.979045, mean_q: 20.022736\n",
            " 120360/150000: episode: 17540, duration: 0.081s, episode steps:   9, steps per second: 110, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.352906, mae: 13.880549, mean_q: 20.223984\n",
            " 120368/150000: episode: 17541, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.581691, mae: 13.879173, mean_q: 20.278505\n",
            " 120376/150000: episode: 17542, duration: 0.077s, episode steps:   8, steps per second: 103, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.375 [0.000, 7.000],  loss: 0.326359, mae: 14.004072, mean_q: 20.214973\n",
            " 120382/150000: episode: 17543, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.833 [1.000, 7.000],  loss: 0.360492, mae: 14.147983, mean_q: 19.853159\n",
            " 120389/150000: episode: 17544, duration: 0.072s, episode steps:   7, steps per second:  97, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.286 [0.000, 8.000],  loss: 1.241403, mae: 13.896524, mean_q: 20.289196\n",
            " 120395/150000: episode: 17545, duration: 0.057s, episode steps:   6, steps per second: 106, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.386469, mae: 13.848633, mean_q: 20.026512\n",
            " 120401/150000: episode: 17546, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 7.000],  loss: 0.385844, mae: 13.912471, mean_q: 20.063271\n",
            " 120410/150000: episode: 17547, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.574638, mae: 13.924828, mean_q: 20.019886\n",
            " 120417/150000: episode: 17548, duration: 0.065s, episode steps:   7, steps per second: 107, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.714 [0.000, 8.000],  loss: 0.754165, mae: 13.748042, mean_q: 20.120749\n",
            " 120424/150000: episode: 17549, duration: 0.067s, episode steps:   7, steps per second: 104, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.286 [0.000, 7.000],  loss: 0.482906, mae: 13.657069, mean_q: 20.256165\n",
            " 120433/150000: episode: 17550, duration: 0.089s, episode steps:   9, steps per second: 102, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.613791, mae: 13.845348, mean_q: 19.944901\n",
            " 120435/150000: episode: 17551, duration: 0.026s, episode steps:   2, steps per second:  78, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 3.000 [3.000, 3.000],  loss: 0.305851, mae: 13.526731, mean_q: 19.724045\n",
            " 120444/150000: episode: 17552, duration: 0.078s, episode steps:   9, steps per second: 115, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.316354, mae: 14.120081, mean_q: 20.147118\n",
            " 120452/150000: episode: 17553, duration: 0.067s, episode steps:   8, steps per second: 119, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.250 [0.000, 8.000],  loss: 0.397018, mae: 13.698296, mean_q: 19.928795\n",
            " 120459/150000: episode: 17554, duration: 0.072s, episode steps:   7, steps per second:  97, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.714 [0.000, 8.000],  loss: 0.250238, mae: 13.660217, mean_q: 20.158825\n",
            " 120466/150000: episode: 17555, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.143 [0.000, 7.000],  loss: 0.456154, mae: 14.005654, mean_q: 19.997427\n",
            " 120473/150000: episode: 17556, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 5.143 [2.000, 8.000],  loss: 0.356388, mae: 13.907320, mean_q: 19.994019\n",
            " 120478/150000: episode: 17557, duration: 0.061s, episode steps:   5, steps per second:  83, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.374985, mae: 14.135187, mean_q: 19.997967\n",
            " 120486/150000: episode: 17558, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.361863, mae: 13.888359, mean_q: 19.859356\n",
            " 120493/150000: episode: 17559, duration: 0.066s, episode steps:   7, steps per second: 105, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.571 [1.000, 7.000],  loss: 0.566674, mae: 13.665052, mean_q: 20.030609\n",
            " 120501/150000: episode: 17560, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.419427, mae: 13.877058, mean_q: 20.225910\n",
            " 120506/150000: episode: 17561, duration: 0.046s, episode steps:   5, steps per second: 109, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.200 [0.000, 8.000],  loss: 0.564887, mae: 13.620852, mean_q: 19.881495\n",
            " 120512/150000: episode: 17562, duration: 0.065s, episode steps:   6, steps per second:  92, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.286945, mae: 13.969151, mean_q: 20.152346\n",
            " 120515/150000: episode: 17563, duration: 0.034s, episode steps:   3, steps per second:  89, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 0.667 [0.000, 2.000],  loss: 0.198127, mae: 14.049305, mean_q: 20.230843\n",
            " 120521/150000: episode: 17564, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 6.000],  loss: 0.314980, mae: 13.996053, mean_q: 20.091087\n",
            " 120529/150000: episode: 17565, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.347546, mae: 13.875713, mean_q: 20.049578\n",
            " 120536/150000: episode: 17566, duration: 0.076s, episode steps:   7, steps per second:  92, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.429 [0.000, 7.000],  loss: 0.482674, mae: 13.862135, mean_q: 20.128782\n",
            " 120543/150000: episode: 17567, duration: 0.071s, episode steps:   7, steps per second:  99, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.270984, mae: 13.902557, mean_q: 19.956623\n",
            " 120551/150000: episode: 17568, duration: 0.069s, episode steps:   8, steps per second: 116, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.302084, mae: 13.790690, mean_q: 20.166954\n",
            " 120557/150000: episode: 17569, duration: 0.053s, episode steps:   6, steps per second: 113, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [2.000, 7.000],  loss: 0.522977, mae: 13.696499, mean_q: 19.835146\n",
            " 120564/150000: episode: 17570, duration: 0.079s, episode steps:   7, steps per second:  89, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.312465, mae: 13.602530, mean_q: 20.177485\n",
            " 120573/150000: episode: 17571, duration: 0.077s, episode steps:   9, steps per second: 116, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.263705, mae: 13.854151, mean_q: 20.132843\n",
            " 120578/150000: episode: 17572, duration: 0.057s, episode steps:   5, steps per second:  88, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.200 [0.000, 5.000],  loss: 0.200583, mae: 13.752195, mean_q: 19.955414\n",
            " 120585/150000: episode: 17573, duration: 0.067s, episode steps:   7, steps per second: 104, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 7.000],  loss: 0.519392, mae: 13.856320, mean_q: 19.934353\n",
            " 120593/150000: episode: 17574, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.330977, mae: 13.842789, mean_q: 20.077936\n",
            " 120602/150000: episode: 17575, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.257773, mae: 13.708530, mean_q: 20.186750\n",
            " 120611/150000: episode: 17576, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.367099, mae: 13.910507, mean_q: 19.949869\n",
            " 120620/150000: episode: 17577, duration: 0.081s, episode steps:   9, steps per second: 112, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.239829, mae: 13.955962, mean_q: 20.087528\n",
            " 120628/150000: episode: 17578, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.311572, mae: 13.923919, mean_q: 20.236345\n",
            " 120631/150000: episode: 17579, duration: 0.031s, episode steps:   3, steps per second:  97, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.326132, mae: 13.630549, mean_q: 20.039215\n",
            " 120637/150000: episode: 17580, duration: 0.068s, episode steps:   6, steps per second:  88, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.000 [0.000, 7.000],  loss: 0.333559, mae: 13.983467, mean_q: 19.774096\n",
            " 120641/150000: episode: 17581, duration: 0.039s, episode steps:   4, steps per second: 102, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.500 [2.000, 6.000],  loss: 0.186365, mae: 14.382887, mean_q: 20.171667\n",
            " 120650/150000: episode: 17582, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.889 [0.000, 8.000],  loss: 0.418969, mae: 13.745241, mean_q: 19.896858\n",
            " 120657/150000: episode: 17583, duration: 0.062s, episode steps:   7, steps per second: 113, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.185253, mae: 13.495901, mean_q: 20.105671\n",
            " 120666/150000: episode: 17584, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.387955, mae: 13.759116, mean_q: 19.844717\n",
            " 120675/150000: episode: 17585, duration: 0.078s, episode steps:   9, steps per second: 116, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.333 [0.000, 8.000],  loss: 0.280848, mae: 14.021425, mean_q: 19.872021\n",
            " 120684/150000: episode: 17586, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.207596, mae: 13.866175, mean_q: 20.143974\n",
            " 120690/150000: episode: 17587, duration: 0.071s, episode steps:   6, steps per second:  85, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.000 [0.000, 7.000],  loss: 0.205991, mae: 13.901839, mean_q: 20.144081\n",
            " 120697/150000: episode: 17588, duration: 0.066s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 7.000],  loss: 0.247106, mae: 13.928939, mean_q: 19.964750\n",
            " 120704/150000: episode: 17589, duration: 0.062s, episode steps:   7, steps per second: 112, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.702332, mae: 14.002055, mean_q: 19.972996\n",
            " 120711/150000: episode: 17590, duration: 0.061s, episode steps:   7, steps per second: 115, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 7.000],  loss: 0.275346, mae: 13.940356, mean_q: 20.126957\n",
            " 120717/150000: episode: 17591, duration: 0.069s, episode steps:   6, steps per second:  87, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.000 [0.000, 8.000],  loss: 0.208946, mae: 13.854362, mean_q: 20.073532\n",
            " 120722/150000: episode: 17592, duration: 0.049s, episode steps:   5, steps per second: 101, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.200 [0.000, 5.000],  loss: 0.997293, mae: 13.350492, mean_q: 19.743994\n",
            " 120729/150000: episode: 17593, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.429 [0.000, 7.000],  loss: 0.390258, mae: 13.787413, mean_q: 19.918533\n",
            " 120736/150000: episode: 17594, duration: 0.063s, episode steps:   7, steps per second: 111, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.287075, mae: 13.906751, mean_q: 20.138790\n",
            " 120742/150000: episode: 17595, duration: 0.068s, episode steps:   6, steps per second:  88, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.445049, mae: 14.137557, mean_q: 20.225395\n",
            " 120751/150000: episode: 17596, duration: 0.077s, episode steps:   9, steps per second: 116, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.219550, mae: 13.615989, mean_q: 19.977766\n",
            " 120760/150000: episode: 17597, duration: 0.078s, episode steps:   9, steps per second: 116, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.111 [0.000, 8.000],  loss: 0.397981, mae: 13.724747, mean_q: 19.926489\n",
            " 120766/150000: episode: 17598, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.167 [0.000, 8.000],  loss: 0.233279, mae: 14.074758, mean_q: 20.293159\n",
            " 120775/150000: episode: 17599, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.392367, mae: 13.785100, mean_q: 19.913982\n",
            " 120782/150000: episode: 17600, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.426679, mae: 13.959251, mean_q: 20.157904\n",
            " 120790/150000: episode: 17601, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.447169, mae: 14.071218, mean_q: 19.956125\n",
            " 120797/150000: episode: 17602, duration: 0.071s, episode steps:   7, steps per second:  99, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.770469, mae: 13.732224, mean_q: 19.931242\n",
            " 120806/150000: episode: 17603, duration: 0.082s, episode steps:   9, steps per second: 109, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.889 [0.000, 8.000],  loss: 0.362628, mae: 13.948454, mean_q: 20.136774\n",
            " 120815/150000: episode: 17604, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.490180, mae: 13.831150, mean_q: 19.983171\n",
            " 120820/150000: episode: 17605, duration: 0.047s, episode steps:   5, steps per second: 107, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [1.000, 8.000],  loss: 0.334302, mae: 13.839144, mean_q: 20.109379\n",
            " 120827/150000: episode: 17606, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [0.000, 8.000],  loss: 0.589141, mae: 13.768382, mean_q: 19.992220\n",
            " 120836/150000: episode: 17607, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.603895, mae: 13.532581, mean_q: 20.175697\n",
            " 120843/150000: episode: 17608, duration: 0.076s, episode steps:   7, steps per second:  92, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.355462, mae: 13.905330, mean_q: 20.036129\n",
            " 120850/150000: episode: 17609, duration: 0.062s, episode steps:   7, steps per second: 112, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 7.000],  loss: 0.264492, mae: 13.974681, mean_q: 20.333012\n",
            " 120858/150000: episode: 17610, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.248596, mae: 13.626194, mean_q: 19.936287\n",
            " 120865/150000: episode: 17611, duration: 0.071s, episode steps:   7, steps per second:  98, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.538782, mae: 13.677310, mean_q: 20.057257\n",
            " 120872/150000: episode: 17612, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.420566, mae: 13.946969, mean_q: 19.731857\n",
            " 120878/150000: episode: 17613, duration: 0.056s, episode steps:   6, steps per second: 108, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 7.000],  loss: 0.432905, mae: 14.042058, mean_q: 20.281443\n",
            " 120883/150000: episode: 17614, duration: 0.058s, episode steps:   5, steps per second:  87, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.200 [2.000, 7.000],  loss: 0.659184, mae: 14.012741, mean_q: 20.216503\n",
            " 120892/150000: episode: 17615, duration: 0.076s, episode steps:   9, steps per second: 118, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.587040, mae: 14.124273, mean_q: 19.974316\n",
            " 120898/150000: episode: 17616, duration: 0.064s, episode steps:   6, steps per second:  94, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [1.000, 7.000],  loss: 0.893317, mae: 13.790620, mean_q: 20.230707\n",
            " 120907/150000: episode: 17617, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.356705, mae: 14.019001, mean_q: 20.249580\n",
            " 120915/150000: episode: 17618, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.413854, mae: 13.986652, mean_q: 20.164467\n",
            " 120924/150000: episode: 17619, duration: 0.077s, episode steps:   9, steps per second: 118, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.667 [0.000, 7.000],  loss: 0.728218, mae: 13.742670, mean_q: 19.865067\n",
            " 120931/150000: episode: 17620, duration: 0.073s, episode steps:   7, steps per second:  96, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 5.286 [2.000, 8.000],  loss: 0.275388, mae: 13.535575, mean_q: 20.254761\n",
            " 120937/150000: episode: 17621, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 6.000],  loss: 0.573363, mae: 13.895496, mean_q: 20.114662\n",
            " 120942/150000: episode: 17622, duration: 0.048s, episode steps:   5, steps per second: 105, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 8.000],  loss: 0.241966, mae: 14.371264, mean_q: 20.073439\n",
            " 120947/150000: episode: 17623, duration: 0.047s, episode steps:   5, steps per second: 106, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.442341, mae: 13.597208, mean_q: 20.193972\n",
            " 120953/150000: episode: 17624, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.408415, mae: 14.128055, mean_q: 20.100882\n",
            " 120962/150000: episode: 17625, duration: 0.081s, episode steps:   9, steps per second: 110, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.315341, mae: 13.793560, mean_q: 20.366276\n",
            " 120970/150000: episode: 17626, duration: 0.071s, episode steps:   8, steps per second: 112, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.250272, mae: 14.014198, mean_q: 19.924299\n",
            " 120973/150000: episode: 17627, duration: 0.032s, episode steps:   3, steps per second:  92, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 5.000 [3.000, 6.000],  loss: 1.014589, mae: 13.615959, mean_q: 20.011690\n",
            " 120978/150000: episode: 17628, duration: 0.061s, episode steps:   5, steps per second:  82, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.297673, mae: 13.484866, mean_q: 19.862906\n",
            " 120986/150000: episode: 17629, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.289207, mae: 13.994593, mean_q: 20.162399\n",
            " 120993/150000: episode: 17630, duration: 0.094s, episode steps:   7, steps per second:  74, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.320816, mae: 13.912086, mean_q: 20.094265\n",
            " 121002/150000: episode: 17631, duration: 0.138s, episode steps:   9, steps per second:  65, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.687422, mae: 13.834446, mean_q: 19.912046\n",
            " 121010/150000: episode: 17632, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.279526, mae: 13.945129, mean_q: 20.124989\n",
            " 121015/150000: episode: 17633, duration: 0.077s, episode steps:   5, steps per second:  65, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.253714, mae: 13.947245, mean_q: 20.202908\n",
            " 121021/150000: episode: 17634, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.833 [1.000, 7.000],  loss: 0.156035, mae: 14.022784, mean_q: 19.946913\n",
            " 121027/150000: episode: 17635, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.422255, mae: 13.867429, mean_q: 20.076822\n",
            " 121033/150000: episode: 17636, duration: 0.072s, episode steps:   6, steps per second:  84, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.667 [0.000, 8.000],  loss: 0.371850, mae: 13.918655, mean_q: 19.958761\n",
            " 121041/150000: episode: 17637, duration: 0.102s, episode steps:   8, steps per second:  79, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.265015, mae: 13.984324, mean_q: 20.127972\n",
            " 121047/150000: episode: 17638, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 8.000],  loss: 0.219580, mae: 13.883925, mean_q: 20.003931\n",
            " 121051/150000: episode: 17639, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 6.250 [5.000, 8.000],  loss: 0.338232, mae: 14.221172, mean_q: 20.035126\n",
            " 121058/150000: episode: 17640, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 7.000],  loss: 0.247576, mae: 13.813893, mean_q: 20.172270\n",
            " 121067/150000: episode: 17641, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.529219, mae: 13.967175, mean_q: 19.954515\n",
            " 121075/150000: episode: 17642, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.331220, mae: 13.916587, mean_q: 20.153728\n",
            " 121083/150000: episode: 17643, duration: 0.116s, episode steps:   8, steps per second:  69, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.435158, mae: 13.651546, mean_q: 20.012852\n",
            " 121090/150000: episode: 17644, duration: 0.103s, episode steps:   7, steps per second:  68, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.333864, mae: 13.819457, mean_q: 19.949183\n",
            " 121099/150000: episode: 17645, duration: 0.135s, episode steps:   9, steps per second:  67, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.235878, mae: 13.888906, mean_q: 19.935762\n",
            " 121107/150000: episode: 17646, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.250 [0.000, 8.000],  loss: 0.379181, mae: 13.945032, mean_q: 20.245659\n",
            " 121112/150000: episode: 17647, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 6.000 [2.000, 8.000],  loss: 0.329142, mae: 13.996111, mean_q: 20.183283\n",
            " 121118/150000: episode: 17648, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.000 [0.000, 8.000],  loss: 0.709025, mae: 13.716119, mean_q: 20.112093\n",
            " 121126/150000: episode: 17649, duration: 0.121s, episode steps:   8, steps per second:  66, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.427439, mae: 13.735954, mean_q: 20.060955\n",
            " 121135/150000: episode: 17650, duration: 0.126s, episode steps:   9, steps per second:  72, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.370077, mae: 13.995135, mean_q: 19.973896\n",
            " 121144/150000: episode: 17651, duration: 0.140s, episode steps:   9, steps per second:  64, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.883788, mae: 13.973332, mean_q: 19.884018\n",
            " 121153/150000: episode: 17652, duration: 0.143s, episode steps:   9, steps per second:  63, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.271345, mae: 13.775951, mean_q: 20.295597\n",
            " 121160/150000: episode: 17653, duration: 0.117s, episode steps:   7, steps per second:  60, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.859083, mae: 13.891459, mean_q: 19.849548\n",
            " 121167/150000: episode: 17654, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.308547, mae: 14.070463, mean_q: 20.039387\n",
            " 121171/150000: episode: 17655, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.456386, mae: 14.124166, mean_q: 20.438858\n",
            " 121179/150000: episode: 17656, duration: 0.128s, episode steps:   8, steps per second:  63, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.431699, mae: 14.164690, mean_q: 20.115128\n",
            " 121187/150000: episode: 17657, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.278841, mae: 13.896082, mean_q: 19.936483\n",
            " 121196/150000: episode: 17658, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.236273, mae: 14.059925, mean_q: 19.982206\n",
            " 121201/150000: episode: 17659, duration: 0.081s, episode steps:   5, steps per second:  62, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 8.000],  loss: 0.766173, mae: 13.758604, mean_q: 19.840675\n",
            " 121207/150000: episode: 17660, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.390534, mae: 13.933894, mean_q: 20.021492\n",
            " 121213/150000: episode: 17661, duration: 0.087s, episode steps:   6, steps per second:  69, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.199442, mae: 13.660809, mean_q: 20.072470\n",
            " 121219/150000: episode: 17662, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.000 [0.000, 8.000],  loss: 0.585970, mae: 13.907943, mean_q: 20.009970\n",
            " 121226/150000: episode: 17663, duration: 0.134s, episode steps:   7, steps per second:  52, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.180665, mae: 14.195722, mean_q: 20.010706\n",
            " 121234/150000: episode: 17664, duration: 0.117s, episode steps:   8, steps per second:  68, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.652606, mae: 13.764419, mean_q: 20.035694\n",
            " 121243/150000: episode: 17665, duration: 0.137s, episode steps:   9, steps per second:  66, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.413054, mae: 13.595983, mean_q: 19.969582\n",
            " 121247/150000: episode: 17666, duration: 0.050s, episode steps:   4, steps per second:  81, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 2.500 [0.000, 6.000],  loss: 0.322353, mae: 14.082990, mean_q: 20.660181\n",
            " 121256/150000: episode: 17667, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.261975, mae: 13.825480, mean_q: 20.100349\n",
            " 121264/150000: episode: 17668, duration: 0.069s, episode steps:   8, steps per second: 116, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.453649, mae: 13.913614, mean_q: 20.073400\n",
            " 121271/150000: episode: 17669, duration: 0.072s, episode steps:   7, steps per second:  97, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.616552, mae: 13.789449, mean_q: 19.821161\n",
            " 121280/150000: episode: 17670, duration: 0.076s, episode steps:   9, steps per second: 118, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.252236, mae: 14.008572, mean_q: 20.095778\n",
            " 121289/150000: episode: 17671, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.402283, mae: 13.907579, mean_q: 20.025030\n",
            " 121294/150000: episode: 17672, duration: 0.048s, episode steps:   5, steps per second: 104, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.252583, mae: 13.839888, mean_q: 20.132366\n",
            " 121303/150000: episode: 17673, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.446443, mae: 13.855939, mean_q: 20.086174\n",
            " 121312/150000: episode: 17674, duration: 0.080s, episode steps:   9, steps per second: 112, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.493695, mae: 14.048969, mean_q: 20.105574\n",
            " 121321/150000: episode: 17675, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.536582, mae: 13.944867, mean_q: 19.976736\n",
            " 121326/150000: episode: 17676, duration: 0.052s, episode steps:   5, steps per second:  97, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.200 [0.000, 8.000],  loss: 0.251388, mae: 14.028738, mean_q: 20.180241\n",
            " 121332/150000: episode: 17677, duration: 0.065s, episode steps:   6, steps per second:  93, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.000 [0.000, 8.000],  loss: 0.465484, mae: 13.862992, mean_q: 20.046659\n",
            " 121341/150000: episode: 17678, duration: 0.082s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.419455, mae: 14.013395, mean_q: 20.062401\n",
            " 121346/150000: episode: 17679, duration: 0.048s, episode steps:   5, steps per second: 104, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.200 [0.000, 8.000],  loss: 0.311356, mae: 13.957077, mean_q: 20.036734\n",
            " 121352/150000: episode: 17680, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.333 [1.000, 8.000],  loss: 0.464288, mae: 13.871793, mean_q: 20.040457\n",
            " 121359/150000: episode: 17681, duration: 0.081s, episode steps:   7, steps per second:  86, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.538257, mae: 14.056459, mean_q: 20.258654\n",
            " 121366/150000: episode: 17682, duration: 0.067s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.507617, mae: 13.500786, mean_q: 19.960802\n",
            " 121375/150000: episode: 17683, duration: 0.085s, episode steps:   9, steps per second: 105, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.331159, mae: 13.919931, mean_q: 20.162188\n",
            " 121382/150000: episode: 17684, duration: 0.075s, episode steps:   7, steps per second:  93, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [1.000, 8.000],  loss: 0.305666, mae: 14.065730, mean_q: 19.995251\n",
            " 121389/150000: episode: 17685, duration: 0.067s, episode steps:   7, steps per second: 104, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.143 [0.000, 8.000],  loss: 0.242529, mae: 14.228173, mean_q: 19.970819\n",
            " 121398/150000: episode: 17686, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.215697, mae: 13.876068, mean_q: 19.996307\n",
            " 121405/150000: episode: 17687, duration: 0.073s, episode steps:   7, steps per second:  96, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.282802, mae: 13.932455, mean_q: 20.021969\n",
            " 121407/150000: episode: 17688, duration: 0.030s, episode steps:   2, steps per second:  67, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 3.000 [3.000, 3.000],  loss: 0.459889, mae: 13.878023, mean_q: 19.742601\n",
            " 121415/150000: episode: 17689, duration: 0.080s, episode steps:   8, steps per second: 101, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.375 [0.000, 7.000],  loss: 0.436861, mae: 13.987950, mean_q: 19.989620\n",
            " 121421/150000: episode: 17690, duration: 0.066s, episode steps:   6, steps per second:  91, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.357503, mae: 13.907514, mean_q: 19.969908\n",
            " 121430/150000: episode: 17691, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.723640, mae: 14.296051, mean_q: 20.210325\n",
            " 121439/150000: episode: 17692, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.433867, mae: 13.964913, mean_q: 19.867838\n",
            " 121448/150000: episode: 17693, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.342307, mae: 13.852946, mean_q: 20.050735\n",
            " 121456/150000: episode: 17694, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.875 [1.000, 8.000],  loss: 0.347887, mae: 14.027572, mean_q: 20.158821\n",
            " 121462/150000: episode: 17695, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.667 [0.000, 8.000],  loss: 0.323002, mae: 13.504163, mean_q: 20.045149\n",
            " 121468/150000: episode: 17696, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.448714, mae: 13.449433, mean_q: 20.132227\n",
            " 121473/150000: episode: 17697, duration: 0.048s, episode steps:   5, steps per second: 103, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.303514, mae: 14.208328, mean_q: 20.337011\n",
            " 121478/150000: episode: 17698, duration: 0.055s, episode steps:   5, steps per second:  91, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 8.000],  loss: 0.490571, mae: 13.701952, mean_q: 20.071102\n",
            " 121484/150000: episode: 17699, duration: 0.062s, episode steps:   6, steps per second:  97, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.524653, mae: 13.554122, mean_q: 19.773083\n",
            " 121491/150000: episode: 17700, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.422023, mae: 13.912743, mean_q: 20.239201\n",
            " 121497/150000: episode: 17701, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [2.000, 8.000],  loss: 0.334049, mae: 13.960851, mean_q: 20.153193\n",
            " 121505/150000: episode: 17702, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.327976, mae: 13.758588, mean_q: 20.056313\n",
            " 121513/150000: episode: 17703, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.268098, mae: 14.057440, mean_q: 20.111404\n",
            " 121518/150000: episode: 17704, duration: 0.065s, episode steps:   5, steps per second:  77, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.000 [0.000, 5.000],  loss: 0.326211, mae: 14.136579, mean_q: 20.038761\n",
            " 121526/150000: episode: 17705, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.276421, mae: 13.764380, mean_q: 19.997152\n",
            " 121534/150000: episode: 17706, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.230359, mae: 13.905911, mean_q: 20.022705\n",
            " 121542/150000: episode: 17707, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.557678, mae: 14.139879, mean_q: 20.061317\n",
            " 121547/150000: episode: 17708, duration: 0.047s, episode steps:   5, steps per second: 105, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.400 [0.000, 6.000],  loss: 0.388031, mae: 13.665709, mean_q: 19.903606\n",
            " 121552/150000: episode: 17709, duration: 0.062s, episode steps:   5, steps per second:  80, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.526798, mae: 14.121401, mean_q: 20.347828\n",
            " 121558/150000: episode: 17710, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.339212, mae: 13.589366, mean_q: 19.984858\n",
            " 121564/150000: episode: 17711, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.384130, mae: 13.987183, mean_q: 19.998018\n",
            " 121569/150000: episode: 17712, duration: 0.050s, episode steps:   5, steps per second: 101, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.200 [2.000, 8.000],  loss: 0.543007, mae: 14.074452, mean_q: 20.084587\n",
            " 121576/150000: episode: 17713, duration: 0.075s, episode steps:   7, steps per second:  93, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.240495, mae: 13.773702, mean_q: 20.037981\n",
            " 121581/150000: episode: 17714, duration: 0.054s, episode steps:   5, steps per second:  93, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 1.800 [0.000, 6.000],  loss: 0.242635, mae: 13.973392, mean_q: 20.195530\n",
            " 121590/150000: episode: 17715, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.234923, mae: 13.979112, mean_q: 20.076107\n",
            " 121598/150000: episode: 17716, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.483948, mae: 13.880154, mean_q: 19.778189\n",
            " 121604/150000: episode: 17717, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 6.000],  loss: 0.240866, mae: 13.889497, mean_q: 20.255682\n",
            " 121611/150000: episode: 17718, duration: 0.063s, episode steps:   7, steps per second: 111, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.357398, mae: 13.941075, mean_q: 20.343153\n",
            " 121617/150000: episode: 17719, duration: 0.060s, episode steps:   6, steps per second:  99, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 8.000],  loss: 0.291331, mae: 13.315339, mean_q: 19.889502\n",
            " 121623/150000: episode: 17720, duration: 0.074s, episode steps:   6, steps per second:  81, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.257186, mae: 13.488704, mean_q: 20.196756\n",
            " 121630/150000: episode: 17721, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.315729, mae: 13.913659, mean_q: 19.893280\n",
            " 121635/150000: episode: 17722, duration: 0.045s, episode steps:   5, steps per second: 111, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.200 [0.000, 8.000],  loss: 0.205943, mae: 14.017650, mean_q: 20.081228\n",
            " 121643/150000: episode: 17723, duration: 0.070s, episode steps:   8, steps per second: 114, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.844338, mae: 14.023914, mean_q: 19.968113\n",
            " 121651/150000: episode: 17724, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.219339, mae: 14.013744, mean_q: 19.807131\n",
            " 121658/150000: episode: 17725, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.393042, mae: 14.372107, mean_q: 20.146374\n",
            " 121664/150000: episode: 17726, duration: 0.056s, episode steps:   6, steps per second: 108, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [0.000, 8.000],  loss: 0.497591, mae: 14.067863, mean_q: 19.817060\n",
            " 121671/150000: episode: 17727, duration: 0.061s, episode steps:   7, steps per second: 116, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.571 [0.000, 7.000],  loss: 0.223207, mae: 13.683858, mean_q: 20.122740\n",
            " 121677/150000: episode: 17728, duration: 0.070s, episode steps:   6, steps per second:  86, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [1.000, 8.000],  loss: 0.466385, mae: 14.047582, mean_q: 20.154316\n",
            " 121685/150000: episode: 17729, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.500 [0.000, 8.000],  loss: 0.268435, mae: 13.861101, mean_q: 20.049578\n",
            " 121690/150000: episode: 17730, duration: 0.048s, episode steps:   5, steps per second: 104, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [1.000, 8.000],  loss: 0.339500, mae: 13.806662, mean_q: 19.922161\n",
            " 121695/150000: episode: 17731, duration: 0.049s, episode steps:   5, steps per second: 102, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.800 [0.000, 5.000],  loss: 0.259320, mae: 13.578829, mean_q: 20.029812\n",
            " 121702/150000: episode: 17732, duration: 0.078s, episode steps:   7, steps per second:  90, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 7.000],  loss: 0.371559, mae: 13.851686, mean_q: 20.097082\n",
            " 121708/150000: episode: 17733, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.167 [1.000, 7.000],  loss: 0.264883, mae: 13.853009, mean_q: 20.016582\n",
            " 121716/150000: episode: 17734, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.567825, mae: 14.214361, mean_q: 20.141518\n",
            " 121722/150000: episode: 17735, duration: 0.053s, episode steps:   6, steps per second: 114, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.425620, mae: 13.899474, mean_q: 20.077566\n",
            " 121730/150000: episode: 17736, duration: 0.102s, episode steps:   8, steps per second:  79, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.404166, mae: 14.046393, mean_q: 20.053177\n",
            " 121739/150000: episode: 17737, duration: 0.077s, episode steps:   9, steps per second: 118, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.191280, mae: 14.016206, mean_q: 20.137995\n",
            " 121746/150000: episode: 17738, duration: 0.062s, episode steps:   7, steps per second: 113, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.323930, mae: 14.140185, mean_q: 20.003229\n",
            " 121755/150000: episode: 17739, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.471508, mae: 14.051169, mean_q: 19.881006\n",
            " 121763/150000: episode: 17740, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.400194, mae: 14.053885, mean_q: 20.162725\n",
            " 121770/150000: episode: 17741, duration: 0.060s, episode steps:   7, steps per second: 117, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.286821, mae: 13.854601, mean_q: 19.884304\n",
            " 121775/150000: episode: 17742, duration: 0.051s, episode steps:   5, steps per second:  98, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 6.400 [3.000, 8.000],  loss: 0.187582, mae: 13.868988, mean_q: 20.046734\n",
            " 121784/150000: episode: 17743, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.283814, mae: 14.046173, mean_q: 20.005949\n",
            " 121793/150000: episode: 17744, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.889 [0.000, 8.000],  loss: 0.324573, mae: 14.164913, mean_q: 20.043646\n",
            " 121799/150000: episode: 17745, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.167 [2.000, 8.000],  loss: 0.581582, mae: 13.868371, mean_q: 19.997019\n",
            " 121805/150000: episode: 17746, duration: 0.052s, episode steps:   6, steps per second: 116, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.241799, mae: 13.938029, mean_q: 19.902803\n",
            " 121814/150000: episode: 17747, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.216765, mae: 13.986054, mean_q: 20.198151\n",
            " 121822/150000: episode: 17748, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.628649, mae: 13.847829, mean_q: 19.929321\n",
            " 121831/150000: episode: 17749, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.324253, mae: 13.772790, mean_q: 19.912544\n",
            " 121840/150000: episode: 17750, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.273824, mae: 14.182880, mean_q: 20.094452\n",
            " 121847/150000: episode: 17751, duration: 0.066s, episode steps:   7, steps per second: 107, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.857 [0.000, 8.000],  loss: 0.287785, mae: 13.887126, mean_q: 20.020542\n",
            " 121854/150000: episode: 17752, duration: 0.062s, episode steps:   7, steps per second: 113, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.259400, mae: 13.871549, mean_q: 20.004765\n",
            " 121860/150000: episode: 17753, duration: 0.055s, episode steps:   6, steps per second: 108, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [1.000, 8.000],  loss: 0.282355, mae: 13.744915, mean_q: 19.929438\n",
            " 121868/150000: episode: 17754, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.274848, mae: 14.049741, mean_q: 20.047234\n",
            " 121874/150000: episode: 17755, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 6.000],  loss: 0.382453, mae: 13.515691, mean_q: 19.833036\n",
            " 121881/150000: episode: 17756, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.527980, mae: 14.013572, mean_q: 20.227581\n",
            " 121889/150000: episode: 17757, duration: 0.074s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.392032, mae: 14.065485, mean_q: 19.857576\n",
            " 121898/150000: episode: 17758, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.435743, mae: 13.809530, mean_q: 20.055801\n",
            " 121906/150000: episode: 17759, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.285019, mae: 13.599641, mean_q: 20.070234\n",
            " 121912/150000: episode: 17760, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.311838, mae: 14.152893, mean_q: 19.825655\n",
            " 121919/150000: episode: 17761, duration: 0.075s, episode steps:   7, steps per second:  94, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.408181, mae: 13.713872, mean_q: 20.067739\n",
            " 121927/150000: episode: 17762, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.613463, mae: 13.818562, mean_q: 20.119133\n",
            " 121933/150000: episode: 17763, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.561705, mae: 13.992139, mean_q: 19.897673\n",
            " 121937/150000: episode: 17764, duration: 0.050s, episode steps:   4, steps per second:  80, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 1.750 [0.000, 5.000],  loss: 0.314362, mae: 13.603481, mean_q: 20.094009\n",
            " 121944/150000: episode: 17765, duration: 0.076s, episode steps:   7, steps per second:  92, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.440497, mae: 13.997031, mean_q: 20.023281\n",
            " 121951/150000: episode: 17766, duration: 0.063s, episode steps:   7, steps per second: 112, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.460887, mae: 13.689955, mean_q: 20.140564\n",
            " 121960/150000: episode: 17767, duration: 0.077s, episode steps:   9, steps per second: 117, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.693690, mae: 13.580334, mean_q: 19.952621\n",
            " 121968/150000: episode: 17768, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.422660, mae: 14.106909, mean_q: 20.276804\n",
            " 121973/150000: episode: 17769, duration: 0.051s, episode steps:   5, steps per second:  98, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.200 [3.000, 7.000],  loss: 0.573540, mae: 13.592260, mean_q: 19.807430\n",
            " 121981/150000: episode: 17770, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.317454, mae: 13.790565, mean_q: 19.988831\n",
            " 121989/150000: episode: 17771, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.221826, mae: 13.746810, mean_q: 20.085102\n",
            " 121995/150000: episode: 17772, duration: 0.064s, episode steps:   6, steps per second:  93, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.167 [1.000, 7.000],  loss: 0.732632, mae: 13.597000, mean_q: 20.040331\n",
            " 122000/150000: episode: 17773, duration: 0.049s, episode steps:   5, steps per second: 102, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.800 [3.000, 8.000],  loss: 0.420446, mae: 13.850779, mean_q: 19.957739\n",
            " 122006/150000: episode: 17774, duration: 0.054s, episode steps:   6, steps per second: 110, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.625422, mae: 13.868726, mean_q: 19.896502\n",
            " 122014/150000: episode: 17775, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.750 [0.000, 8.000],  loss: 0.700093, mae: 13.852777, mean_q: 20.327629\n",
            " 122022/150000: episode: 17776, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.448945, mae: 13.948790, mean_q: 19.781637\n",
            " 122029/150000: episode: 17777, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [1.000, 8.000],  loss: 0.645911, mae: 13.839929, mean_q: 19.998764\n",
            " 122036/150000: episode: 17778, duration: 0.069s, episode steps:   7, steps per second: 101, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.242773, mae: 13.803993, mean_q: 20.110792\n",
            " 122043/150000: episode: 17779, duration: 0.082s, episode steps:   7, steps per second:  86, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.000 [0.000, 7.000],  loss: 0.421182, mae: 13.607141, mean_q: 20.088867\n",
            " 122048/150000: episode: 17780, duration: 0.051s, episode steps:   5, steps per second:  98, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.400 [2.000, 7.000],  loss: 0.468271, mae: 13.659009, mean_q: 20.045235\n",
            " 122056/150000: episode: 17781, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.722029, mae: 13.641706, mean_q: 19.773808\n",
            " 122062/150000: episode: 17782, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.268877, mae: 13.837914, mean_q: 20.027105\n",
            " 122069/150000: episode: 17783, duration: 0.075s, episode steps:   7, steps per second:  93, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.642458, mae: 13.835288, mean_q: 20.134996\n",
            " 122074/150000: episode: 17784, duration: 0.050s, episode steps:   5, steps per second:  99, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.553404, mae: 14.003548, mean_q: 19.836191\n",
            " 122082/150000: episode: 17785, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.500 [0.000, 8.000],  loss: 0.372287, mae: 13.894156, mean_q: 20.142673\n",
            " 122088/150000: episode: 17786, duration: 0.066s, episode steps:   6, steps per second:  91, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.667 [2.000, 8.000],  loss: 0.287156, mae: 13.975609, mean_q: 20.155870\n",
            " 122090/150000: episode: 17787, duration: 0.041s, episode steps:   2, steps per second:  49, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.280216, mae: 14.139425, mean_q: 19.776295\n",
            " 122096/150000: episode: 17788, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [2.000, 7.000],  loss: 0.295694, mae: 13.911572, mean_q: 20.027943\n",
            " 122105/150000: episode: 17789, duration: 0.080s, episode steps:   9, steps per second: 112, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.561950, mae: 13.899691, mean_q: 20.032814\n",
            " 122110/150000: episode: 17790, duration: 0.048s, episode steps:   5, steps per second: 104, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.600 [0.000, 6.000],  loss: 0.318806, mae: 13.960554, mean_q: 20.064728\n",
            " 122116/150000: episode: 17791, duration: 0.067s, episode steps:   6, steps per second:  89, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.167 [0.000, 7.000],  loss: 0.762621, mae: 13.909045, mean_q: 20.068369\n",
            " 122122/150000: episode: 17792, duration: 0.062s, episode steps:   6, steps per second:  97, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 7.000],  loss: 0.269340, mae: 13.950351, mean_q: 20.002323\n",
            " 122131/150000: episode: 17793, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.577106, mae: 13.967459, mean_q: 20.148119\n",
            " 122140/150000: episode: 17794, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.352364, mae: 13.685266, mean_q: 19.868088\n",
            " 122146/150000: episode: 17795, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.333 [1.000, 7.000],  loss: 0.365876, mae: 13.368960, mean_q: 20.260170\n",
            " 122152/150000: episode: 17796, duration: 0.053s, episode steps:   6, steps per second: 113, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.353861, mae: 14.066597, mean_q: 20.139357\n",
            " 122161/150000: episode: 17797, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.368805, mae: 13.757151, mean_q: 20.141802\n",
            " 122168/150000: episode: 17798, duration: 0.074s, episode steps:   7, steps per second:  94, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.296119, mae: 14.151587, mean_q: 20.068928\n",
            " 122175/150000: episode: 17799, duration: 0.061s, episode steps:   7, steps per second: 115, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.234614, mae: 13.618631, mean_q: 19.997440\n",
            " 122184/150000: episode: 17800, duration: 0.077s, episode steps:   9, steps per second: 117, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.460403, mae: 13.726231, mean_q: 19.911901\n",
            " 122192/150000: episode: 17801, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.244037, mae: 14.136093, mean_q: 20.119972\n",
            " 122195/150000: episode: 17802, duration: 0.036s, episode steps:   3, steps per second:  84, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 3.000 [2.000, 5.000],  loss: 0.466343, mae: 14.018120, mean_q: 20.055189\n",
            " 122204/150000: episode: 17803, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.365548, mae: 14.063829, mean_q: 19.926079\n",
            " 122211/150000: episode: 17804, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.286 [1.000, 8.000],  loss: 0.221042, mae: 13.707170, mean_q: 20.041134\n",
            " 122217/150000: episode: 17805, duration: 0.069s, episode steps:   6, steps per second:  86, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2.667 [0.000, 6.000],  loss: 0.271651, mae: 13.970227, mean_q: 19.930523\n",
            " 122224/150000: episode: 17806, duration: 0.068s, episode steps:   7, steps per second: 103, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.499259, mae: 13.888624, mean_q: 19.794144\n",
            " 122231/150000: episode: 17807, duration: 0.060s, episode steps:   7, steps per second: 117, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.407845, mae: 13.901696, mean_q: 20.058157\n",
            " 122234/150000: episode: 17808, duration: 0.034s, episode steps:   3, steps per second:  89, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.260902, mae: 13.921519, mean_q: 20.313055\n",
            " 122240/150000: episode: 17809, duration: 0.071s, episode steps:   6, steps per second:  85, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 8.000],  loss: 0.354471, mae: 14.097590, mean_q: 20.033281\n",
            " 122246/150000: episode: 17810, duration: 0.065s, episode steps:   6, steps per second:  93, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.167 [0.000, 7.000],  loss: 0.212694, mae: 13.697483, mean_q: 20.016981\n",
            " 122250/150000: episode: 17811, duration: 0.043s, episode steps:   4, steps per second:  94, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.250 [2.000, 7.000],  loss: 0.641490, mae: 14.062917, mean_q: 20.065279\n",
            " 122258/150000: episode: 17812, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.245933, mae: 13.536988, mean_q: 19.938065\n",
            " 122263/150000: episode: 17813, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.000 [1.000, 8.000],  loss: 0.583701, mae: 13.252986, mean_q: 19.892292\n",
            " 122268/150000: episode: 17814, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 8.000],  loss: 0.295113, mae: 14.008146, mean_q: 19.741589\n",
            " 122276/150000: episode: 17815, duration: 0.135s, episode steps:   8, steps per second:  59, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.257584, mae: 13.743116, mean_q: 20.096663\n",
            " 122283/150000: episode: 17816, duration: 0.101s, episode steps:   7, steps per second:  69, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.417920, mae: 13.914316, mean_q: 19.984394\n",
            " 122287/150000: episode: 17817, duration: 0.056s, episode steps:   4, steps per second:  72, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.250 [2.000, 7.000],  loss: 0.149895, mae: 13.998427, mean_q: 20.022144\n",
            " 122292/150000: episode: 17818, duration: 0.085s, episode steps:   5, steps per second:  59, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.200 [0.000, 8.000],  loss: 0.487904, mae: 14.146036, mean_q: 19.950672\n",
            " 122301/150000: episode: 17819, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.360381, mae: 13.653239, mean_q: 19.877604\n",
            " 122306/150000: episode: 17820, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.272680, mae: 13.701612, mean_q: 19.867588\n",
            " 122313/150000: episode: 17821, duration: 0.104s, episode steps:   7, steps per second:  67, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.237280, mae: 13.798820, mean_q: 19.920042\n",
            " 122319/150000: episode: 17822, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 7.000],  loss: 0.410398, mae: 13.969934, mean_q: 19.919983\n",
            " 122325/150000: episode: 17823, duration: 0.076s, episode steps:   6, steps per second:  79, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [1.000, 7.000],  loss: 0.274930, mae: 14.206107, mean_q: 20.055803\n",
            " 122330/150000: episode: 17824, duration: 0.077s, episode steps:   5, steps per second:  65, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.400 [0.000, 7.000],  loss: 0.222247, mae: 13.942057, mean_q: 20.124912\n",
            " 122339/150000: episode: 17825, duration: 0.112s, episode steps:   9, steps per second:  81, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.215136, mae: 13.867424, mean_q: 19.952305\n",
            " 122345/150000: episode: 17826, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.667 [0.000, 8.000],  loss: 0.141088, mae: 14.402714, mean_q: 20.084967\n",
            " 122351/150000: episode: 17827, duration: 0.105s, episode steps:   6, steps per second:  57, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.381742, mae: 13.571479, mean_q: 19.900560\n",
            " 122359/150000: episode: 17828, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.250 [0.000, 8.000],  loss: 0.231955, mae: 13.875196, mean_q: 19.978647\n",
            " 122366/150000: episode: 17829, duration: 0.083s, episode steps:   7, steps per second:  85, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.571 [0.000, 8.000],  loss: 0.225406, mae: 13.877558, mean_q: 20.020292\n",
            " 122374/150000: episode: 17830, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.180377, mae: 13.889943, mean_q: 19.911398\n",
            " 122380/150000: episode: 17831, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.500 [0.000, 8.000],  loss: 0.488498, mae: 14.111108, mean_q: 19.827713\n",
            " 122386/150000: episode: 17832, duration: 0.086s, episode steps:   6, steps per second:  70, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.167 [0.000, 7.000],  loss: 0.717234, mae: 13.879860, mean_q: 20.114016\n",
            " 122392/150000: episode: 17833, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [1.000, 7.000],  loss: 0.507375, mae: 13.973163, mean_q: 20.226097\n",
            " 122401/150000: episode: 17834, duration: 0.124s, episode steps:   9, steps per second:  72, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.472989, mae: 14.261773, mean_q: 20.300224\n",
            " 122408/150000: episode: 17835, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.455094, mae: 13.739676, mean_q: 20.084078\n",
            " 122417/150000: episode: 17836, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.581142, mae: 13.907082, mean_q: 20.184725\n",
            " 122423/150000: episode: 17837, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 7.000],  loss: 0.222594, mae: 14.059398, mean_q: 19.929722\n",
            " 122430/150000: episode: 17838, duration: 0.095s, episode steps:   7, steps per second:  73, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.243458, mae: 13.881904, mean_q: 20.261457\n",
            " 122438/150000: episode: 17839, duration: 0.115s, episode steps:   8, steps per second:  69, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.346262, mae: 13.913166, mean_q: 20.164993\n",
            " 122446/150000: episode: 17840, duration: 0.116s, episode steps:   8, steps per second:  69, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.364504, mae: 13.883344, mean_q: 20.245514\n",
            " 122453/150000: episode: 17841, duration: 0.101s, episode steps:   7, steps per second:  69, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.307782, mae: 13.856713, mean_q: 20.031269\n",
            " 122461/150000: episode: 17842, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.125 [1.000, 8.000],  loss: 0.678387, mae: 13.630236, mean_q: 19.850433\n",
            " 122467/150000: episode: 17843, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.220290, mae: 14.159103, mean_q: 20.083336\n",
            " 122475/150000: episode: 17844, duration: 0.119s, episode steps:   8, steps per second:  67, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.250 [0.000, 7.000],  loss: 0.435381, mae: 13.530703, mean_q: 20.172482\n",
            " 122483/150000: episode: 17845, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.345729, mae: 13.805536, mean_q: 20.053123\n",
            " 122488/150000: episode: 17846, duration: 0.065s, episode steps:   5, steps per second:  77, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.181394, mae: 14.047229, mean_q: 20.017822\n",
            " 122496/150000: episode: 17847, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.750 [0.000, 7.000],  loss: 0.244873, mae: 13.875433, mean_q: 19.970661\n",
            " 122503/150000: episode: 17848, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.280882, mae: 13.950409, mean_q: 20.047230\n",
            " 122509/150000: episode: 17849, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.518004, mae: 13.616740, mean_q: 19.818399\n",
            " 122516/150000: episode: 17850, duration: 0.109s, episode steps:   7, steps per second:  64, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.430081, mae: 14.026269, mean_q: 20.241079\n",
            " 122522/150000: episode: 17851, duration: 0.084s, episode steps:   6, steps per second:  71, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 6.000],  loss: 0.215261, mae: 13.899193, mean_q: 20.144783\n",
            " 122530/150000: episode: 17852, duration: 0.096s, episode steps:   8, steps per second:  84, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.383445, mae: 14.172336, mean_q: 19.827448\n",
            " 122538/150000: episode: 17853, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.448361, mae: 13.738250, mean_q: 20.021488\n",
            " 122544/150000: episode: 17854, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 6.000],  loss: 0.447047, mae: 13.769787, mean_q: 20.254404\n",
            " 122551/150000: episode: 17855, duration: 0.062s, episode steps:   7, steps per second: 113, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.857 [0.000, 8.000],  loss: 0.268594, mae: 14.041552, mean_q: 19.971159\n",
            " 122554/150000: episode: 17856, duration: 0.032s, episode steps:   3, steps per second:  93, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 5.667 [5.000, 7.000],  loss: 1.025011, mae: 13.813436, mean_q: 19.735964\n",
            " 122560/150000: episode: 17857, duration: 0.068s, episode steps:   6, steps per second:  88, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.383734, mae: 13.961288, mean_q: 19.795053\n",
            " 122567/150000: episode: 17858, duration: 0.082s, episode steps:   7, steps per second:  85, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.366813, mae: 14.142816, mean_q: 20.174738\n",
            " 122573/150000: episode: 17859, duration: 0.052s, episode steps:   6, steps per second: 116, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.266346, mae: 14.124949, mean_q: 20.007767\n",
            " 122581/150000: episode: 17860, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.429891, mae: 13.889498, mean_q: 19.933174\n",
            " 122587/150000: episode: 17861, duration: 0.067s, episode steps:   6, steps per second:  90, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [1.000, 8.000],  loss: 0.206460, mae: 13.638073, mean_q: 20.036608\n",
            " 122592/150000: episode: 17862, duration: 0.047s, episode steps:   5, steps per second: 107, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [1.000, 8.000],  loss: 0.302108, mae: 13.938162, mean_q: 20.026737\n",
            " 122599/150000: episode: 17863, duration: 0.062s, episode steps:   7, steps per second: 113, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.364736, mae: 14.074462, mean_q: 20.123106\n",
            " 122606/150000: episode: 17864, duration: 0.080s, episode steps:   7, steps per second:  87, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.449285, mae: 13.919523, mean_q: 20.038824\n",
            " 122615/150000: episode: 17865, duration: 0.079s, episode steps:   9, steps per second: 115, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.315312, mae: 13.747561, mean_q: 20.122574\n",
            " 122624/150000: episode: 17866, duration: 0.076s, episode steps:   9, steps per second: 119, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.302266, mae: 13.981894, mean_q: 20.005064\n",
            " 122633/150000: episode: 17867, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.280404, mae: 13.958386, mean_q: 20.073265\n",
            " 122641/150000: episode: 17868, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.493609, mae: 14.000374, mean_q: 19.940638\n",
            " 122649/150000: episode: 17869, duration: 0.070s, episode steps:   8, steps per second: 115, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.191806, mae: 13.932395, mean_q: 20.005463\n",
            " 122653/150000: episode: 17870, duration: 0.039s, episode steps:   4, steps per second: 102, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.223046, mae: 13.537539, mean_q: 20.009495\n",
            " 122660/150000: episode: 17871, duration: 0.073s, episode steps:   7, steps per second:  96, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 7.000],  loss: 0.629456, mae: 13.703880, mean_q: 19.838333\n",
            " 122669/150000: episode: 17872, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.248980, mae: 14.026607, mean_q: 20.177364\n",
            " 122678/150000: episode: 17873, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.234122, mae: 14.012556, mean_q: 19.976120\n",
            " 122686/150000: episode: 17874, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.250 [1.000, 8.000],  loss: 0.449113, mae: 13.804976, mean_q: 19.910595\n",
            " 122693/150000: episode: 17875, duration: 0.063s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.308068, mae: 14.008928, mean_q: 20.124899\n",
            " 122698/150000: episode: 17876, duration: 0.044s, episode steps:   5, steps per second: 114, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.800 [2.000, 7.000],  loss: 0.293139, mae: 14.125560, mean_q: 20.177946\n",
            " 122706/150000: episode: 17877, duration: 0.068s, episode steps:   8, steps per second: 117, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.624061, mae: 13.852646, mean_q: 19.904423\n",
            " 122714/150000: episode: 17878, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.444145, mae: 14.033782, mean_q: 19.934711\n",
            " 122722/150000: episode: 17879, duration: 0.074s, episode steps:   8, steps per second: 109, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.875 [0.000, 8.000],  loss: 0.514601, mae: 13.882790, mean_q: 19.990536\n",
            " 122730/150000: episode: 17880, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.250 [0.000, 8.000],  loss: 0.308120, mae: 13.938812, mean_q: 20.044989\n",
            " 122738/150000: episode: 17881, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.441988, mae: 13.837333, mean_q: 20.202267\n",
            " 122742/150000: episode: 17882, duration: 0.041s, episode steps:   4, steps per second:  98, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.500 [2.000, 7.000],  loss: 0.926715, mae: 13.921793, mean_q: 20.083546\n",
            " 122750/150000: episode: 17883, duration: 0.074s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.298265, mae: 13.859055, mean_q: 20.190731\n",
            " 122757/150000: episode: 17884, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.398833, mae: 13.867445, mean_q: 20.042482\n",
            " 122762/150000: episode: 17885, duration: 0.059s, episode steps:   5, steps per second:  84, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.384682, mae: 13.801682, mean_q: 20.091312\n",
            " 122769/150000: episode: 17886, duration: 0.063s, episode steps:   7, steps per second: 112, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.429 [1.000, 8.000],  loss: 0.770840, mae: 13.887022, mean_q: 19.825533\n",
            " 122775/150000: episode: 17887, duration: 0.064s, episode steps:   6, steps per second:  94, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.408105, mae: 13.837174, mean_q: 19.961176\n",
            " 122783/150000: episode: 17888, duration: 0.071s, episode steps:   8, steps per second: 112, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.344074, mae: 14.326660, mean_q: 20.368866\n",
            " 122790/150000: episode: 17889, duration: 0.079s, episode steps:   7, steps per second:  89, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 7.000],  loss: 0.269896, mae: 13.963099, mean_q: 20.023783\n",
            " 122799/150000: episode: 17890, duration: 0.082s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.606678, mae: 13.949924, mean_q: 20.138437\n",
            " 122808/150000: episode: 17891, duration: 0.081s, episode steps:   9, steps per second: 110, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.321326, mae: 13.805780, mean_q: 20.059792\n",
            " 122813/150000: episode: 17892, duration: 0.055s, episode steps:   5, steps per second:  91, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.400 [2.000, 8.000],  loss: 0.269098, mae: 14.317513, mean_q: 20.169193\n",
            " 122819/150000: episode: 17893, duration: 0.065s, episode steps:   6, steps per second:  92, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.000 [0.000, 7.000],  loss: 0.356684, mae: 14.008527, mean_q: 19.834450\n",
            " 122825/150000: episode: 17894, duration: 0.055s, episode steps:   6, steps per second: 108, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.167 [2.000, 8.000],  loss: 0.227046, mae: 13.719766, mean_q: 20.081522\n",
            " 122830/150000: episode: 17895, duration: 0.051s, episode steps:   5, steps per second:  98, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.232404, mae: 13.868979, mean_q: 19.908300\n",
            " 122837/150000: episode: 17896, duration: 0.076s, episode steps:   7, steps per second:  93, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.222472, mae: 13.872873, mean_q: 20.094385\n",
            " 122845/150000: episode: 17897, duration: 0.069s, episode steps:   8, steps per second: 115, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.548600, mae: 13.809082, mean_q: 19.728176\n",
            " 122852/150000: episode: 17898, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.206120, mae: 13.780341, mean_q: 20.039812\n",
            " 122855/150000: episode: 17899, duration: 0.034s, episode steps:   3, steps per second:  89, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 5.667 [5.000, 6.000],  loss: 0.159646, mae: 14.016510, mean_q: 20.047108\n",
            " 122861/150000: episode: 17900, duration: 0.067s, episode steps:   6, steps per second:  90, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.819604, mae: 13.905688, mean_q: 19.957054\n",
            " 122869/150000: episode: 17901, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.469581, mae: 13.863556, mean_q: 19.934347\n",
            " 122874/150000: episode: 17902, duration: 0.047s, episode steps:   5, steps per second: 106, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.600 [1.000, 8.000],  loss: 0.472749, mae: 13.920949, mean_q: 20.025875\n",
            " 122882/150000: episode: 17903, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.335639, mae: 14.054543, mean_q: 20.037727\n",
            " 122891/150000: episode: 17904, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.547735, mae: 13.872408, mean_q: 20.083096\n",
            " 122899/150000: episode: 17905, duration: 0.068s, episode steps:   8, steps per second: 117, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.500 [0.000, 8.000],  loss: 0.290927, mae: 13.915588, mean_q: 20.058975\n",
            " 122904/150000: episode: 17906, duration: 0.046s, episode steps:   5, steps per second: 108, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.200 [1.000, 8.000],  loss: 0.319869, mae: 13.548666, mean_q: 19.943460\n",
            " 122911/150000: episode: 17907, duration: 0.062s, episode steps:   7, steps per second: 113, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.433409, mae: 13.887530, mean_q: 20.062994\n",
            " 122919/150000: episode: 17908, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.643148, mae: 14.237592, mean_q: 20.142315\n",
            " 122926/150000: episode: 17909, duration: 0.061s, episode steps:   7, steps per second: 115, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 5.429 [1.000, 8.000],  loss: 0.626635, mae: 13.802424, mean_q: 19.796848\n",
            " 122933/150000: episode: 17910, duration: 0.069s, episode steps:   7, steps per second: 101, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.379489, mae: 14.026353, mean_q: 20.225103\n",
            " 122939/150000: episode: 17911, duration: 0.057s, episode steps:   6, steps per second: 106, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.667 [0.000, 8.000],  loss: 0.398999, mae: 14.210534, mean_q: 20.019650\n",
            " 122944/150000: episode: 17912, duration: 0.060s, episode steps:   5, steps per second:  83, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.200 [2.000, 8.000],  loss: 0.238065, mae: 13.732800, mean_q: 20.073154\n",
            " 122949/150000: episode: 17913, duration: 0.046s, episode steps:   5, steps per second: 108, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.600 [3.000, 6.000],  loss: 0.436940, mae: 14.039922, mean_q: 19.947361\n",
            " 122953/150000: episode: 17914, duration: 0.039s, episode steps:   4, steps per second: 102, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.750 [3.000, 7.000],  loss: 0.396407, mae: 14.115730, mean_q: 20.034403\n",
            " 122962/150000: episode: 17915, duration: 0.082s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.361016, mae: 14.009098, mean_q: 20.103712\n",
            " 122971/150000: episode: 17916, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.222138, mae: 13.947554, mean_q: 19.916578\n",
            " 122980/150000: episode: 17917, duration: 0.079s, episode steps:   9, steps per second: 115, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.341425, mae: 13.853008, mean_q: 19.965275\n",
            " 122986/150000: episode: 17918, duration: 0.068s, episode steps:   6, steps per second:  88, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [1.000, 8.000],  loss: 0.609129, mae: 13.942815, mean_q: 20.082121\n",
            " 122995/150000: episode: 17919, duration: 0.098s, episode steps:   9, steps per second:  91, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.390190, mae: 14.103167, mean_q: 19.990362\n",
            " 123001/150000: episode: 17920, duration: 0.056s, episode steps:   6, steps per second: 108, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.500 [0.000, 7.000],  loss: 0.230124, mae: 14.022232, mean_q: 19.947104\n",
            " 123007/150000: episode: 17921, duration: 0.059s, episode steps:   6, steps per second: 101, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2.667 [0.000, 6.000],  loss: 0.273975, mae: 13.847798, mean_q: 20.043682\n",
            " 123016/150000: episode: 17922, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.354076, mae: 13.931594, mean_q: 19.860237\n",
            " 123022/150000: episode: 17923, duration: 0.065s, episode steps:   6, steps per second:  92, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.444281, mae: 13.962990, mean_q: 19.721815\n",
            " 123031/150000: episode: 17924, duration: 0.079s, episode steps:   9, steps per second: 113, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.352861, mae: 13.882725, mean_q: 20.044752\n",
            " 123040/150000: episode: 17925, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.310047, mae: 14.099076, mean_q: 19.841370\n",
            " 123049/150000: episode: 17926, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.111 [0.000, 8.000],  loss: 0.580153, mae: 13.917801, mean_q: 20.003847\n",
            " 123058/150000: episode: 17927, duration: 0.080s, episode steps:   9, steps per second: 112, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.358294, mae: 13.670551, mean_q: 20.038055\n",
            " 123065/150000: episode: 17928, duration: 0.066s, episode steps:   7, steps per second: 107, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.143 [0.000, 7.000],  loss: 0.309653, mae: 13.582646, mean_q: 20.098743\n",
            " 123072/150000: episode: 17929, duration: 0.071s, episode steps:   7, steps per second:  99, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.239749, mae: 13.664638, mean_q: 20.011105\n",
            " 123077/150000: episode: 17930, duration: 0.051s, episode steps:   5, steps per second:  98, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.322699, mae: 13.582972, mean_q: 20.027365\n",
            " 123085/150000: episode: 17931, duration: 0.069s, episode steps:   8, steps per second: 116, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.245137, mae: 13.655748, mean_q: 20.116587\n",
            " 123090/150000: episode: 17932, duration: 0.058s, episode steps:   5, steps per second:  86, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.000 [0.000, 5.000],  loss: 0.300816, mae: 13.712720, mean_q: 19.882668\n",
            " 123093/150000: episode: 17933, duration: 0.039s, episode steps:   3, steps per second:  77, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 2.667 [0.000, 8.000],  loss: 0.424236, mae: 14.125766, mean_q: 19.884695\n",
            " 123102/150000: episode: 17934, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.463442, mae: 13.945583, mean_q: 20.055462\n",
            " 123109/150000: episode: 17935, duration: 0.062s, episode steps:   7, steps per second: 113, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.595078, mae: 13.974217, mean_q: 19.895662\n",
            " 123112/150000: episode: 17936, duration: 0.031s, episode steps:   3, steps per second:  97, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 1.667 [0.000, 5.000],  loss: 0.267296, mae: 13.824051, mean_q: 20.040007\n",
            " 123121/150000: episode: 17937, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.296348, mae: 14.199192, mean_q: 20.078209\n",
            " 123130/150000: episode: 17938, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.199100, mae: 13.603405, mean_q: 20.139763\n",
            " 123137/150000: episode: 17939, duration: 0.061s, episode steps:   7, steps per second: 115, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.210048, mae: 13.987002, mean_q: 20.002218\n",
            " 123145/150000: episode: 17940, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.184069, mae: 13.826067, mean_q: 20.109421\n",
            " 123154/150000: episode: 17941, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.340084, mae: 13.920373, mean_q: 19.940273\n",
            " 123162/150000: episode: 17942, duration: 0.071s, episode steps:   8, steps per second: 112, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.292854, mae: 13.970854, mean_q: 20.006672\n",
            " 123168/150000: episode: 17943, duration: 0.062s, episode steps:   6, steps per second:  96, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.585863, mae: 13.885818, mean_q: 20.077736\n",
            " 123173/150000: episode: 17944, duration: 0.049s, episode steps:   5, steps per second: 102, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.800 [0.000, 5.000],  loss: 0.246087, mae: 13.588058, mean_q: 19.898457\n",
            " 123182/150000: episode: 17945, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.724793, mae: 13.777218, mean_q: 20.099648\n",
            " 123187/150000: episode: 17946, duration: 0.045s, episode steps:   5, steps per second: 111, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.000 [1.000, 6.000],  loss: 0.381437, mae: 14.009909, mean_q: 20.159946\n",
            " 123193/150000: episode: 17947, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.268697, mae: 13.944461, mean_q: 20.105722\n",
            " 123200/150000: episode: 17948, duration: 0.067s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.248611, mae: 14.174555, mean_q: 20.208096\n",
            " 123209/150000: episode: 17949, duration: 0.077s, episode steps:   9, steps per second: 116, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.329305, mae: 13.891754, mean_q: 20.002296\n",
            " 123217/150000: episode: 17950, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.326855, mae: 13.834717, mean_q: 20.066380\n",
            " 123225/150000: episode: 17951, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.399585, mae: 14.079920, mean_q: 19.958870\n",
            " 123231/150000: episode: 17952, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.000 [0.000, 5.000],  loss: 0.295511, mae: 14.071708, mean_q: 20.071531\n",
            " 123237/150000: episode: 17953, duration: 0.053s, episode steps:   6, steps per second: 113, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.619875, mae: 13.838614, mean_q: 20.156586\n",
            " 123243/150000: episode: 17954, duration: 0.065s, episode steps:   6, steps per second:  92, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 6.000],  loss: 0.460631, mae: 13.572178, mean_q: 20.013697\n",
            " 123252/150000: episode: 17955, duration: 0.075s, episode steps:   9, steps per second: 120, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.261818, mae: 13.816805, mean_q: 20.258362\n",
            " 123260/150000: episode: 17956, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.302988, mae: 13.913683, mean_q: 19.902094\n",
            " 123265/150000: episode: 17957, duration: 0.046s, episode steps:   5, steps per second: 108, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 6.200 [3.000, 8.000],  loss: 0.194805, mae: 13.418469, mean_q: 19.916147\n",
            " 123272/150000: episode: 17958, duration: 0.078s, episode steps:   7, steps per second:  90, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.143 [0.000, 7.000],  loss: 0.500020, mae: 13.674165, mean_q: 19.877184\n",
            " 123276/150000: episode: 17959, duration: 0.042s, episode steps:   4, steps per second:  95, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.750 [3.000, 7.000],  loss: 0.442364, mae: 13.674795, mean_q: 19.731491\n",
            " 123285/150000: episode: 17960, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.634507, mae: 13.976976, mean_q: 20.021919\n",
            " 123293/150000: episode: 17961, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.340496, mae: 13.683822, mean_q: 19.897356\n",
            " 123297/150000: episode: 17962, duration: 0.053s, episode steps:   4, steps per second:  76, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.750 [3.000, 7.000],  loss: 0.281508, mae: 13.907690, mean_q: 19.962912\n",
            " 123303/150000: episode: 17963, duration: 0.063s, episode steps:   6, steps per second:  95, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.301170, mae: 14.084407, mean_q: 20.111818\n",
            " 123310/150000: episode: 17964, duration: 0.061s, episode steps:   7, steps per second: 115, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.260587, mae: 13.668420, mean_q: 20.111111\n",
            " 123316/150000: episode: 17965, duration: 0.054s, episode steps:   6, steps per second: 110, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.294175, mae: 13.380467, mean_q: 19.971483\n",
            " 123321/150000: episode: 17966, duration: 0.063s, episode steps:   5, steps per second:  79, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.800 [1.000, 7.000],  loss: 0.619425, mae: 13.626791, mean_q: 20.021765\n",
            " 123328/150000: episode: 17967, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 7.000],  loss: 0.557871, mae: 13.869908, mean_q: 20.031752\n",
            " 123333/150000: episode: 17968, duration: 0.045s, episode steps:   5, steps per second: 111, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.504108, mae: 14.066788, mean_q: 20.050755\n",
            " 123342/150000: episode: 17969, duration: 0.079s, episode steps:   9, steps per second: 115, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.299374, mae: 13.727320, mean_q: 20.125774\n",
            " 123348/150000: episode: 17970, duration: 0.069s, episode steps:   6, steps per second:  87, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [1.000, 7.000],  loss: 0.728540, mae: 13.763858, mean_q: 19.930269\n",
            " 123354/150000: episode: 17971, duration: 0.055s, episode steps:   6, steps per second: 108, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.250774, mae: 14.018796, mean_q: 19.901299\n",
            " 123359/150000: episode: 17972, duration: 0.046s, episode steps:   5, steps per second: 109, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.400 [2.000, 8.000],  loss: 0.529759, mae: 13.545198, mean_q: 20.004553\n",
            " 123365/150000: episode: 17973, duration: 0.054s, episode steps:   6, steps per second: 111, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.325923, mae: 13.885914, mean_q: 20.025511\n",
            " 123374/150000: episode: 17974, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.579593, mae: 13.525295, mean_q: 20.092810\n",
            " 123383/150000: episode: 17975, duration: 0.075s, episode steps:   9, steps per second: 121, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.326053, mae: 13.978256, mean_q: 20.121050\n",
            " 123389/150000: episode: 17976, duration: 0.050s, episode steps:   6, steps per second: 119, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.459732, mae: 13.685772, mean_q: 19.902094\n",
            " 123396/150000: episode: 17977, duration: 0.061s, episode steps:   7, steps per second: 116, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.252970, mae: 13.902232, mean_q: 20.073980\n",
            " 123404/150000: episode: 17978, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.271375, mae: 13.928072, mean_q: 20.220209\n",
            " 123408/150000: episode: 17979, duration: 0.044s, episode steps:   4, steps per second:  91, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.250 [3.000, 7.000],  loss: 0.212877, mae: 14.339119, mean_q: 19.853516\n",
            " 123413/150000: episode: 17980, duration: 0.048s, episode steps:   5, steps per second: 103, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.000 [0.000, 8.000],  loss: 0.238203, mae: 14.107559, mean_q: 19.884605\n",
            " 123415/150000: episode: 17981, duration: 0.026s, episode steps:   2, steps per second:  78, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.245305, mae: 13.965940, mean_q: 20.121143\n",
            " 123421/150000: episode: 17982, duration: 0.055s, episode steps:   6, steps per second: 108, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [1.000, 8.000],  loss: 0.260859, mae: 13.940150, mean_q: 20.172436\n",
            " 123425/150000: episode: 17983, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.000 [1.000, 6.000],  loss: 0.218292, mae: 13.822315, mean_q: 19.898493\n",
            " 123432/150000: episode: 17984, duration: 0.070s, episode steps:   7, steps per second: 100, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.331539, mae: 13.957214, mean_q: 20.136387\n",
            " 123441/150000: episode: 17985, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.279694, mae: 14.046151, mean_q: 19.901567\n",
            " 123450/150000: episode: 17986, duration: 0.093s, episode steps:   9, steps per second:  96, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.431742, mae: 13.960418, mean_q: 20.154978\n",
            " 123456/150000: episode: 17987, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.167 [2.000, 8.000],  loss: 0.271407, mae: 13.860100, mean_q: 19.994619\n",
            " 123463/150000: episode: 17988, duration: 0.061s, episode steps:   7, steps per second: 114, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.228984, mae: 13.861870, mean_q: 20.017157\n",
            " 123468/150000: episode: 17989, duration: 0.047s, episode steps:   5, steps per second: 107, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.000 [1.000, 6.000],  loss: 0.276532, mae: 14.044813, mean_q: 19.938177\n",
            " 123477/150000: episode: 17990, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.461983, mae: 13.917409, mean_q: 20.055767\n",
            " 123486/150000: episode: 17991, duration: 0.081s, episode steps:   9, steps per second: 112, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.475107, mae: 13.833221, mean_q: 19.909447\n",
            " 123494/150000: episode: 17992, duration: 0.070s, episode steps:   8, steps per second: 114, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.245535, mae: 13.945940, mean_q: 20.156317\n",
            " 123502/150000: episode: 17993, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.677173, mae: 13.912294, mean_q: 19.843893\n",
            " 123511/150000: episode: 17994, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.317453, mae: 13.737470, mean_q: 19.975620\n",
            " 123518/150000: episode: 17995, duration: 0.062s, episode steps:   7, steps per second: 114, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.857 [0.000, 8.000],  loss: 0.418777, mae: 14.074608, mean_q: 20.143576\n",
            " 123523/150000: episode: 17996, duration: 0.047s, episode steps:   5, steps per second: 107, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.355862, mae: 13.888849, mean_q: 20.005541\n",
            " 123530/150000: episode: 17997, duration: 0.077s, episode steps:   7, steps per second:  91, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.000 [0.000, 7.000],  loss: 0.460206, mae: 13.965219, mean_q: 20.186207\n",
            " 123539/150000: episode: 17998, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.315186, mae: 14.014662, mean_q: 19.900358\n",
            " 123546/150000: episode: 17999, duration: 0.076s, episode steps:   7, steps per second:  92, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [0.000, 8.000],  loss: 0.349612, mae: 14.251246, mean_q: 20.018934\n",
            " 123552/150000: episode: 18000, duration: 0.094s, episode steps:   6, steps per second:  64, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 7.000],  loss: 0.552982, mae: 13.858218, mean_q: 19.758467\n",
            " 123559/150000: episode: 18001, duration: 0.099s, episode steps:   7, steps per second:  70, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.252118, mae: 14.040845, mean_q: 20.152134\n",
            " 123567/150000: episode: 18002, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.490608, mae: 13.720887, mean_q: 20.012115\n",
            " 123574/150000: episode: 18003, duration: 0.098s, episode steps:   7, steps per second:  71, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 7.000],  loss: 0.476950, mae: 13.848178, mean_q: 20.107267\n",
            " 123580/150000: episode: 18004, duration: 0.076s, episode steps:   6, steps per second:  78, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.330245, mae: 13.845128, mean_q: 20.024277\n",
            " 123588/150000: episode: 18005, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.293954, mae: 14.072568, mean_q: 20.336502\n",
            " 123596/150000: episode: 18006, duration: 0.116s, episode steps:   8, steps per second:  69, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.333367, mae: 13.914514, mean_q: 20.012033\n",
            " 123602/150000: episode: 18007, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.281375, mae: 14.014842, mean_q: 20.084229\n",
            " 123610/150000: episode: 18008, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.733953, mae: 13.515041, mean_q: 20.039654\n",
            " 123616/150000: episode: 18009, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.390676, mae: 13.735786, mean_q: 20.136761\n",
            " 123622/150000: episode: 18010, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 6.000],  loss: 0.306143, mae: 14.082085, mean_q: 20.293879\n",
            " 123631/150000: episode: 18011, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.351032, mae: 13.662593, mean_q: 20.019558\n",
            " 123639/150000: episode: 18012, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.357328, mae: 14.033511, mean_q: 20.016554\n",
            " 123647/150000: episode: 18013, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.670075, mae: 13.837563, mean_q: 20.091566\n",
            " 123655/150000: episode: 18014, duration: 0.122s, episode steps:   8, steps per second:  65, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.603481, mae: 14.022717, mean_q: 20.166807\n",
            " 123662/150000: episode: 18015, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.283542, mae: 13.658832, mean_q: 20.075253\n",
            " 123670/150000: episode: 18016, duration: 0.117s, episode steps:   8, steps per second:  68, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.221588, mae: 13.975667, mean_q: 20.177267\n",
            " 123677/150000: episode: 18017, duration: 0.113s, episode steps:   7, steps per second:  62, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 7.000],  loss: 0.386183, mae: 13.739177, mean_q: 19.846880\n",
            " 123685/150000: episode: 18018, duration: 0.116s, episode steps:   8, steps per second:  69, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.310158, mae: 13.893340, mean_q: 19.982925\n",
            " 123691/150000: episode: 18019, duration: 0.092s, episode steps:   6, steps per second:  65, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.309887, mae: 14.085328, mean_q: 20.183867\n",
            " 123697/150000: episode: 18020, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.561093, mae: 14.114133, mean_q: 19.947752\n",
            " 123706/150000: episode: 18021, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.300681, mae: 13.787551, mean_q: 20.128708\n",
            " 123713/150000: episode: 18022, duration: 0.106s, episode steps:   7, steps per second:  66, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.376411, mae: 14.004732, mean_q: 20.121428\n",
            " 123720/150000: episode: 18023, duration: 0.099s, episode steps:   7, steps per second:  71, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.212948, mae: 13.955501, mean_q: 19.995701\n",
            " 123726/150000: episode: 18024, duration: 0.096s, episode steps:   6, steps per second:  62, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.443505, mae: 13.862584, mean_q: 19.929810\n",
            " 123731/150000: episode: 18025, duration: 0.076s, episode steps:   5, steps per second:  66, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.574723, mae: 14.003096, mean_q: 20.230574\n",
            " 123738/150000: episode: 18026, duration: 0.127s, episode steps:   7, steps per second:  55, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.858763, mae: 13.821795, mean_q: 19.782074\n",
            " 123745/150000: episode: 18027, duration: 0.113s, episode steps:   7, steps per second:  62, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 7.000],  loss: 0.267409, mae: 14.157661, mean_q: 20.201063\n",
            " 123751/150000: episode: 18028, duration: 0.089s, episode steps:   6, steps per second:  67, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.309087, mae: 13.888031, mean_q: 20.262045\n",
            " 123757/150000: episode: 18029, duration: 0.102s, episode steps:   6, steps per second:  59, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.245287, mae: 14.025693, mean_q: 20.151875\n",
            " 123766/150000: episode: 18030, duration: 0.130s, episode steps:   9, steps per second:  69, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.415645, mae: 13.902618, mean_q: 19.960732\n",
            " 123770/150000: episode: 18031, duration: 0.070s, episode steps:   4, steps per second:  57, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.250 [4.000, 7.000],  loss: 0.604930, mae: 13.572445, mean_q: 20.023130\n",
            " 123775/150000: episode: 18032, duration: 0.073s, episode steps:   5, steps per second:  68, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.800 [1.000, 7.000],  loss: 0.408421, mae: 13.882726, mean_q: 20.080288\n",
            " 123782/150000: episode: 18033, duration: 0.102s, episode steps:   7, steps per second:  69, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.249391, mae: 13.639935, mean_q: 20.102783\n",
            " 123791/150000: episode: 18034, duration: 0.126s, episode steps:   9, steps per second:  71, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.889 [0.000, 8.000],  loss: 0.443188, mae: 14.060078, mean_q: 19.947475\n",
            " 123793/150000: episode: 18035, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 3.000 [3.000, 3.000],  loss: 0.149604, mae: 14.268406, mean_q: 19.977493\n",
            " 123800/150000: episode: 18036, duration: 0.109s, episode steps:   7, steps per second:  64, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.549102, mae: 13.572710, mean_q: 20.141674\n",
            " 123808/150000: episode: 18037, duration: 0.102s, episode steps:   8, steps per second:  79, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.380521, mae: 14.040134, mean_q: 19.947975\n",
            " 123817/150000: episode: 18038, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.211313, mae: 13.895787, mean_q: 20.028040\n",
            " 123826/150000: episode: 18039, duration: 0.101s, episode steps:   9, steps per second:  90, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.245265, mae: 13.809821, mean_q: 19.960825\n",
            " 123834/150000: episode: 18040, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.383335, mae: 13.915367, mean_q: 19.933846\n",
            " 123841/150000: episode: 18041, duration: 0.068s, episode steps:   7, steps per second: 103, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.336233, mae: 13.874987, mean_q: 20.143795\n",
            " 123848/150000: episode: 18042, duration: 0.066s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.232600, mae: 13.777257, mean_q: 19.870499\n",
            " 123856/150000: episode: 18043, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.875 [1.000, 7.000],  loss: 0.347582, mae: 14.181131, mean_q: 20.193508\n",
            " 123863/150000: episode: 18044, duration: 0.063s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.320084, mae: 14.268561, mean_q: 19.944492\n",
            " 123869/150000: episode: 18045, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.485162, mae: 14.098304, mean_q: 19.860571\n",
            " 123877/150000: episode: 18046, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.584090, mae: 14.092056, mean_q: 19.984886\n",
            " 123884/150000: episode: 18047, duration: 0.065s, episode steps:   7, steps per second: 107, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.857 [2.000, 8.000],  loss: 0.328108, mae: 13.815471, mean_q: 19.838377\n",
            " 123891/150000: episode: 18048, duration: 0.077s, episode steps:   7, steps per second:  91, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.143 [0.000, 7.000],  loss: 0.320138, mae: 13.967722, mean_q: 20.246817\n",
            " 123895/150000: episode: 18049, duration: 0.050s, episode steps:   4, steps per second:  81, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.750 [0.000, 7.000],  loss: 0.219251, mae: 14.272215, mean_q: 20.311056\n",
            " 123899/150000: episode: 18050, duration: 0.045s, episode steps:   4, steps per second:  89, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.358938, mae: 13.967805, mean_q: 19.738510\n",
            " 123905/150000: episode: 18051, duration: 0.057s, episode steps:   6, steps per second: 106, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 8.000],  loss: 0.216538, mae: 13.835312, mean_q: 19.948801\n",
            " 123913/150000: episode: 18052, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.192249, mae: 13.790589, mean_q: 20.022808\n",
            " 123918/150000: episode: 18053, duration: 0.048s, episode steps:   5, steps per second: 104, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.654527, mae: 13.823400, mean_q: 20.084070\n",
            " 123923/150000: episode: 18054, duration: 0.049s, episode steps:   5, steps per second: 103, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.285235, mae: 13.887186, mean_q: 19.982750\n",
            " 123929/150000: episode: 18055, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.491027, mae: 13.936470, mean_q: 20.131014\n",
            " 123933/150000: episode: 18056, duration: 0.042s, episode steps:   4, steps per second:  96, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.750 [1.000, 8.000],  loss: 0.573006, mae: 13.979971, mean_q: 20.040258\n",
            " 123941/150000: episode: 18057, duration: 0.085s, episode steps:   8, steps per second:  95, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.241954, mae: 13.823221, mean_q: 20.198387\n",
            " 123948/150000: episode: 18058, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.571 [0.000, 7.000],  loss: 0.226263, mae: 13.679197, mean_q: 20.011271\n",
            " 123951/150000: episode: 18059, duration: 0.030s, episode steps:   3, steps per second:  99, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 2.667 [2.000, 3.000],  loss: 0.244151, mae: 13.976312, mean_q: 19.873653\n",
            " 123960/150000: episode: 18060, duration: 0.078s, episode steps:   9, steps per second: 116, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.261139, mae: 13.978107, mean_q: 20.217022\n",
            " 123965/150000: episode: 18061, duration: 0.055s, episode steps:   5, steps per second:  90, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.800 [0.000, 8.000],  loss: 0.141177, mae: 14.290787, mean_q: 20.092716\n",
            " 123971/150000: episode: 18062, duration: 0.055s, episode steps:   6, steps per second: 110, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 6.000],  loss: 0.226757, mae: 13.803979, mean_q: 19.996918\n",
            " 123978/150000: episode: 18063, duration: 0.063s, episode steps:   7, steps per second: 112, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.143 [1.000, 8.000],  loss: 0.261328, mae: 13.764424, mean_q: 20.035421\n",
            " 123984/150000: episode: 18064, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.000 [0.000, 8.000],  loss: 0.541737, mae: 13.996601, mean_q: 19.886850\n",
            " 123989/150000: episode: 18065, duration: 0.062s, episode steps:   5, steps per second:  81, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.400 [0.000, 6.000],  loss: 0.555049, mae: 13.663717, mean_q: 19.744455\n",
            " 123996/150000: episode: 18066, duration: 0.071s, episode steps:   7, steps per second:  98, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.363896, mae: 14.076036, mean_q: 20.244059\n",
            " 124002/150000: episode: 18067, duration: 0.065s, episode steps:   6, steps per second:  92, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [1.000, 8.000],  loss: 0.393309, mae: 13.977689, mean_q: 20.167801\n",
            " 124009/150000: episode: 18068, duration: 0.107s, episode steps:   7, steps per second:  65, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.296035, mae: 13.691703, mean_q: 20.267828\n",
            " 124017/150000: episode: 18069, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.250 [0.000, 8.000],  loss: 0.278005, mae: 13.667637, mean_q: 19.988897\n",
            " 124023/150000: episode: 18070, duration: 0.060s, episode steps:   6, steps per second:  99, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.167 [0.000, 8.000],  loss: 0.575641, mae: 13.610695, mean_q: 19.940594\n",
            " 124028/150000: episode: 18071, duration: 0.046s, episode steps:   5, steps per second: 110, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.800 [0.000, 8.000],  loss: 0.285569, mae: 13.836016, mean_q: 20.072948\n",
            " 124034/150000: episode: 18072, duration: 0.071s, episode steps:   6, steps per second:  84, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [1.000, 8.000],  loss: 0.329505, mae: 13.807216, mean_q: 20.046127\n",
            " 124040/150000: episode: 18073, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.500 [1.000, 7.000],  loss: 0.371902, mae: 13.656952, mean_q: 20.100937\n",
            " 124047/150000: episode: 18074, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.248546, mae: 14.021052, mean_q: 19.899977\n",
            " 124054/150000: episode: 18075, duration: 0.068s, episode steps:   7, steps per second: 102, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.514903, mae: 14.113286, mean_q: 20.164341\n",
            " 124062/150000: episode: 18076, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.497497, mae: 13.797360, mean_q: 19.843796\n",
            " 124068/150000: episode: 18077, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.667 [0.000, 8.000],  loss: 0.329315, mae: 13.995632, mean_q: 19.955032\n",
            " 124075/150000: episode: 18078, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.271188, mae: 14.057763, mean_q: 20.252043\n",
            " 124081/150000: episode: 18079, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.334658, mae: 14.116446, mean_q: 20.006697\n",
            " 124086/150000: episode: 18080, duration: 0.062s, episode steps:   5, steps per second:  81, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.600 [1.000, 8.000],  loss: 0.244803, mae: 13.816958, mean_q: 20.093721\n",
            " 124093/150000: episode: 18081, duration: 0.068s, episode steps:   7, steps per second: 104, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.498789, mae: 14.009008, mean_q: 19.788540\n",
            " 124098/150000: episode: 18082, duration: 0.049s, episode steps:   5, steps per second: 102, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.000 [3.000, 8.000],  loss: 0.370845, mae: 13.741793, mean_q: 19.938623\n",
            " 124106/150000: episode: 18083, duration: 0.068s, episode steps:   8, steps per second: 117, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.369669, mae: 13.971821, mean_q: 20.053888\n",
            " 124113/150000: episode: 18084, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.475559, mae: 13.880037, mean_q: 20.013275\n",
            " 124120/150000: episode: 18085, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.234791, mae: 13.997602, mean_q: 20.025667\n",
            " 124128/150000: episode: 18086, duration: 0.070s, episode steps:   8, steps per second: 114, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.215832, mae: 13.986718, mean_q: 20.151382\n",
            " 124135/150000: episode: 18087, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.725324, mae: 14.288060, mean_q: 19.713541\n",
            " 124144/150000: episode: 18088, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.359746, mae: 14.136024, mean_q: 20.129412\n",
            " 124151/150000: episode: 18089, duration: 0.063s, episode steps:   7, steps per second: 110, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 2.571 [0.000, 7.000],  loss: 0.258912, mae: 13.898453, mean_q: 20.176117\n",
            " 124158/150000: episode: 18090, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.558199, mae: 13.878911, mean_q: 20.146641\n",
            " 124165/150000: episode: 18091, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.278018, mae: 13.917177, mean_q: 19.960844\n",
            " 124171/150000: episode: 18092, duration: 0.071s, episode steps:   6, steps per second:  85, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.500 [1.000, 8.000],  loss: 0.203099, mae: 13.990144, mean_q: 20.189089\n",
            " 124180/150000: episode: 18093, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.225105, mae: 13.866403, mean_q: 20.037256\n",
            " 124188/150000: episode: 18094, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.507769, mae: 13.905000, mean_q: 20.094915\n",
            " 124194/150000: episode: 18095, duration: 0.067s, episode steps:   6, steps per second:  90, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [0.000, 8.000],  loss: 0.206415, mae: 14.202956, mean_q: 19.955513\n",
            " 124200/150000: episode: 18096, duration: 0.064s, episode steps:   6, steps per second:  94, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 8.000],  loss: 0.355294, mae: 13.730367, mean_q: 20.036072\n",
            " 124207/150000: episode: 18097, duration: 0.067s, episode steps:   7, steps per second: 104, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.714 [0.000, 8.000],  loss: 0.341548, mae: 13.754827, mean_q: 20.016806\n",
            " 124209/150000: episode: 18098, duration: 0.024s, episode steps:   2, steps per second:  83, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 3.000 [3.000, 3.000],  loss: 0.436050, mae: 13.930727, mean_q: 19.994665\n",
            " 124216/150000: episode: 18099, duration: 0.081s, episode steps:   7, steps per second:  87, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.282248, mae: 13.924204, mean_q: 20.229656\n",
            " 124223/150000: episode: 18100, duration: 0.071s, episode steps:   7, steps per second:  99, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.221404, mae: 14.015155, mean_q: 19.939938\n",
            " 124226/150000: episode: 18101, duration: 0.035s, episode steps:   3, steps per second:  85, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 4.667 [4.000, 5.000],  loss: 0.254458, mae: 14.096221, mean_q: 19.766039\n",
            " 124234/150000: episode: 18102, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.216019, mae: 14.031078, mean_q: 20.148396\n",
            " 124240/150000: episode: 18103, duration: 0.067s, episode steps:   6, steps per second:  90, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [1.000, 8.000],  loss: 0.281983, mae: 14.199422, mean_q: 20.035807\n",
            " 124248/150000: episode: 18104, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.462810, mae: 13.971864, mean_q: 19.939827\n",
            " 124256/150000: episode: 18105, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.304280, mae: 13.869163, mean_q: 20.051939\n",
            " 124262/150000: episode: 18106, duration: 0.060s, episode steps:   6, steps per second: 101, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [2.000, 8.000],  loss: 0.285925, mae: 13.893298, mean_q: 20.127371\n",
            " 124271/150000: episode: 18107, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.242551, mae: 13.980594, mean_q: 20.161402\n",
            " 124280/150000: episode: 18108, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.246077, mae: 14.158710, mean_q: 19.964067\n",
            " 124287/150000: episode: 18109, duration: 0.080s, episode steps:   7, steps per second:  88, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.787039, mae: 13.788134, mean_q: 19.912561\n",
            " 124294/150000: episode: 18110, duration: 0.071s, episode steps:   7, steps per second:  99, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.281464, mae: 14.000197, mean_q: 19.844175\n",
            " 124303/150000: episode: 18111, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.546701, mae: 13.762174, mean_q: 20.038282\n",
            " 124310/150000: episode: 18112, duration: 0.077s, episode steps:   7, steps per second:  91, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.783879, mae: 13.870820, mean_q: 19.865231\n",
            " 124317/150000: episode: 18113, duration: 0.087s, episode steps:   7, steps per second:  81, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.310503, mae: 13.667700, mean_q: 20.071333\n",
            " 124323/150000: episode: 18114, duration: 0.061s, episode steps:   6, steps per second:  98, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.500 [0.000, 8.000],  loss: 0.289964, mae: 13.693267, mean_q: 19.896917\n",
            " 124331/150000: episode: 18115, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.294595, mae: 13.828804, mean_q: 20.060244\n",
            " 124340/150000: episode: 18116, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.216890, mae: 13.808499, mean_q: 20.074232\n",
            " 124344/150000: episode: 18117, duration: 0.042s, episode steps:   4, steps per second:  95, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.750 [1.000, 6.000],  loss: 0.335586, mae: 13.662659, mean_q: 20.007595\n",
            " 124352/150000: episode: 18118, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.437111, mae: 13.782122, mean_q: 19.961754\n",
            " 124358/150000: episode: 18119, duration: 0.069s, episode steps:   6, steps per second:  87, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.524497, mae: 14.016150, mean_q: 19.810080\n",
            " 124365/150000: episode: 18120, duration: 0.069s, episode steps:   7, steps per second: 102, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.220249, mae: 14.189850, mean_q: 20.051041\n",
            " 124372/150000: episode: 18121, duration: 0.067s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.210152, mae: 13.913066, mean_q: 20.046829\n",
            " 124380/150000: episode: 18122, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.349525, mae: 14.035492, mean_q: 19.911573\n",
            " 124387/150000: episode: 18123, duration: 0.070s, episode steps:   7, steps per second: 100, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.429 [0.000, 7.000],  loss: 0.360310, mae: 13.568654, mean_q: 19.828630\n",
            " 124395/150000: episode: 18124, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.433350, mae: 14.014715, mean_q: 19.873968\n",
            " 124403/150000: episode: 18125, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.273238, mae: 13.473053, mean_q: 19.888412\n",
            " 124409/150000: episode: 18126, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [1.000, 8.000],  loss: 0.416633, mae: 13.458922, mean_q: 20.072702\n",
            " 124413/150000: episode: 18127, duration: 0.045s, episode steps:   4, steps per second:  89, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 2.250 [0.000, 7.000],  loss: 0.276997, mae: 13.857150, mean_q: 19.925009\n",
            " 124422/150000: episode: 18128, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.273078, mae: 13.806632, mean_q: 20.098186\n",
            " 124427/150000: episode: 18129, duration: 0.064s, episode steps:   5, steps per second:  78, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 1.800 [0.000, 5.000],  loss: 0.221272, mae: 13.867422, mean_q: 20.141937\n",
            " 124434/150000: episode: 18130, duration: 0.067s, episode steps:   7, steps per second: 104, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.669793, mae: 13.890719, mean_q: 19.918299\n",
            " 124442/150000: episode: 18131, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 5.250 [1.000, 8.000],  loss: 0.259613, mae: 13.853138, mean_q: 20.079853\n",
            " 124447/150000: episode: 18132, duration: 0.048s, episode steps:   5, steps per second: 104, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.400 [0.000, 7.000],  loss: 0.330422, mae: 13.633159, mean_q: 20.001549\n",
            " 124455/150000: episode: 18133, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.194441, mae: 13.859335, mean_q: 19.871496\n",
            " 124461/150000: episode: 18134, duration: 0.056s, episode steps:   6, steps per second: 108, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.833 [2.000, 8.000],  loss: 0.319290, mae: 13.787307, mean_q: 19.978098\n",
            " 124466/150000: episode: 18135, duration: 0.048s, episode steps:   5, steps per second: 105, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 8.000],  loss: 0.647177, mae: 13.551743, mean_q: 19.735947\n",
            " 124473/150000: episode: 18136, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.707054, mae: 13.656960, mean_q: 20.097889\n",
            " 124481/150000: episode: 18137, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.625 [0.000, 7.000],  loss: 0.303455, mae: 13.823006, mean_q: 20.006330\n",
            " 124489/150000: episode: 18138, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.464974, mae: 13.549051, mean_q: 20.274557\n",
            " 124497/150000: episode: 18139, duration: 0.070s, episode steps:   8, steps per second: 115, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.387824, mae: 13.994465, mean_q: 19.906696\n",
            " 124503/150000: episode: 18140, duration: 0.055s, episode steps:   6, steps per second: 110, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.167 [0.000, 8.000],  loss: 0.462275, mae: 14.066195, mean_q: 19.901823\n",
            " 124511/150000: episode: 18141, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.625 [1.000, 8.000],  loss: 0.504096, mae: 13.961275, mean_q: 20.049770\n",
            " 124518/150000: episode: 18142, duration: 0.063s, episode steps:   7, steps per second: 112, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.269543, mae: 14.049165, mean_q: 20.005663\n",
            " 124525/150000: episode: 18143, duration: 0.072s, episode steps:   7, steps per second:  98, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.316916, mae: 13.767799, mean_q: 20.101154\n",
            " 124527/150000: episode: 18144, duration: 0.024s, episode steps:   2, steps per second:  83, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 7.000 [7.000, 7.000],  loss: 0.389358, mae: 13.753878, mean_q: 19.934780\n",
            " 124534/150000: episode: 18145, duration: 0.073s, episode steps:   7, steps per second:  95, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 2.857 [0.000, 7.000],  loss: 0.231368, mae: 13.593590, mean_q: 20.198441\n",
            " 124542/150000: episode: 18146, duration: 0.070s, episode steps:   8, steps per second: 114, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.529878, mae: 13.627548, mean_q: 19.819538\n",
            " 124551/150000: episode: 18147, duration: 0.075s, episode steps:   9, steps per second: 120, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.273409, mae: 13.667399, mean_q: 20.011723\n",
            " 124559/150000: episode: 18148, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.239812, mae: 13.794395, mean_q: 19.937355\n",
            " 124565/150000: episode: 18149, duration: 0.069s, episode steps:   6, steps per second:  87, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [2.000, 8.000],  loss: 0.264412, mae: 13.783898, mean_q: 20.166323\n",
            " 124571/150000: episode: 18150, duration: 0.057s, episode steps:   6, steps per second: 106, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.333 [3.000, 8.000],  loss: 0.537358, mae: 13.526370, mean_q: 19.739584\n",
            " 124578/150000: episode: 18151, duration: 0.067s, episode steps:   7, steps per second: 104, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.386207, mae: 13.647484, mean_q: 19.944986\n",
            " 124587/150000: episode: 18152, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.275705, mae: 13.882538, mean_q: 20.124840\n",
            " 124595/150000: episode: 18153, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.262948, mae: 13.671726, mean_q: 20.086119\n",
            " 124601/150000: episode: 18154, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.333 [0.000, 7.000],  loss: 0.352226, mae: 14.172030, mean_q: 20.034973\n",
            " 124608/150000: episode: 18155, duration: 0.067s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.251396, mae: 13.949936, mean_q: 20.059824\n",
            " 124614/150000: episode: 18156, duration: 0.072s, episode steps:   6, steps per second:  84, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.293869, mae: 13.891818, mean_q: 19.987326\n",
            " 124622/150000: episode: 18157, duration: 0.070s, episode steps:   8, steps per second: 114, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.488433, mae: 14.181475, mean_q: 20.096029\n",
            " 124624/150000: episode: 18158, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.161535, mae: 14.454254, mean_q: 20.138546\n",
            " 124633/150000: episode: 18159, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.386940, mae: 13.715354, mean_q: 19.975801\n",
            " 124640/150000: episode: 18160, duration: 0.067s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.896687, mae: 13.928850, mean_q: 20.028112\n",
            " 124645/150000: episode: 18161, duration: 0.047s, episode steps:   5, steps per second: 106, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.246411, mae: 13.747485, mean_q: 19.981771\n",
            " 124653/150000: episode: 18162, duration: 0.074s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.863984, mae: 13.670436, mean_q: 20.088299\n",
            " 124660/150000: episode: 18163, duration: 0.086s, episode steps:   7, steps per second:  82, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.401593, mae: 14.114528, mean_q: 20.109966\n",
            " 124667/150000: episode: 18164, duration: 0.068s, episode steps:   7, steps per second: 102, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.330188, mae: 13.874437, mean_q: 20.211000\n",
            " 124675/150000: episode: 18165, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.315835, mae: 13.944127, mean_q: 20.268147\n",
            " 124681/150000: episode: 18166, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 6.000],  loss: 0.395795, mae: 14.129270, mean_q: 20.063656\n",
            " 124688/150000: episode: 18167, duration: 0.083s, episode steps:   7, steps per second:  84, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.286 [0.000, 8.000],  loss: 0.558094, mae: 14.025515, mean_q: 19.824511\n",
            " 124694/150000: episode: 18168, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [2.000, 7.000],  loss: 0.296189, mae: 13.925344, mean_q: 20.163292\n",
            " 124700/150000: episode: 18169, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.833 [0.000, 6.000],  loss: 0.321209, mae: 13.831864, mean_q: 20.028971\n",
            " 124706/150000: episode: 18170, duration: 0.054s, episode steps:   6, steps per second: 111, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [1.000, 7.000],  loss: 0.369364, mae: 13.865983, mean_q: 20.038424\n",
            " 124711/150000: episode: 18171, duration: 0.054s, episode steps:   5, steps per second:  92, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.200 [0.000, 7.000],  loss: 0.735733, mae: 14.061688, mean_q: 20.000980\n",
            " 124716/150000: episode: 18172, duration: 0.054s, episode steps:   5, steps per second:  93, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 6.000 [4.000, 8.000],  loss: 0.393850, mae: 13.557696, mean_q: 19.773279\n",
            " 124724/150000: episode: 18173, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.221856, mae: 13.818961, mean_q: 20.155085\n",
            " 124732/150000: episode: 18174, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.420220, mae: 13.792187, mean_q: 19.963791\n",
            " 124740/150000: episode: 18175, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.244613, mae: 13.937653, mean_q: 20.139263\n",
            " 124745/150000: episode: 18176, duration: 0.053s, episode steps:   5, steps per second:  94, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.600 [0.000, 6.000],  loss: 0.205851, mae: 14.143288, mean_q: 20.038864\n",
            " 124754/150000: episode: 18177, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.225450, mae: 13.679743, mean_q: 20.007589\n",
            " 124758/150000: episode: 18178, duration: 0.049s, episode steps:   4, steps per second:  82, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.000 [0.000, 8.000],  loss: 0.317709, mae: 14.295382, mean_q: 20.026491\n",
            " 124763/150000: episode: 18179, duration: 0.049s, episode steps:   5, steps per second: 103, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [0.000, 8.000],  loss: 0.252440, mae: 13.812070, mean_q: 19.905495\n",
            " 124768/150000: episode: 18180, duration: 0.047s, episode steps:   5, steps per second: 107, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.400 [0.000, 7.000],  loss: 0.261285, mae: 13.660031, mean_q: 19.936136\n",
            " 124774/150000: episode: 18181, duration: 0.059s, episode steps:   6, steps per second: 101, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.349679, mae: 13.586887, mean_q: 20.136503\n",
            " 124781/150000: episode: 18182, duration: 0.075s, episode steps:   7, steps per second:  94, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 5.429 [2.000, 8.000],  loss: 0.327807, mae: 14.042368, mean_q: 20.221769\n",
            " 124786/150000: episode: 18183, duration: 0.052s, episode steps:   5, steps per second:  96, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 8.000],  loss: 0.185139, mae: 13.685117, mean_q: 19.875942\n",
            " 124795/150000: episode: 18184, duration: 0.134s, episode steps:   9, steps per second:  67, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.355636, mae: 13.740504, mean_q: 19.951332\n",
            " 124800/150000: episode: 18185, duration: 0.072s, episode steps:   5, steps per second:  69, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.292131, mae: 13.989954, mean_q: 20.023012\n",
            " 124807/150000: episode: 18186, duration: 0.094s, episode steps:   7, steps per second:  74, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.261846, mae: 13.682302, mean_q: 19.870996\n",
            " 124811/150000: episode: 18187, duration: 0.077s, episode steps:   4, steps per second:  52, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.213577, mae: 13.964653, mean_q: 20.234056\n",
            " 124820/150000: episode: 18188, duration: 0.133s, episode steps:   9, steps per second:  68, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.535890, mae: 13.870006, mean_q: 19.843124\n",
            " 124828/150000: episode: 18189, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.538999, mae: 13.803905, mean_q: 20.318222\n",
            " 124837/150000: episode: 18190, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.338074, mae: 13.835238, mean_q: 20.018120\n",
            " 124845/150000: episode: 18191, duration: 0.117s, episode steps:   8, steps per second:  68, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.750 [1.000, 8.000],  loss: 0.373263, mae: 13.869478, mean_q: 20.020943\n",
            " 124854/150000: episode: 18192, duration: 0.130s, episode steps:   9, steps per second:  69, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.719514, mae: 13.825705, mean_q: 19.925093\n",
            " 124862/150000: episode: 18193, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.750 [0.000, 7.000],  loss: 0.493261, mae: 13.704957, mean_q: 20.010326\n",
            " 124869/150000: episode: 18194, duration: 0.096s, episode steps:   7, steps per second:  73, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.857 [0.000, 8.000],  loss: 0.371601, mae: 13.590125, mean_q: 19.979204\n",
            " 124878/150000: episode: 18195, duration: 0.109s, episode steps:   9, steps per second:  83, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.405893, mae: 13.715631, mean_q: 19.925585\n",
            " 124886/150000: episode: 18196, duration: 0.099s, episode steps:   8, steps per second:  80, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.357530, mae: 13.840080, mean_q: 19.921970\n",
            " 124894/150000: episode: 18197, duration: 0.146s, episode steps:   8, steps per second:  55, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.336776, mae: 13.634853, mean_q: 19.924309\n",
            " 124902/150000: episode: 18198, duration: 0.116s, episode steps:   8, steps per second:  69, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.293027, mae: 13.796539, mean_q: 20.179472\n",
            " 124907/150000: episode: 18199, duration: 0.075s, episode steps:   5, steps per second:  67, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.000 [1.000, 8.000],  loss: 0.337920, mae: 13.612948, mean_q: 20.029165\n",
            " 124914/150000: episode: 18200, duration: 0.111s, episode steps:   7, steps per second:  63, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.524169, mae: 13.963338, mean_q: 19.966764\n",
            " 124919/150000: episode: 18201, duration: 0.077s, episode steps:   5, steps per second:  65, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.400 [2.000, 8.000],  loss: 0.270696, mae: 14.048864, mean_q: 20.052750\n",
            " 124925/150000: episode: 18202, duration: 0.085s, episode steps:   6, steps per second:  70, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [1.000, 8.000],  loss: 0.424690, mae: 13.937715, mean_q: 20.230150\n",
            " 124934/150000: episode: 18203, duration: 0.133s, episode steps:   9, steps per second:  68, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.315279, mae: 13.860265, mean_q: 19.900835\n",
            " 124943/150000: episode: 18204, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.313505, mae: 13.919785, mean_q: 20.113319\n",
            " 124952/150000: episode: 18205, duration: 0.133s, episode steps:   9, steps per second:  68, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.299791, mae: 13.971546, mean_q: 19.850822\n",
            " 124957/150000: episode: 18206, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.309314, mae: 13.979253, mean_q: 20.019785\n",
            " 124966/150000: episode: 18207, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.311692, mae: 13.831918, mean_q: 19.938778\n",
            " 124971/150000: episode: 18208, duration: 0.086s, episode steps:   5, steps per second:  58, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.000 [2.000, 7.000],  loss: 0.178021, mae: 14.097133, mean_q: 20.096315\n",
            " 124978/150000: episode: 18209, duration: 0.100s, episode steps:   7, steps per second:  70, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.618030, mae: 14.156108, mean_q: 20.169647\n",
            " 124985/150000: episode: 18210, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.714 [0.000, 8.000],  loss: 0.239713, mae: 13.824684, mean_q: 20.013851\n",
            " 124992/150000: episode: 18211, duration: 0.113s, episode steps:   7, steps per second:  62, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.857 [0.000, 8.000],  loss: 0.206006, mae: 13.868564, mean_q: 20.180698\n",
            " 124999/150000: episode: 18212, duration: 0.098s, episode steps:   7, steps per second:  71, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 2.857 [0.000, 7.000],  loss: 0.237805, mae: 13.917752, mean_q: 20.167356\n",
            " 125007/150000: episode: 18213, duration: 0.115s, episode steps:   8, steps per second:  70, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.125 [0.000, 8.000],  loss: 0.282002, mae: 13.843319, mean_q: 19.917297\n",
            " 125015/150000: episode: 18214, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.296748, mae: 14.073586, mean_q: 20.049866\n",
            " 125022/150000: episode: 18215, duration: 0.090s, episode steps:   7, steps per second:  77, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.220882, mae: 13.961839, mean_q: 20.055607\n",
            " 125027/150000: episode: 18216, duration: 0.075s, episode steps:   5, steps per second:  67, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 1.583704, mae: 13.613370, mean_q: 19.875599\n",
            " 125031/150000: episode: 18217, duration: 0.064s, episode steps:   4, steps per second:  63, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.000 [1.000, 7.000],  loss: 0.356185, mae: 13.821014, mean_q: 19.861862\n",
            " 125037/150000: episode: 18218, duration: 0.093s, episode steps:   6, steps per second:  65, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 6.000],  loss: 0.377980, mae: 13.951415, mean_q: 19.981415\n",
            " 125044/150000: episode: 18219, duration: 0.116s, episode steps:   7, steps per second:  60, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.000 [0.000, 8.000],  loss: 0.265852, mae: 13.956329, mean_q: 19.978333\n",
            " 125052/150000: episode: 18220, duration: 0.074s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.346969, mae: 13.861191, mean_q: 19.996067\n",
            " 125058/150000: episode: 18221, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.291527, mae: 14.328064, mean_q: 20.015442\n",
            " 125065/150000: episode: 18222, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 7.000],  loss: 0.229822, mae: 13.511165, mean_q: 19.967455\n",
            " 125071/150000: episode: 18223, duration: 0.071s, episode steps:   6, steps per second:  85, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.237751, mae: 13.858954, mean_q: 20.013044\n",
            " 125078/150000: episode: 18224, duration: 0.070s, episode steps:   7, steps per second: 100, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.196505, mae: 13.866485, mean_q: 20.109518\n",
            " 125084/150000: episode: 18225, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.500 [1.000, 7.000],  loss: 0.355377, mae: 13.646419, mean_q: 19.934603\n",
            " 125089/150000: episode: 18226, duration: 0.050s, episode steps:   5, steps per second:  99, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 6.000],  loss: 0.767611, mae: 13.945074, mean_q: 19.705687\n",
            " 125095/150000: episode: 18227, duration: 0.072s, episode steps:   6, steps per second:  84, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 6.000],  loss: 0.598749, mae: 13.824932, mean_q: 19.992207\n",
            " 125103/150000: episode: 18228, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.279402, mae: 14.022892, mean_q: 20.173183\n",
            " 125110/150000: episode: 18229, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 5.143 [1.000, 8.000],  loss: 0.424889, mae: 13.846663, mean_q: 20.000914\n",
            " 125116/150000: episode: 18230, duration: 0.062s, episode steps:   6, steps per second:  96, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.830689, mae: 13.620288, mean_q: 19.850950\n",
            " 125122/150000: episode: 18231, duration: 0.070s, episode steps:   6, steps per second:  86, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.320288, mae: 13.828078, mean_q: 20.143881\n",
            " 125130/150000: episode: 18232, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.363999, mae: 13.959465, mean_q: 19.836948\n",
            " 125136/150000: episode: 18233, duration: 0.064s, episode steps:   6, steps per second:  94, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 1.105420, mae: 13.727464, mean_q: 20.040985\n",
            " 125140/150000: episode: 18234, duration: 0.045s, episode steps:   4, steps per second:  89, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 6.000 [2.000, 8.000],  loss: 0.561205, mae: 13.935116, mean_q: 19.684965\n",
            " 125147/150000: episode: 18235, duration: 0.069s, episode steps:   7, steps per second: 102, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.280627, mae: 13.998934, mean_q: 20.071239\n",
            " 125154/150000: episode: 18236, duration: 0.063s, episode steps:   7, steps per second: 111, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.429 [0.000, 8.000],  loss: 0.451116, mae: 13.675052, mean_q: 19.940817\n",
            " 125160/150000: episode: 18237, duration: 0.069s, episode steps:   6, steps per second:  87, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.553124, mae: 13.822207, mean_q: 19.982203\n",
            " 125169/150000: episode: 18238, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.576027, mae: 13.641883, mean_q: 19.976828\n",
            " 125177/150000: episode: 18239, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.277404, mae: 13.637129, mean_q: 19.786438\n",
            " 125185/150000: episode: 18240, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.260816, mae: 14.094273, mean_q: 20.237415\n",
            " 125189/150000: episode: 18241, duration: 0.041s, episode steps:   4, steps per second:  97, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.000 [4.000, 6.000],  loss: 0.345057, mae: 13.788185, mean_q: 19.848175\n",
            " 125197/150000: episode: 18242, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.223242, mae: 13.850066, mean_q: 20.039257\n",
            " 125205/150000: episode: 18243, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.539948, mae: 13.716751, mean_q: 19.861542\n",
            " 125214/150000: episode: 18244, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.556 [0.000, 8.000],  loss: 0.332892, mae: 14.039724, mean_q: 19.829197\n",
            " 125219/150000: episode: 18245, duration: 0.048s, episode steps:   5, steps per second: 104, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.600 [2.000, 5.000],  loss: 0.350481, mae: 13.858220, mean_q: 20.164722\n",
            " 125226/150000: episode: 18246, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.596879, mae: 13.720909, mean_q: 19.903259\n",
            " 125233/150000: episode: 18247, duration: 0.074s, episode steps:   7, steps per second:  94, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.678155, mae: 13.826838, mean_q: 20.072405\n",
            " 125241/150000: episode: 18248, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.499160, mae: 13.828339, mean_q: 20.054562\n",
            " 125248/150000: episode: 18249, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.857 [1.000, 8.000],  loss: 0.329488, mae: 13.711003, mean_q: 20.121159\n",
            " 125255/150000: episode: 18250, duration: 0.062s, episode steps:   7, steps per second: 113, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.000 [0.000, 8.000],  loss: 0.206402, mae: 14.108104, mean_q: 20.080074\n",
            " 125263/150000: episode: 18251, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.274743, mae: 13.853234, mean_q: 20.142792\n",
            " 125270/150000: episode: 18252, duration: 0.073s, episode steps:   7, steps per second:  95, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.322708, mae: 13.449981, mean_q: 19.958372\n",
            " 125275/150000: episode: 18253, duration: 0.047s, episode steps:   5, steps per second: 107, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.000 [1.000, 7.000],  loss: 0.251451, mae: 14.110809, mean_q: 20.275137\n",
            " 125283/150000: episode: 18254, duration: 0.072s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.706546, mae: 13.903286, mean_q: 20.002300\n",
            " 125290/150000: episode: 18255, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.578826, mae: 13.774843, mean_q: 20.156063\n",
            " 125293/150000: episode: 18256, duration: 0.041s, episode steps:   3, steps per second:  74, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 2.000 [0.000, 6.000],  loss: 0.333673, mae: 13.954079, mean_q: 20.022238\n",
            " 125299/150000: episode: 18257, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 7.000],  loss: 0.316324, mae: 14.054527, mean_q: 20.086226\n",
            " 125305/150000: episode: 18258, duration: 0.054s, episode steps:   6, steps per second: 110, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.605790, mae: 13.504120, mean_q: 20.110918\n",
            " 125311/150000: episode: 18259, duration: 0.056s, episode steps:   6, steps per second: 106, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 1.133892, mae: 13.990996, mean_q: 20.112249\n",
            " 125318/150000: episode: 18260, duration: 0.082s, episode steps:   7, steps per second:  85, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.492145, mae: 13.637628, mean_q: 19.957365\n",
            " 125326/150000: episode: 18261, duration: 0.069s, episode steps:   8, steps per second: 116, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.425722, mae: 13.546442, mean_q: 20.313120\n",
            " 125332/150000: episode: 18262, duration: 0.053s, episode steps:   6, steps per second: 114, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [1.000, 8.000],  loss: 0.354891, mae: 13.966496, mean_q: 20.002390\n",
            " 125339/150000: episode: 18263, duration: 0.075s, episode steps:   7, steps per second:  93, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.407191, mae: 14.110567, mean_q: 19.999166\n",
            " 125347/150000: episode: 18264, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.353447, mae: 13.701551, mean_q: 19.933002\n",
            " 125355/150000: episode: 18265, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.314421, mae: 14.342260, mean_q: 20.032604\n",
            " 125364/150000: episode: 18266, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.359493, mae: 13.919304, mean_q: 20.115828\n",
            " 125368/150000: episode: 18267, duration: 0.045s, episode steps:   4, steps per second:  89, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.000 [2.000, 7.000],  loss: 0.252051, mae: 14.060164, mean_q: 20.192936\n",
            " 125373/150000: episode: 18268, duration: 0.054s, episode steps:   5, steps per second:  92, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.327600, mae: 14.369046, mean_q: 20.126841\n",
            " 125379/150000: episode: 18269, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.000 [0.000, 8.000],  loss: 0.602282, mae: 14.289346, mean_q: 20.352018\n",
            " 125385/150000: episode: 18270, duration: 0.053s, episode steps:   6, steps per second: 112, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 7.000],  loss: 0.243334, mae: 13.808259, mean_q: 19.965715\n",
            " 125392/150000: episode: 18271, duration: 0.075s, episode steps:   7, steps per second:  94, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.000 [0.000, 8.000],  loss: 0.871586, mae: 13.925535, mean_q: 20.038181\n",
            " 125400/150000: episode: 18272, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.359161, mae: 14.026690, mean_q: 19.841450\n",
            " 125406/150000: episode: 18273, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.329448, mae: 13.697347, mean_q: 20.233252\n",
            " 125411/150000: episode: 18274, duration: 0.051s, episode steps:   5, steps per second:  99, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.200 [0.000, 6.000],  loss: 0.421378, mae: 13.504570, mean_q: 20.065151\n",
            " 125417/150000: episode: 18275, duration: 0.068s, episode steps:   6, steps per second:  89, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.672353, mae: 13.871392, mean_q: 19.765917\n",
            " 125423/150000: episode: 18276, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 1.054658, mae: 14.004799, mean_q: 19.891550\n",
            " 125430/150000: episode: 18277, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.460880, mae: 13.925604, mean_q: 20.255011\n",
            " 125435/150000: episode: 18278, duration: 0.056s, episode steps:   5, steps per second:  89, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 6.400 [3.000, 8.000],  loss: 0.438825, mae: 13.947659, mean_q: 20.429047\n",
            " 125441/150000: episode: 18279, duration: 0.067s, episode steps:   6, steps per second:  89, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 7.000],  loss: 0.473751, mae: 13.619765, mean_q: 20.047136\n",
            " 125449/150000: episode: 18280, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.287826, mae: 13.941698, mean_q: 20.056751\n",
            " 125458/150000: episode: 18281, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.253880, mae: 13.823831, mean_q: 20.165812\n",
            " 125466/150000: episode: 18282, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.223159, mae: 13.902066, mean_q: 20.177156\n",
            " 125475/150000: episode: 18283, duration: 0.081s, episode steps:   9, steps per second: 112, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.361025, mae: 13.771458, mean_q: 19.993723\n",
            " 125482/150000: episode: 18284, duration: 0.065s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.311994, mae: 13.832217, mean_q: 20.008503\n",
            " 125487/150000: episode: 18285, duration: 0.049s, episode steps:   5, steps per second: 102, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.000 [0.000, 8.000],  loss: 0.294091, mae: 13.861155, mean_q: 19.908590\n",
            " 125491/150000: episode: 18286, duration: 0.053s, episode steps:   4, steps per second:  75, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.250 [4.000, 6.000],  loss: 0.725543, mae: 13.855417, mean_q: 20.050835\n",
            " 125499/150000: episode: 18287, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.336780, mae: 13.951042, mean_q: 20.021418\n",
            " 125505/150000: episode: 18288, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [2.000, 7.000],  loss: 0.227784, mae: 14.116375, mean_q: 20.125498\n",
            " 125510/150000: episode: 18289, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.400 [0.000, 7.000],  loss: 0.314001, mae: 14.067652, mean_q: 20.128315\n",
            " 125517/150000: episode: 18290, duration: 0.080s, episode steps:   7, steps per second:  88, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.571 [0.000, 8.000],  loss: 0.298931, mae: 13.852054, mean_q: 20.108591\n",
            " 125526/150000: episode: 18291, duration: 0.079s, episode steps:   9, steps per second: 115, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.393755, mae: 13.889155, mean_q: 20.106060\n",
            " 125532/150000: episode: 18292, duration: 0.059s, episode steps:   6, steps per second: 101, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [2.000, 8.000],  loss: 0.628058, mae: 13.770767, mean_q: 19.882494\n",
            " 125541/150000: episode: 18293, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.246319, mae: 14.316484, mean_q: 20.112144\n",
            " 125549/150000: episode: 18294, duration: 0.071s, episode steps:   8, steps per second: 112, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.207427, mae: 13.993917, mean_q: 20.088917\n",
            " 125557/150000: episode: 18295, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.212445, mae: 13.597845, mean_q: 20.137623\n",
            " 125563/150000: episode: 18296, duration: 0.062s, episode steps:   6, steps per second:  96, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [2.000, 7.000],  loss: 0.479203, mae: 14.030942, mean_q: 19.897032\n",
            " 125568/150000: episode: 18297, duration: 0.056s, episode steps:   5, steps per second:  89, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.593751, mae: 13.775011, mean_q: 19.862034\n",
            " 125570/150000: episode: 18298, duration: 0.024s, episode steps:   2, steps per second:  83, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 0.000 [0.000, 0.000],  loss: 0.424264, mae: 13.457440, mean_q: 19.739044\n",
            " 125576/150000: episode: 18299, duration: 0.057s, episode steps:   6, steps per second: 106, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [1.000, 8.000],  loss: 0.372809, mae: 13.905301, mean_q: 20.107454\n",
            " 125584/150000: episode: 18300, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.625 [0.000, 8.000],  loss: 0.454684, mae: 13.679480, mean_q: 19.902260\n",
            " 125592/150000: episode: 18301, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.459534, mae: 14.055765, mean_q: 20.110613\n",
            " 125599/150000: episode: 18302, duration: 0.066s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.441827, mae: 13.952649, mean_q: 20.120649\n",
            " 125605/150000: episode: 18303, duration: 0.053s, episode steps:   6, steps per second: 113, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.576514, mae: 13.390823, mean_q: 19.968615\n",
            " 125612/150000: episode: 18304, duration: 0.081s, episode steps:   7, steps per second:  87, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.397398, mae: 13.965474, mean_q: 19.998514\n",
            " 125620/150000: episode: 18305, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.570203, mae: 13.958116, mean_q: 20.014919\n",
            " 125626/150000: episode: 18306, duration: 0.055s, episode steps:   6, steps per second: 110, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [1.000, 8.000],  loss: 0.462304, mae: 14.153890, mean_q: 19.969622\n",
            " 125635/150000: episode: 18307, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.241451, mae: 14.073704, mean_q: 20.213425\n",
            " 125642/150000: episode: 18308, duration: 0.085s, episode steps:   7, steps per second:  83, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.857 [0.000, 8.000],  loss: 0.633437, mae: 13.831858, mean_q: 19.907663\n",
            " 125651/150000: episode: 18309, duration: 0.077s, episode steps:   9, steps per second: 116, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.476599, mae: 13.936205, mean_q: 20.133566\n",
            " 125656/150000: episode: 18310, duration: 0.056s, episode steps:   5, steps per second:  89, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [1.000, 8.000],  loss: 0.391245, mae: 14.089785, mean_q: 20.154293\n",
            " 125664/150000: episode: 18311, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.456858, mae: 13.911440, mean_q: 20.016579\n",
            " 125670/150000: episode: 18312, duration: 0.055s, episode steps:   6, steps per second: 110, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.000 [0.000, 8.000],  loss: 0.412040, mae: 13.813600, mean_q: 20.054644\n",
            " 125675/150000: episode: 18313, duration: 0.049s, episode steps:   5, steps per second: 103, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.800 [1.000, 7.000],  loss: 0.243781, mae: 13.916550, mean_q: 19.971546\n",
            " 125684/150000: episode: 18314, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.224926, mae: 14.088284, mean_q: 20.159777\n",
            " 125690/150000: episode: 18315, duration: 0.056s, episode steps:   6, steps per second: 106, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 8.000],  loss: 0.250617, mae: 13.882401, mean_q: 19.863626\n",
            " 125699/150000: episode: 18316, duration: 0.078s, episode steps:   9, steps per second: 116, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.350173, mae: 13.902323, mean_q: 19.873480\n",
            " 125707/150000: episode: 18317, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 5.125 [1.000, 8.000],  loss: 0.374796, mae: 14.278507, mean_q: 19.962879\n",
            " 125711/150000: episode: 18318, duration: 0.043s, episode steps:   4, steps per second:  93, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 1.500 [0.000, 4.000],  loss: 0.426661, mae: 13.919876, mean_q: 20.049221\n",
            " 125719/150000: episode: 18319, duration: 0.070s, episode steps:   8, steps per second: 114, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.367086, mae: 13.964635, mean_q: 19.995956\n",
            " 125728/150000: episode: 18320, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.572805, mae: 13.887779, mean_q: 20.005953\n",
            " 125737/150000: episode: 18321, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.306478, mae: 13.894576, mean_q: 19.930117\n",
            " 125745/150000: episode: 18322, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.436467, mae: 13.911489, mean_q: 20.115978\n",
            " 125750/150000: episode: 18323, duration: 0.060s, episode steps:   5, steps per second:  84, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.272419, mae: 14.127439, mean_q: 20.243099\n",
            " 125755/150000: episode: 18324, duration: 0.049s, episode steps:   5, steps per second: 103, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [0.000, 8.000],  loss: 0.396947, mae: 13.719870, mean_q: 19.835674\n",
            " 125763/150000: episode: 18325, duration: 0.088s, episode steps:   8, steps per second:  90, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.456254, mae: 14.007312, mean_q: 20.045048\n",
            " 125769/150000: episode: 18326, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.833 [0.000, 6.000],  loss: 0.331615, mae: 13.851735, mean_q: 19.940111\n",
            " 125774/150000: episode: 18327, duration: 0.047s, episode steps:   5, steps per second: 107, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.000 [0.000, 5.000],  loss: 0.835334, mae: 14.003812, mean_q: 19.898050\n",
            " 125783/150000: episode: 18328, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.250464, mae: 14.070185, mean_q: 20.109171\n",
            " 125786/150000: episode: 18329, duration: 0.039s, episode steps:   3, steps per second:  76, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.631822, mae: 13.678123, mean_q: 19.953466\n",
            " 125794/150000: episode: 18330, duration: 0.074s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.609058, mae: 13.785521, mean_q: 19.991089\n",
            " 125802/150000: episode: 18331, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.260169, mae: 13.854588, mean_q: 19.989786\n",
            " 125807/150000: episode: 18332, duration: 0.050s, episode steps:   5, steps per second: 101, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.600 [2.000, 8.000],  loss: 0.333684, mae: 13.973910, mean_q: 19.972298\n",
            " 125811/150000: episode: 18333, duration: 0.051s, episode steps:   4, steps per second:  78, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.000 [4.000, 6.000],  loss: 0.194569, mae: 14.304306, mean_q: 20.112465\n",
            " 125817/150000: episode: 18334, duration: 0.056s, episode steps:   6, steps per second: 108, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [1.000, 8.000],  loss: 0.223349, mae: 14.063535, mean_q: 20.028341\n",
            " 125825/150000: episode: 18335, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.215758, mae: 13.819152, mean_q: 19.931889\n",
            " 125833/150000: episode: 18336, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 5.375 [2.000, 8.000],  loss: 0.397337, mae: 13.871796, mean_q: 19.878195\n",
            " 125839/150000: episode: 18337, duration: 0.068s, episode steps:   6, steps per second:  88, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 7.000],  loss: 0.270224, mae: 13.910417, mean_q: 19.846300\n",
            " 125848/150000: episode: 18338, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.260587, mae: 13.968664, mean_q: 19.884260\n",
            " 125857/150000: episode: 18339, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.181163, mae: 13.870289, mean_q: 19.941477\n",
            " 125865/150000: episode: 18340, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.432386, mae: 13.781448, mean_q: 20.059967\n",
            " 125872/150000: episode: 18341, duration: 0.062s, episode steps:   7, steps per second: 113, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.908924, mae: 13.578860, mean_q: 19.727797\n",
            " 125878/150000: episode: 18342, duration: 0.053s, episode steps:   6, steps per second: 113, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.000 [0.000, 8.000],  loss: 0.409276, mae: 13.629056, mean_q: 20.059113\n",
            " 125882/150000: episode: 18343, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.000 [0.000, 5.000],  loss: 0.196317, mae: 13.807491, mean_q: 20.141846\n",
            " 125888/150000: episode: 18344, duration: 0.055s, episode steps:   6, steps per second: 110, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.167 [0.000, 8.000],  loss: 0.511011, mae: 13.784860, mean_q: 19.886129\n",
            " 125895/150000: episode: 18345, duration: 0.061s, episode steps:   7, steps per second: 114, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.342007, mae: 13.818177, mean_q: 19.956900\n",
            " 125903/150000: episode: 18346, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.435010, mae: 14.034113, mean_q: 19.950184\n",
            " 125912/150000: episode: 18347, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.210824, mae: 13.917205, mean_q: 19.936525\n",
            " 125921/150000: episode: 18348, duration: 0.078s, episode steps:   9, steps per second: 115, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.330889, mae: 14.126797, mean_q: 19.966860\n",
            " 125927/150000: episode: 18349, duration: 0.053s, episode steps:   6, steps per second: 112, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.333 [1.000, 7.000],  loss: 0.623318, mae: 13.670039, mean_q: 20.065268\n",
            " 125936/150000: episode: 18350, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.390766, mae: 13.941175, mean_q: 19.862846\n",
            " 125942/150000: episode: 18351, duration: 0.066s, episode steps:   6, steps per second:  90, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.500 [0.000, 8.000],  loss: 0.235489, mae: 13.806020, mean_q: 20.025743\n",
            " 125951/150000: episode: 18352, duration: 0.078s, episode steps:   9, steps per second: 115, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.192635, mae: 13.805002, mean_q: 19.982664\n",
            " 125954/150000: episode: 18353, duration: 0.030s, episode steps:   3, steps per second:  99, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 5.667 [5.000, 6.000],  loss: 0.211779, mae: 14.225724, mean_q: 19.860954\n",
            " 125959/150000: episode: 18354, duration: 0.064s, episode steps:   5, steps per second:  78, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.428142, mae: 14.056070, mean_q: 19.989624\n",
            " 125965/150000: episode: 18355, duration: 0.061s, episode steps:   6, steps per second:  98, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.667 [0.000, 6.000],  loss: 0.206974, mae: 13.609428, mean_q: 19.990503\n",
            " 125970/150000: episode: 18356, duration: 0.060s, episode steps:   5, steps per second:  84, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.000 [3.000, 7.000],  loss: 0.831292, mae: 13.903323, mean_q: 19.875219\n",
            " 125977/150000: episode: 18357, duration: 0.063s, episode steps:   7, steps per second: 112, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.429 [1.000, 7.000],  loss: 0.523257, mae: 13.353229, mean_q: 19.795328\n",
            " 125983/150000: episode: 18358, duration: 0.068s, episode steps:   6, steps per second:  88, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.351249, mae: 13.712467, mean_q: 20.311892\n",
            " 125991/150000: episode: 18359, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.250 [0.000, 8.000],  loss: 0.454850, mae: 13.808244, mean_q: 19.785576\n",
            " 125997/150000: episode: 18360, duration: 0.054s, episode steps:   6, steps per second: 111, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.833 [2.000, 7.000],  loss: 0.286193, mae: 13.964929, mean_q: 19.812296\n",
            " 126006/150000: episode: 18361, duration: 0.079s, episode steps:   9, steps per second: 115, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.316012, mae: 13.743008, mean_q: 20.183592\n",
            " 126012/150000: episode: 18362, duration: 0.065s, episode steps:   6, steps per second:  92, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.229619, mae: 14.123940, mean_q: 20.041281\n",
            " 126019/150000: episode: 18363, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.237288, mae: 14.025370, mean_q: 19.862640\n",
            " 126024/150000: episode: 18364, duration: 0.048s, episode steps:   5, steps per second: 105, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.800 [0.000, 5.000],  loss: 0.796492, mae: 13.705646, mean_q: 20.072701\n",
            " 126031/150000: episode: 18365, duration: 0.076s, episode steps:   7, steps per second:  92, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.274507, mae: 13.739146, mean_q: 20.017017\n",
            " 126039/150000: episode: 18366, duration: 0.074s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.294824, mae: 13.759132, mean_q: 20.200897\n",
            " 126045/150000: episode: 18367, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.579365, mae: 13.714656, mean_q: 19.839340\n",
            " 126050/150000: episode: 18368, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [1.000, 8.000],  loss: 0.260229, mae: 14.300669, mean_q: 20.039005\n",
            " 126059/150000: episode: 18369, duration: 0.133s, episode steps:   9, steps per second:  68, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.315145, mae: 13.584325, mean_q: 20.042053\n",
            " 126065/150000: episode: 18370, duration: 0.098s, episode steps:   6, steps per second:  61, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.167 [2.000, 8.000],  loss: 0.184678, mae: 14.251572, mean_q: 20.179808\n",
            " 126070/150000: episode: 18371, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.400 [0.000, 7.000],  loss: 0.180800, mae: 14.105585, mean_q: 20.063416\n",
            " 126076/150000: episode: 18372, duration: 0.087s, episode steps:   6, steps per second:  69, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.345073, mae: 13.693337, mean_q: 19.901144\n",
            " 126083/150000: episode: 18373, duration: 0.088s, episode steps:   7, steps per second:  79, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.252044, mae: 13.917779, mean_q: 20.127186\n",
            " 126092/150000: episode: 18374, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.382663, mae: 13.780869, mean_q: 20.020561\n",
            " 126100/150000: episode: 18375, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.198248, mae: 14.057270, mean_q: 20.186398\n",
            " 126109/150000: episode: 18376, duration: 0.111s, episode steps:   9, steps per second:  81, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.188249, mae: 13.811386, mean_q: 19.985893\n",
            " 126117/150000: episode: 18377, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.190624, mae: 13.861267, mean_q: 19.882971\n",
            " 126124/150000: episode: 18378, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.714 [0.000, 8.000],  loss: 0.156903, mae: 14.034975, mean_q: 20.041159\n",
            " 126133/150000: episode: 18379, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.301000, mae: 13.947276, mean_q: 19.965233\n",
            " 126142/150000: episode: 18380, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.386066, mae: 13.776223, mean_q: 20.152880\n",
            " 126149/150000: episode: 18381, duration: 0.085s, episode steps:   7, steps per second:  82, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.188685, mae: 13.918698, mean_q: 19.991978\n",
            " 126158/150000: episode: 18382, duration: 0.103s, episode steps:   9, steps per second:  87, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.491578, mae: 13.800795, mean_q: 20.096397\n",
            " 126166/150000: episode: 18383, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.303302, mae: 13.871901, mean_q: 19.956642\n",
            " 126175/150000: episode: 18384, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.203765, mae: 14.146079, mean_q: 20.134602\n",
            " 126183/150000: episode: 18385, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.476083, mae: 13.499289, mean_q: 19.747469\n",
            " 126190/150000: episode: 18386, duration: 0.090s, episode steps:   7, steps per second:  78, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.429 [0.000, 8.000],  loss: 0.240540, mae: 13.899290, mean_q: 20.126322\n",
            " 126198/150000: episode: 18387, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.198377, mae: 14.102889, mean_q: 20.071308\n",
            " 126204/150000: episode: 18388, duration: 0.087s, episode steps:   6, steps per second:  69, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.333 [2.000, 7.000],  loss: 0.161840, mae: 13.770530, mean_q: 20.127420\n",
            " 126208/150000: episode: 18389, duration: 0.060s, episode steps:   4, steps per second:  67, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.750 [3.000, 7.000],  loss: 0.173732, mae: 13.985968, mean_q: 20.101219\n",
            " 126214/150000: episode: 18390, duration: 0.091s, episode steps:   6, steps per second:  66, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.228403, mae: 14.104745, mean_q: 19.872675\n",
            " 126223/150000: episode: 18391, duration: 0.148s, episode steps:   9, steps per second:  61, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.613404, mae: 13.971920, mean_q: 20.239967\n",
            " 126231/150000: episode: 18392, duration: 0.124s, episode steps:   8, steps per second:  65, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.493252, mae: 13.813262, mean_q: 19.941000\n",
            " 126236/150000: episode: 18393, duration: 0.072s, episode steps:   5, steps per second:  69, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.000 [0.000, 8.000],  loss: 0.672502, mae: 13.687059, mean_q: 19.810638\n",
            " 126244/150000: episode: 18394, duration: 0.109s, episode steps:   8, steps per second:  74, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.393005, mae: 13.692514, mean_q: 20.060482\n",
            " 126253/150000: episode: 18395, duration: 0.142s, episode steps:   9, steps per second:  64, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.331852, mae: 13.878056, mean_q: 20.009714\n",
            " 126261/150000: episode: 18396, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.500 [0.000, 8.000],  loss: 0.331862, mae: 14.074402, mean_q: 19.975193\n",
            " 126268/150000: episode: 18397, duration: 0.104s, episode steps:   7, steps per second:  67, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.364603, mae: 13.822786, mean_q: 20.087301\n",
            " 126274/150000: episode: 18398, duration: 0.087s, episode steps:   6, steps per second:  69, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [1.000, 8.000],  loss: 0.314380, mae: 13.520106, mean_q: 19.972198\n",
            " 126279/150000: episode: 18399, duration: 0.079s, episode steps:   5, steps per second:  64, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.800 [1.000, 7.000],  loss: 0.431672, mae: 13.450755, mean_q: 19.987425\n",
            " 126284/150000: episode: 18400, duration: 0.072s, episode steps:   5, steps per second:  69, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.400 [2.000, 8.000],  loss: 0.266345, mae: 13.842871, mean_q: 20.048130\n",
            " 126289/150000: episode: 18401, duration: 0.078s, episode steps:   5, steps per second:  65, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.739559, mae: 13.613650, mean_q: 19.954426\n",
            " 126298/150000: episode: 18402, duration: 0.121s, episode steps:   9, steps per second:  75, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.821220, mae: 13.733665, mean_q: 20.013535\n",
            " 126306/150000: episode: 18403, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.441160, mae: 14.013400, mean_q: 20.037167\n",
            " 126313/150000: episode: 18404, duration: 0.099s, episode steps:   7, steps per second:  71, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.347167, mae: 13.647002, mean_q: 20.263203\n",
            " 126316/150000: episode: 18405, duration: 0.052s, episode steps:   3, steps per second:  58, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 5.333 [5.000, 6.000],  loss: 0.217027, mae: 14.267104, mean_q: 20.188629\n",
            " 126324/150000: episode: 18406, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.491919, mae: 13.972059, mean_q: 19.963127\n",
            " 126332/150000: episode: 18407, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.276174, mae: 13.638704, mean_q: 19.958755\n",
            " 126340/150000: episode: 18408, duration: 0.077s, episode steps:   8, steps per second: 103, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.376115, mae: 13.855271, mean_q: 20.051033\n",
            " 126345/150000: episode: 18409, duration: 0.048s, episode steps:   5, steps per second: 103, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.600 [2.000, 8.000],  loss: 0.237229, mae: 13.993004, mean_q: 20.033426\n",
            " 126354/150000: episode: 18410, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.239548, mae: 14.105145, mean_q: 20.076406\n",
            " 126363/150000: episode: 18411, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.231762, mae: 14.031742, mean_q: 20.078955\n",
            " 126369/150000: episode: 18412, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [2.000, 7.000],  loss: 0.840307, mae: 14.007225, mean_q: 20.100290\n",
            " 126376/150000: episode: 18413, duration: 0.070s, episode steps:   7, steps per second: 100, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.223073, mae: 13.909109, mean_q: 20.033085\n",
            " 126382/150000: episode: 18414, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.385360, mae: 13.832458, mean_q: 20.181377\n",
            " 126390/150000: episode: 18415, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.424623, mae: 13.953316, mean_q: 20.008255\n",
            " 126395/150000: episode: 18416, duration: 0.051s, episode steps:   5, steps per second:  98, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.400 [0.000, 6.000],  loss: 0.920918, mae: 13.850881, mean_q: 20.173075\n",
            " 126404/150000: episode: 18417, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.499609, mae: 13.884913, mean_q: 20.121525\n",
            " 126412/150000: episode: 18418, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.322167, mae: 13.682552, mean_q: 20.008137\n",
            " 126418/150000: episode: 18419, duration: 0.067s, episode steps:   6, steps per second:  90, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.167 [0.000, 8.000],  loss: 0.275724, mae: 13.933236, mean_q: 20.426683\n",
            " 126426/150000: episode: 18420, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.303937, mae: 13.815902, mean_q: 19.926697\n",
            " 126430/150000: episode: 18421, duration: 0.039s, episode steps:   4, steps per second: 102, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.750 [2.000, 7.000],  loss: 0.252567, mae: 13.983408, mean_q: 20.295090\n",
            " 126434/150000: episode: 18422, duration: 0.040s, episode steps:   4, steps per second: 101, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.750 [3.000, 8.000],  loss: 0.150628, mae: 14.349991, mean_q: 20.236364\n",
            " 126441/150000: episode: 18423, duration: 0.074s, episode steps:   7, steps per second:  95, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.217807, mae: 13.861581, mean_q: 19.930098\n",
            " 126448/150000: episode: 18424, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.326028, mae: 14.011123, mean_q: 19.978647\n",
            " 126456/150000: episode: 18425, duration: 0.074s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.429097, mae: 14.200942, mean_q: 20.276014\n",
            " 126465/150000: episode: 18426, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.240246, mae: 13.745176, mean_q: 19.975939\n",
            " 126471/150000: episode: 18427, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.721796, mae: 13.891965, mean_q: 20.265196\n",
            " 126480/150000: episode: 18428, duration: 0.078s, episode steps:   9, steps per second: 115, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.306410, mae: 14.043948, mean_q: 20.117332\n",
            " 126485/150000: episode: 18429, duration: 0.058s, episode steps:   5, steps per second:  86, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.000 [0.000, 6.000],  loss: 0.256763, mae: 14.033930, mean_q: 20.352030\n",
            " 126491/150000: episode: 18430, duration: 0.070s, episode steps:   6, steps per second:  86, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [2.000, 7.000],  loss: 0.299972, mae: 13.861939, mean_q: 20.125013\n",
            " 126498/150000: episode: 18431, duration: 0.062s, episode steps:   7, steps per second: 112, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.706154, mae: 14.126683, mean_q: 20.169485\n",
            " 126504/150000: episode: 18432, duration: 0.057s, episode steps:   6, steps per second: 106, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 7.000],  loss: 0.298514, mae: 13.891349, mean_q: 20.169962\n",
            " 126513/150000: episode: 18433, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.496339, mae: 14.108458, mean_q: 20.095930\n",
            " 126522/150000: episode: 18434, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.450695, mae: 13.890141, mean_q: 20.248529\n",
            " 126530/150000: episode: 18435, duration: 0.074s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.228588, mae: 13.869885, mean_q: 20.063694\n",
            " 126539/150000: episode: 18436, duration: 0.078s, episode steps:   9, steps per second: 115, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.284986, mae: 13.721933, mean_q: 20.123650\n",
            " 126548/150000: episode: 18437, duration: 0.095s, episode steps:   9, steps per second:  94, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.290939, mae: 14.042039, mean_q: 20.011429\n",
            " 126554/150000: episode: 18438, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.000 [0.000, 8.000],  loss: 0.290276, mae: 13.590152, mean_q: 20.018730\n",
            " 126563/150000: episode: 18439, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.333 [0.000, 7.000],  loss: 0.195163, mae: 13.749200, mean_q: 20.023294\n",
            " 126570/150000: episode: 18440, duration: 0.074s, episode steps:   7, steps per second:  95, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.626420, mae: 13.829921, mean_q: 20.093821\n",
            " 126577/150000: episode: 18441, duration: 0.076s, episode steps:   7, steps per second:  92, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.277921, mae: 13.960607, mean_q: 20.068554\n",
            " 126585/150000: episode: 18442, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.385002, mae: 13.850270, mean_q: 20.116894\n",
            " 126592/150000: episode: 18443, duration: 0.081s, episode steps:   7, steps per second:  86, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 2.429 [0.000, 7.000],  loss: 0.241422, mae: 13.666338, mean_q: 20.249620\n",
            " 126601/150000: episode: 18444, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.192813, mae: 13.693955, mean_q: 20.096436\n",
            " 126605/150000: episode: 18445, duration: 0.043s, episode steps:   4, steps per second:  93, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.750 [0.000, 8.000],  loss: 0.249860, mae: 13.796997, mean_q: 20.278999\n",
            " 126611/150000: episode: 18446, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.000 [0.000, 8.000],  loss: 0.182696, mae: 14.033246, mean_q: 20.098883\n",
            " 126619/150000: episode: 18447, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.284155, mae: 14.016989, mean_q: 20.105425\n",
            " 126625/150000: episode: 18448, duration: 0.056s, episode steps:   6, steps per second: 108, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.240631, mae: 14.166419, mean_q: 20.082182\n",
            " 126632/150000: episode: 18449, duration: 0.066s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.643269, mae: 13.570388, mean_q: 19.953800\n",
            " 126637/150000: episode: 18450, duration: 0.051s, episode steps:   5, steps per second:  98, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.362650, mae: 14.239988, mean_q: 20.359821\n",
            " 126645/150000: episode: 18451, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.297697, mae: 13.771103, mean_q: 20.044651\n",
            " 126649/150000: episode: 18452, duration: 0.041s, episode steps:   4, steps per second:  99, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 6.250 [3.000, 8.000],  loss: 0.247711, mae: 13.324137, mean_q: 20.130688\n",
            " 126657/150000: episode: 18453, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.375 [1.000, 8.000],  loss: 0.359722, mae: 13.827503, mean_q: 20.149464\n",
            " 126665/150000: episode: 18454, duration: 0.070s, episode steps:   8, steps per second: 115, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.250 [0.000, 8.000],  loss: 0.270561, mae: 13.821825, mean_q: 20.115911\n",
            " 126670/150000: episode: 18455, duration: 0.061s, episode steps:   5, steps per second:  82, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [2.000, 8.000],  loss: 0.453398, mae: 13.843745, mean_q: 20.194777\n",
            " 126679/150000: episode: 18456, duration: 0.077s, episode steps:   9, steps per second: 117, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.374891, mae: 14.098021, mean_q: 20.200418\n",
            " 126686/150000: episode: 18457, duration: 0.063s, episode steps:   7, steps per second: 111, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.285012, mae: 13.844902, mean_q: 20.326151\n",
            " 126694/150000: episode: 18458, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.000 [0.000, 7.000],  loss: 0.378199, mae: 14.113333, mean_q: 20.127888\n",
            " 126702/150000: episode: 18459, duration: 0.072s, episode steps:   8, steps per second: 112, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.750 [0.000, 8.000],  loss: 0.685175, mae: 13.723653, mean_q: 20.151655\n",
            " 126709/150000: episode: 18460, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.318367, mae: 14.088823, mean_q: 20.051208\n",
            " 126717/150000: episode: 18461, duration: 0.077s, episode steps:   8, steps per second: 105, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.258080, mae: 13.885212, mean_q: 20.216417\n",
            " 126724/150000: episode: 18462, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.712043, mae: 13.989264, mean_q: 19.978384\n",
            " 126727/150000: episode: 18463, duration: 0.033s, episode steps:   3, steps per second:  91, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 4.333 [3.000, 7.000],  loss: 0.612175, mae: 14.001287, mean_q: 20.202265\n",
            " 126734/150000: episode: 18464, duration: 0.062s, episode steps:   7, steps per second: 113, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.346774, mae: 13.934980, mean_q: 20.220755\n",
            " 126741/150000: episode: 18465, duration: 0.073s, episode steps:   7, steps per second:  96, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.233078, mae: 14.105609, mean_q: 20.012945\n",
            " 126748/150000: episode: 18466, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.473033, mae: 14.014712, mean_q: 20.146744\n",
            " 126754/150000: episode: 18467, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.000 [1.000, 8.000],  loss: 0.217920, mae: 14.086967, mean_q: 20.013578\n",
            " 126762/150000: episode: 18468, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.250 [0.000, 8.000],  loss: 0.263796, mae: 13.796923, mean_q: 20.238792\n",
            " 126768/150000: episode: 18469, duration: 0.067s, episode steps:   6, steps per second:  89, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 7.000],  loss: 0.194220, mae: 14.385620, mean_q: 19.929020\n",
            " 126774/150000: episode: 18470, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 2.667 [0.000, 6.000],  loss: 0.161489, mae: 13.934951, mean_q: 20.013956\n",
            " 126783/150000: episode: 18471, duration: 0.074s, episode steps:   9, steps per second: 121, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.289201, mae: 13.832743, mean_q: 20.087229\n",
            " 126788/150000: episode: 18472, duration: 0.043s, episode steps:   5, steps per second: 115, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.800 [3.000, 7.000],  loss: 0.282366, mae: 13.940099, mean_q: 20.014048\n",
            " 126793/150000: episode: 18473, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.200 [0.000, 8.000],  loss: 0.634338, mae: 13.657019, mean_q: 20.155516\n",
            " 126802/150000: episode: 18474, duration: 0.076s, episode steps:   9, steps per second: 118, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.667 [0.000, 8.000],  loss: 0.209147, mae: 14.250423, mean_q: 20.075335\n",
            " 126810/150000: episode: 18475, duration: 0.069s, episode steps:   8, steps per second: 116, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.215534, mae: 14.190356, mean_q: 20.236881\n",
            " 126818/150000: episode: 18476, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.548421, mae: 14.124249, mean_q: 19.878263\n",
            " 126821/150000: episode: 18477, duration: 0.033s, episode steps:   3, steps per second:  90, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 2.667 [2.000, 4.000],  loss: 0.242431, mae: 14.185712, mean_q: 20.249655\n",
            " 126828/150000: episode: 18478, duration: 0.060s, episode steps:   7, steps per second: 117, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.143 [0.000, 8.000],  loss: 0.249217, mae: 13.979202, mean_q: 20.174696\n",
            " 126835/150000: episode: 18479, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.294240, mae: 13.925529, mean_q: 20.266525\n",
            " 126844/150000: episode: 18480, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.190285, mae: 13.975751, mean_q: 19.997458\n",
            " 126853/150000: episode: 18481, duration: 0.077s, episode steps:   9, steps per second: 117, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.294829, mae: 14.062976, mean_q: 20.045238\n",
            " 126860/150000: episode: 18482, duration: 0.061s, episode steps:   7, steps per second: 114, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 7.000],  loss: 0.364401, mae: 14.100599, mean_q: 20.104441\n",
            " 126869/150000: episode: 18483, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.620521, mae: 14.014487, mean_q: 20.100147\n",
            " 126878/150000: episode: 18484, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.526808, mae: 14.305088, mean_q: 20.222853\n",
            " 126885/150000: episode: 18485, duration: 0.063s, episode steps:   7, steps per second: 111, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.280280, mae: 14.053964, mean_q: 20.293423\n",
            " 126894/150000: episode: 18486, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.111 [0.000, 8.000],  loss: 0.257111, mae: 13.887159, mean_q: 20.217012\n",
            " 126901/150000: episode: 18487, duration: 0.075s, episode steps:   7, steps per second:  94, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.387187, mae: 14.051178, mean_q: 20.142033\n",
            " 126909/150000: episode: 18488, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.370307, mae: 13.926174, mean_q: 20.108135\n",
            " 126915/150000: episode: 18489, duration: 0.062s, episode steps:   6, steps per second:  97, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.212605, mae: 14.045583, mean_q: 20.271053\n",
            " 126918/150000: episode: 18490, duration: 0.040s, episode steps:   3, steps per second:  76, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 6.333 [6.000, 7.000],  loss: 0.639216, mae: 13.657562, mean_q: 20.274920\n",
            " 126922/150000: episode: 18491, duration: 0.040s, episode steps:   4, steps per second:  99, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.750 [5.000, 7.000],  loss: 0.531866, mae: 13.691986, mean_q: 19.908823\n",
            " 126931/150000: episode: 18492, duration: 0.078s, episode steps:   9, steps per second: 116, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.478402, mae: 14.019588, mean_q: 20.110062\n",
            " 126939/150000: episode: 18493, duration: 0.085s, episode steps:   8, steps per second:  95, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.373228, mae: 13.866179, mean_q: 20.159916\n",
            " 126948/150000: episode: 18494, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.268174, mae: 14.091160, mean_q: 20.361032\n",
            " 126954/150000: episode: 18495, duration: 0.055s, episode steps:   6, steps per second: 110, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.609636, mae: 13.954305, mean_q: 19.999998\n",
            " 126957/150000: episode: 18496, duration: 0.031s, episode steps:   3, steps per second:  96, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 4.667 [4.000, 6.000],  loss: 0.380649, mae: 14.107666, mean_q: 19.964039\n",
            " 126966/150000: episode: 18497, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.371824, mae: 13.900728, mean_q: 20.141039\n",
            " 126974/150000: episode: 18498, duration: 0.066s, episode steps:   8, steps per second: 120, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.336518, mae: 14.135306, mean_q: 20.176325\n",
            " 126979/150000: episode: 18499, duration: 0.046s, episode steps:   5, steps per second: 109, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [0.000, 8.000],  loss: 0.384004, mae: 14.141329, mean_q: 20.120077\n",
            " 126984/150000: episode: 18500, duration: 0.045s, episode steps:   5, steps per second: 110, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.200 [0.000, 7.000],  loss: 0.491228, mae: 14.208905, mean_q: 20.136333\n",
            " 126993/150000: episode: 18501, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.529289, mae: 13.752680, mean_q: 19.977827\n",
            " 127002/150000: episode: 18502, duration: 0.076s, episode steps:   9, steps per second: 118, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.222 [0.000, 8.000],  loss: 0.610114, mae: 13.887761, mean_q: 20.105682\n",
            " 127010/150000: episode: 18503, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.279500, mae: 14.112192, mean_q: 19.957203\n",
            " 127019/150000: episode: 18504, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.305557, mae: 14.039595, mean_q: 20.038502\n",
            " 127027/150000: episode: 18505, duration: 0.070s, episode steps:   8, steps per second: 114, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.392418, mae: 13.912005, mean_q: 19.995478\n",
            " 127032/150000: episode: 18506, duration: 0.044s, episode steps:   5, steps per second: 114, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [2.000, 8.000],  loss: 0.423306, mae: 14.214396, mean_q: 20.063412\n",
            " 127036/150000: episode: 18507, duration: 0.038s, episode steps:   4, steps per second: 106, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.250 [3.000, 7.000],  loss: 0.264485, mae: 13.821583, mean_q: 20.139824\n",
            " 127043/150000: episode: 18508, duration: 0.079s, episode steps:   7, steps per second:  88, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.143 [0.000, 8.000],  loss: 0.684267, mae: 14.267604, mean_q: 20.014385\n",
            " 127051/150000: episode: 18509, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.256850, mae: 14.230167, mean_q: 20.015556\n",
            " 127056/150000: episode: 18510, duration: 0.052s, episode steps:   5, steps per second:  96, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.442415, mae: 13.903117, mean_q: 20.036900\n",
            " 127063/150000: episode: 18511, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.326349, mae: 14.028795, mean_q: 20.018145\n",
            " 127070/150000: episode: 18512, duration: 0.082s, episode steps:   7, steps per second:  86, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 1.178698, mae: 14.097895, mean_q: 19.916260\n",
            " 127078/150000: episode: 18513, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.639684, mae: 13.846264, mean_q: 20.077631\n",
            " 127082/150000: episode: 18514, duration: 0.042s, episode steps:   4, steps per second:  95, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.250 [2.000, 7.000],  loss: 0.512753, mae: 13.497295, mean_q: 20.531967\n",
            " 127091/150000: episode: 18515, duration: 0.089s, episode steps:   9, steps per second: 102, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.451668, mae: 14.141275, mean_q: 20.036463\n",
            " 127097/150000: episode: 18516, duration: 0.069s, episode steps:   6, steps per second:  87, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 7.000],  loss: 0.267822, mae: 14.156616, mean_q: 20.264551\n",
            " 127104/150000: episode: 18517, duration: 0.063s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.330606, mae: 13.912496, mean_q: 20.384222\n",
            " 127113/150000: episode: 18518, duration: 0.097s, episode steps:   9, steps per second:  92, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.556 [0.000, 8.000],  loss: 0.261784, mae: 14.229309, mean_q: 19.980904\n",
            " 127121/150000: episode: 18519, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.565955, mae: 13.686388, mean_q: 20.011833\n",
            " 127128/150000: episode: 18520, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.433949, mae: 13.877280, mean_q: 20.083467\n",
            " 127135/150000: episode: 18521, duration: 0.074s, episode steps:   7, steps per second:  94, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.000 [0.000, 8.000],  loss: 0.460828, mae: 14.030801, mean_q: 20.105953\n",
            " 127144/150000: episode: 18522, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 1.010603, mae: 13.951659, mean_q: 19.986122\n",
            " 127150/150000: episode: 18523, duration: 0.056s, episode steps:   6, steps per second: 108, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.333 [0.000, 8.000],  loss: 0.237202, mae: 13.994462, mean_q: 20.071762\n",
            " 127154/150000: episode: 18524, duration: 0.040s, episode steps:   4, steps per second: 100, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.750 [0.000, 8.000],  loss: 0.713227, mae: 14.062896, mean_q: 19.778763\n",
            " 127157/150000: episode: 18525, duration: 0.049s, episode steps:   3, steps per second:  61, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 1.667 [0.000, 5.000],  loss: 0.330809, mae: 13.752075, mean_q: 19.906992\n",
            " 127164/150000: episode: 18526, duration: 0.079s, episode steps:   7, steps per second:  89, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.714 [0.000, 8.000],  loss: 0.318311, mae: 13.727271, mean_q: 20.198889\n",
            " 127170/150000: episode: 18527, duration: 0.054s, episode steps:   6, steps per second: 111, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [0.000, 7.000],  loss: 0.364591, mae: 14.012199, mean_q: 19.936434\n",
            " 127176/150000: episode: 18528, duration: 0.057s, episode steps:   6, steps per second: 106, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.333 [0.000, 8.000],  loss: 0.288099, mae: 13.789350, mean_q: 20.019938\n",
            " 127181/150000: episode: 18529, duration: 0.062s, episode steps:   5, steps per second:  81, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.000 [3.000, 7.000],  loss: 0.200543, mae: 13.853079, mean_q: 20.183496\n",
            " 127188/150000: episode: 18530, duration: 0.063s, episode steps:   7, steps per second: 111, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.316149, mae: 13.951365, mean_q: 20.002207\n",
            " 127194/150000: episode: 18531, duration: 0.065s, episode steps:   6, steps per second:  92, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.333 [0.000, 7.000],  loss: 0.313939, mae: 13.758056, mean_q: 20.115107\n",
            " 127201/150000: episode: 18532, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.294174, mae: 13.685185, mean_q: 19.911732\n",
            " 127206/150000: episode: 18533, duration: 0.062s, episode steps:   5, steps per second:  81, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.322849, mae: 13.961598, mean_q: 20.333822\n",
            " 127213/150000: episode: 18534, duration: 0.069s, episode steps:   7, steps per second: 101, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.535647, mae: 14.236300, mean_q: 19.961550\n",
            " 127222/150000: episode: 18535, duration: 0.080s, episode steps:   9, steps per second: 112, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.260006, mae: 14.128635, mean_q: 20.142487\n",
            " 127230/150000: episode: 18536, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.346964, mae: 13.643428, mean_q: 20.133724\n",
            " 127236/150000: episode: 18537, duration: 0.062s, episode steps:   6, steps per second:  97, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 7.000],  loss: 0.233686, mae: 13.893758, mean_q: 20.025200\n",
            " 127242/150000: episode: 18538, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.456766, mae: 13.768012, mean_q: 20.038736\n",
            " 127247/150000: episode: 18539, duration: 0.051s, episode steps:   5, steps per second:  99, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.200 [0.000, 7.000],  loss: 0.283495, mae: 13.984095, mean_q: 20.063063\n",
            " 127254/150000: episode: 18540, duration: 0.078s, episode steps:   7, steps per second:  90, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.143 [2.000, 7.000],  loss: 0.315252, mae: 14.033422, mean_q: 20.224646\n",
            " 127260/150000: episode: 18541, duration: 0.062s, episode steps:   6, steps per second:  96, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.165055, mae: 14.058155, mean_q: 20.115076\n",
            " 127267/150000: episode: 18542, duration: 0.066s, episode steps:   7, steps per second: 107, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 5.000 [1.000, 8.000],  loss: 0.194699, mae: 13.842943, mean_q: 20.070614\n",
            " 127273/150000: episode: 18543, duration: 0.060s, episode steps:   6, steps per second:  99, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [1.000, 7.000],  loss: 0.308061, mae: 14.053126, mean_q: 20.039610\n",
            " 127280/150000: episode: 18544, duration: 0.083s, episode steps:   7, steps per second:  85, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.377717, mae: 13.815494, mean_q: 20.078533\n",
            " 127283/150000: episode: 18545, duration: 0.033s, episode steps:   3, steps per second:  90, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 3.667 [2.000, 7.000],  loss: 0.243852, mae: 14.443347, mean_q: 19.793444\n",
            " 127291/150000: episode: 18546, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.437914, mae: 13.953642, mean_q: 20.142355\n",
            " 127299/150000: episode: 18547, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.383007, mae: 13.966101, mean_q: 19.890678\n",
            " 127304/150000: episode: 18548, duration: 0.059s, episode steps:   5, steps per second:  84, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.800 [0.000, 8.000],  loss: 0.390045, mae: 14.067877, mean_q: 19.959770\n",
            " 127312/150000: episode: 18549, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.272307, mae: 13.650317, mean_q: 20.177902\n",
            " 127318/150000: episode: 18550, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 2.833 [1.000, 6.000],  loss: 0.309800, mae: 13.913597, mean_q: 19.838976\n",
            " 127323/150000: episode: 18551, duration: 0.052s, episode steps:   5, steps per second:  95, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.800 [0.000, 5.000],  loss: 0.191324, mae: 13.897949, mean_q: 20.054134\n",
            " 127329/150000: episode: 18552, duration: 0.092s, episode steps:   6, steps per second:  65, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [2.000, 8.000],  loss: 0.638710, mae: 13.695789, mean_q: 19.866486\n",
            " 127335/150000: episode: 18553, duration: 0.092s, episode steps:   6, steps per second:  65, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.167 [1.000, 8.000],  loss: 0.346483, mae: 13.958522, mean_q: 19.976938\n",
            " 127343/150000: episode: 18554, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.299618, mae: 14.062813, mean_q: 20.134565\n",
            " 127352/150000: episode: 18555, duration: 0.142s, episode steps:   9, steps per second:  63, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.172932, mae: 14.018746, mean_q: 19.947298\n",
            " 127361/150000: episode: 18556, duration: 0.139s, episode steps:   9, steps per second:  65, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.364608, mae: 13.934025, mean_q: 20.006584\n",
            " 127370/150000: episode: 18557, duration: 0.113s, episode steps:   9, steps per second:  79, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.263697, mae: 14.160605, mean_q: 20.132669\n",
            " 127376/150000: episode: 18558, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 7.000],  loss: 0.429393, mae: 13.719357, mean_q: 19.952675\n",
            " 127383/150000: episode: 18559, duration: 0.105s, episode steps:   7, steps per second:  66, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.000 [0.000, 8.000],  loss: 0.285630, mae: 13.982429, mean_q: 19.980349\n",
            " 127392/150000: episode: 18560, duration: 0.186s, episode steps:   9, steps per second:  48, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.248330, mae: 13.773250, mean_q: 20.067993\n",
            " 127401/150000: episode: 18561, duration: 0.215s, episode steps:   9, steps per second:  42, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.196204, mae: 14.011858, mean_q: 20.047577\n",
            " 127410/150000: episode: 18562, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.330352, mae: 13.896254, mean_q: 19.947449\n",
            " 127416/150000: episode: 18563, duration: 0.084s, episode steps:   6, steps per second:  71, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.667 [0.000, 7.000],  loss: 0.232923, mae: 14.007504, mean_q: 20.088243\n",
            " 127425/150000: episode: 18564, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.280857, mae: 14.035580, mean_q: 20.046215\n",
            " 127431/150000: episode: 18565, duration: 0.089s, episode steps:   6, steps per second:  68, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [1.000, 8.000],  loss: 0.143799, mae: 13.986964, mean_q: 20.041819\n",
            " 127438/150000: episode: 18566, duration: 0.099s, episode steps:   7, steps per second:  70, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.143 [0.000, 8.000],  loss: 0.438193, mae: 13.697963, mean_q: 20.018110\n",
            " 127443/150000: episode: 18567, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 1.800 [0.000, 4.000],  loss: 0.569619, mae: 13.777945, mean_q: 19.976276\n",
            " 127452/150000: episode: 18568, duration: 0.134s, episode steps:   9, steps per second:  67, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.385441, mae: 13.889584, mean_q: 19.976227\n",
            " 127458/150000: episode: 18569, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 7.000],  loss: 0.445311, mae: 14.055302, mean_q: 20.091509\n",
            " 127464/150000: episode: 18570, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 7.000],  loss: 0.268255, mae: 13.911151, mean_q: 19.991484\n",
            " 127473/150000: episode: 18571, duration: 0.162s, episode steps:   9, steps per second:  56, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.494752, mae: 14.024921, mean_q: 20.029549\n",
            " 127476/150000: episode: 18572, duration: 0.052s, episode steps:   3, steps per second:  58, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 2.000 [0.000, 6.000],  loss: 0.330229, mae: 13.827542, mean_q: 20.169422\n",
            " 127485/150000: episode: 18573, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.256325, mae: 14.088343, mean_q: 20.123182\n",
            " 127494/150000: episode: 18574, duration: 0.159s, episode steps:   9, steps per second:  56, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.420372, mae: 14.029652, mean_q: 20.033583\n",
            " 127503/150000: episode: 18575, duration: 0.133s, episode steps:   9, steps per second:  67, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.337966, mae: 14.124217, mean_q: 20.105198\n",
            " 127511/150000: episode: 18576, duration: 0.117s, episode steps:   8, steps per second:  68, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.370022, mae: 14.072678, mean_q: 20.270920\n",
            " 127516/150000: episode: 18577, duration: 0.084s, episode steps:   5, steps per second:  59, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.600 [1.000, 8.000],  loss: 0.442857, mae: 13.710879, mean_q: 20.075365\n",
            " 127523/150000: episode: 18578, duration: 0.103s, episode steps:   7, steps per second:  68, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.447707, mae: 14.141133, mean_q: 20.244436\n",
            " 127531/150000: episode: 18579, duration: 0.130s, episode steps:   8, steps per second:  62, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.875 [0.000, 8.000],  loss: 0.635728, mae: 13.839880, mean_q: 19.997177\n",
            " 127539/150000: episode: 18580, duration: 0.128s, episode steps:   8, steps per second:  62, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.697990, mae: 14.015903, mean_q: 19.871801\n",
            " 127545/150000: episode: 18581, duration: 0.099s, episode steps:   6, steps per second:  60, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.500 [1.000, 6.000],  loss: 0.311258, mae: 13.662236, mean_q: 20.047691\n",
            " 127554/150000: episode: 18582, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.533025, mae: 14.084269, mean_q: 20.061680\n",
            " 127562/150000: episode: 18583, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.478782, mae: 13.827684, mean_q: 19.861584\n",
            " 127565/150000: episode: 18584, duration: 0.034s, episode steps:   3, steps per second:  87, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 5.333 [2.000, 7.000],  loss: 0.369979, mae: 14.098395, mean_q: 20.507599\n",
            " 127572/150000: episode: 18585, duration: 0.079s, episode steps:   7, steps per second:  89, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.714 [1.000, 7.000],  loss: 1.262460, mae: 13.960889, mean_q: 19.892542\n",
            " 127578/150000: episode: 18586, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 6.000],  loss: 0.378693, mae: 13.792926, mean_q: 20.156435\n",
            " 127585/150000: episode: 18587, duration: 0.067s, episode steps:   7, steps per second: 104, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.714 [1.000, 8.000],  loss: 0.314064, mae: 13.879713, mean_q: 20.202042\n",
            " 127594/150000: episode: 18588, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.424536, mae: 14.005032, mean_q: 19.765156\n",
            " 127602/150000: episode: 18589, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.000 [0.000, 7.000],  loss: 0.660095, mae: 13.815884, mean_q: 20.139238\n",
            " 127611/150000: episode: 18590, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.111 [0.000, 8.000],  loss: 0.425207, mae: 13.622530, mean_q: 20.136934\n",
            " 127618/150000: episode: 18591, duration: 0.077s, episode steps:   7, steps per second:  91, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.571 [1.000, 8.000],  loss: 0.380792, mae: 13.778494, mean_q: 20.027178\n",
            " 127627/150000: episode: 18592, duration: 0.084s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.199286, mae: 13.951485, mean_q: 20.134298\n",
            " 127632/150000: episode: 18593, duration: 0.051s, episode steps:   5, steps per second:  98, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.000 [2.000, 8.000],  loss: 0.440012, mae: 14.054853, mean_q: 20.041821\n",
            " 127638/150000: episode: 18594, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.500 [1.000, 7.000],  loss: 0.315767, mae: 13.968425, mean_q: 20.271639\n",
            " 127641/150000: episode: 18595, duration: 0.047s, episode steps:   3, steps per second:  63, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 5.000 [4.000, 7.000],  loss: 0.693031, mae: 13.778829, mean_q: 19.965498\n",
            " 127645/150000: episode: 18596, duration: 0.042s, episode steps:   4, steps per second:  95, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.000 [1.000, 7.000],  loss: 0.434446, mae: 13.755578, mean_q: 20.158516\n",
            " 127650/150000: episode: 18597, duration: 0.046s, episode steps:   5, steps per second: 108, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.600 [1.000, 7.000],  loss: 0.473808, mae: 14.039162, mean_q: 19.978943\n",
            " 127656/150000: episode: 18598, duration: 0.054s, episode steps:   6, steps per second: 111, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 7.000],  loss: 0.524396, mae: 13.692630, mean_q: 20.169270\n",
            " 127662/150000: episode: 18599, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 6.000],  loss: 0.257424, mae: 13.925129, mean_q: 20.060953\n",
            " 127671/150000: episode: 18600, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.245927, mae: 13.953380, mean_q: 20.237553\n",
            " 127680/150000: episode: 18601, duration: 0.080s, episode steps:   9, steps per second: 112, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.297326, mae: 13.472256, mean_q: 20.094913\n",
            " 127685/150000: episode: 18602, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [1.000, 8.000],  loss: 0.211428, mae: 13.977079, mean_q: 20.161007\n",
            " 127692/150000: episode: 18603, duration: 0.062s, episode steps:   7, steps per second: 112, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.264486, mae: 14.005813, mean_q: 20.070852\n",
            " 127696/150000: episode: 18604, duration: 0.053s, episode steps:   4, steps per second:  76, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 2.250 [0.000, 7.000],  loss: 0.261472, mae: 14.111234, mean_q: 20.304771\n",
            " 127701/150000: episode: 18605, duration: 0.049s, episode steps:   5, steps per second: 103, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.800 [1.000, 7.000],  loss: 0.553953, mae: 13.895355, mean_q: 20.242153\n",
            " 127706/150000: episode: 18606, duration: 0.057s, episode steps:   5, steps per second:  88, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.310680, mae: 13.854742, mean_q: 19.896471\n",
            " 127713/150000: episode: 18607, duration: 0.078s, episode steps:   7, steps per second:  89, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.143 [0.000, 7.000],  loss: 0.546757, mae: 13.972106, mean_q: 19.946573\n",
            " 127719/150000: episode: 18608, duration: 0.070s, episode steps:   6, steps per second:  86, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.000 [0.000, 8.000],  loss: 0.343891, mae: 13.728770, mean_q: 19.837740\n",
            " 127727/150000: episode: 18609, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.298846, mae: 13.866081, mean_q: 20.212784\n",
            " 127736/150000: episode: 18610, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.735235, mae: 14.000463, mean_q: 19.798897\n",
            " 127744/150000: episode: 18611, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.327474, mae: 14.123066, mean_q: 20.333384\n",
            " 127752/150000: episode: 18612, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.688048, mae: 13.985613, mean_q: 19.791224\n",
            " 127759/150000: episode: 18613, duration: 0.068s, episode steps:   7, steps per second: 103, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.267355, mae: 13.735095, mean_q: 20.084879\n",
            " 127765/150000: episode: 18614, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.500 [1.000, 7.000],  loss: 0.955153, mae: 13.790329, mean_q: 20.070618\n",
            " 127772/150000: episode: 18615, duration: 0.098s, episode steps:   7, steps per second:  71, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [0.000, 8.000],  loss: 0.287339, mae: 14.048857, mean_q: 19.982702\n",
            " 127779/150000: episode: 18616, duration: 0.103s, episode steps:   7, steps per second:  68, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.857 [0.000, 8.000],  loss: 0.402794, mae: 13.808701, mean_q: 19.964140\n",
            " 127787/150000: episode: 18617, duration: 0.070s, episode steps:   8, steps per second: 114, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.875 [0.000, 8.000],  loss: 0.304376, mae: 14.340736, mean_q: 20.127262\n",
            " 127795/150000: episode: 18618, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.393155, mae: 13.911243, mean_q: 20.011631\n",
            " 127801/150000: episode: 18619, duration: 0.069s, episode steps:   6, steps per second:  87, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [0.000, 7.000],  loss: 0.191424, mae: 14.066392, mean_q: 19.969534\n",
            " 127808/150000: episode: 18620, duration: 0.065s, episode steps:   7, steps per second: 107, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.857 [0.000, 7.000],  loss: 0.384322, mae: 13.633466, mean_q: 19.970541\n",
            " 127816/150000: episode: 18621, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.875 [0.000, 8.000],  loss: 0.484876, mae: 13.922821, mean_q: 20.162943\n",
            " 127824/150000: episode: 18622, duration: 0.090s, episode steps:   8, steps per second:  88, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.401522, mae: 13.867866, mean_q: 19.887924\n",
            " 127829/150000: episode: 18623, duration: 0.055s, episode steps:   5, steps per second:  91, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.600 [0.000, 8.000],  loss: 0.900007, mae: 14.275996, mean_q: 20.149715\n",
            " 127835/150000: episode: 18624, duration: 0.054s, episode steps:   6, steps per second: 112, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.306086, mae: 14.022277, mean_q: 19.989918\n",
            " 127842/150000: episode: 18625, duration: 0.077s, episode steps:   7, steps per second:  91, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.429 [0.000, 7.000],  loss: 0.286049, mae: 14.078161, mean_q: 20.198133\n",
            " 127848/150000: episode: 18626, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 8.000],  loss: 0.638972, mae: 14.084740, mean_q: 20.043312\n",
            " 127857/150000: episode: 18627, duration: 0.084s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.262511, mae: 13.828217, mean_q: 20.002178\n",
            " 127860/150000: episode: 18628, duration: 0.032s, episode steps:   3, steps per second:  92, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.319056, mae: 13.794052, mean_q: 20.071100\n",
            " 127869/150000: episode: 18629, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.340751, mae: 13.746275, mean_q: 20.044701\n",
            " 127876/150000: episode: 18630, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.805920, mae: 14.079274, mean_q: 19.790930\n",
            " 127885/150000: episode: 18631, duration: 0.082s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.743593, mae: 14.079659, mean_q: 20.145708\n",
            " 127893/150000: episode: 18632, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.405001, mae: 14.147766, mean_q: 19.963539\n",
            " 127899/150000: episode: 18633, duration: 0.068s, episode steps:   6, steps per second:  88, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [1.000, 8.000],  loss: 0.346121, mae: 14.127978, mean_q: 20.472963\n",
            " 127906/150000: episode: 18634, duration: 0.068s, episode steps:   7, steps per second: 104, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.513235, mae: 13.912958, mean_q: 20.053715\n",
            " 127914/150000: episode: 18635, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.440994, mae: 13.941126, mean_q: 19.959904\n",
            " 127922/150000: episode: 18636, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.246024, mae: 13.937203, mean_q: 20.093391\n",
            " 127927/150000: episode: 18637, duration: 0.048s, episode steps:   5, steps per second: 104, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.895121, mae: 14.526011, mean_q: 20.022390\n",
            " 127935/150000: episode: 18638, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.237145, mae: 14.111737, mean_q: 19.792250\n",
            " 127944/150000: episode: 18639, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.483055, mae: 13.922450, mean_q: 19.957466\n",
            " 127949/150000: episode: 18640, duration: 0.045s, episode steps:   5, steps per second: 111, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.400 [2.000, 8.000],  loss: 0.524143, mae: 14.402344, mean_q: 20.070107\n",
            " 127958/150000: episode: 18641, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.252456, mae: 13.965503, mean_q: 20.081490\n",
            " 127965/150000: episode: 18642, duration: 0.068s, episode steps:   7, steps per second: 103, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.286 [0.000, 8.000],  loss: 0.212175, mae: 13.942664, mean_q: 20.070530\n",
            " 127973/150000: episode: 18643, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.385558, mae: 13.730924, mean_q: 20.020020\n",
            " 127978/150000: episode: 18644, duration: 0.049s, episode steps:   5, steps per second: 103, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.269961, mae: 13.654074, mean_q: 19.921289\n",
            " 127985/150000: episode: 18645, duration: 0.076s, episode steps:   7, steps per second:  92, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.429 [0.000, 8.000],  loss: 0.505717, mae: 14.128382, mean_q: 19.993788\n",
            " 127992/150000: episode: 18646, duration: 0.061s, episode steps:   7, steps per second: 116, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.857 [0.000, 7.000],  loss: 0.350027, mae: 13.882104, mean_q: 19.996967\n",
            " 128001/150000: episode: 18647, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.239273, mae: 13.947894, mean_q: 20.087044\n",
            " 128006/150000: episode: 18648, duration: 0.062s, episode steps:   5, steps per second:  80, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.200 [0.000, 8.000],  loss: 0.251169, mae: 13.687167, mean_q: 20.037907\n",
            " 128014/150000: episode: 18649, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.343073, mae: 13.761694, mean_q: 19.993587\n",
            " 128022/150000: episode: 18650, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.219174, mae: 14.187391, mean_q: 19.980856\n",
            " 128030/150000: episode: 18651, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.174155, mae: 13.771616, mean_q: 20.155640\n",
            " 128037/150000: episode: 18652, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.429 [0.000, 8.000],  loss: 0.282112, mae: 13.894461, mean_q: 20.023359\n",
            " 128042/150000: episode: 18653, duration: 0.050s, episode steps:   5, steps per second: 101, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.249003, mae: 13.705284, mean_q: 20.146811\n",
            " 128050/150000: episode: 18654, duration: 0.070s, episode steps:   8, steps per second: 114, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.333474, mae: 13.762852, mean_q: 19.994865\n",
            " 128052/150000: episode: 18655, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 1.156047, mae: 13.589637, mean_q: 19.751244\n",
            " 128061/150000: episode: 18656, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.229808, mae: 13.874668, mean_q: 20.116636\n",
            " 128064/150000: episode: 18657, duration: 0.035s, episode steps:   3, steps per second:  86, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 5.333 [0.000, 8.000],  loss: 0.370277, mae: 14.064359, mean_q: 20.185732\n",
            " 128073/150000: episode: 18658, duration: 0.078s, episode steps:   9, steps per second: 115, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.259870, mae: 14.084383, mean_q: 19.851316\n",
            " 128082/150000: episode: 18659, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.348759, mae: 13.717341, mean_q: 19.926413\n",
            " 128089/150000: episode: 18660, duration: 0.062s, episode steps:   7, steps per second: 114, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.369776, mae: 13.852987, mean_q: 20.028608\n",
            " 128098/150000: episode: 18661, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.444733, mae: 14.128839, mean_q: 19.907328\n",
            " 128106/150000: episode: 18662, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.250 [1.000, 8.000],  loss: 0.370622, mae: 14.125574, mean_q: 20.170403\n",
            " 128114/150000: episode: 18663, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.245954, mae: 13.529931, mean_q: 19.833717\n",
            " 128120/150000: episode: 18664, duration: 0.063s, episode steps:   6, steps per second:  95, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [1.000, 8.000],  loss: 0.218094, mae: 13.986580, mean_q: 19.938082\n",
            " 128129/150000: episode: 18665, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.238748, mae: 13.994695, mean_q: 20.050436\n",
            " 128136/150000: episode: 18666, duration: 0.061s, episode steps:   7, steps per second: 115, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.320916, mae: 14.020923, mean_q: 19.849400\n",
            " 128144/150000: episode: 18667, duration: 0.068s, episode steps:   8, steps per second: 117, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.618633, mae: 13.866287, mean_q: 19.903904\n",
            " 128151/150000: episode: 18668, duration: 0.059s, episode steps:   7, steps per second: 118, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.354500, mae: 14.061065, mean_q: 19.766649\n",
            " 128158/150000: episode: 18669, duration: 0.071s, episode steps:   7, steps per second:  99, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 7.000],  loss: 0.222818, mae: 14.369069, mean_q: 20.044086\n",
            " 128166/150000: episode: 18670, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.981782, mae: 13.614733, mean_q: 19.987211\n",
            " 128174/150000: episode: 18671, duration: 0.069s, episode steps:   8, steps per second: 116, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.465682, mae: 14.081831, mean_q: 19.883305\n",
            " 128180/150000: episode: 18672, duration: 0.054s, episode steps:   6, steps per second: 112, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.000 [0.000, 7.000],  loss: 0.368370, mae: 13.813045, mean_q: 20.273136\n",
            " 128189/150000: episode: 18673, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.265941, mae: 13.962952, mean_q: 19.986135\n",
            " 128194/150000: episode: 18674, duration: 0.050s, episode steps:   5, steps per second:  99, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.800 [1.000, 6.000],  loss: 0.295066, mae: 13.581439, mean_q: 20.110670\n",
            " 128200/150000: episode: 18675, duration: 0.063s, episode steps:   6, steps per second:  95, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 7.000],  loss: 0.222893, mae: 13.511669, mean_q: 19.847101\n",
            " 128207/150000: episode: 18676, duration: 0.067s, episode steps:   7, steps per second: 104, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.286 [0.000, 8.000],  loss: 0.652680, mae: 13.780747, mean_q: 19.835392\n",
            " 128214/150000: episode: 18677, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.000 [0.000, 7.000],  loss: 0.458115, mae: 13.875308, mean_q: 19.917576\n",
            " 128219/150000: episode: 18678, duration: 0.058s, episode steps:   5, steps per second:  87, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.289071, mae: 14.247049, mean_q: 20.137104\n",
            " 128227/150000: episode: 18679, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.338236, mae: 13.826082, mean_q: 19.949127\n",
            " 128233/150000: episode: 18680, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.167 [0.000, 6.000],  loss: 0.502900, mae: 14.040359, mean_q: 20.129370\n",
            " 128241/150000: episode: 18681, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.267746, mae: 13.900287, mean_q: 20.020824\n",
            " 128250/150000: episode: 18682, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.185657, mae: 13.890733, mean_q: 20.129066\n",
            " 128258/150000: episode: 18683, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.277862, mae: 13.984306, mean_q: 19.819984\n",
            " 128267/150000: episode: 18684, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.385621, mae: 13.916050, mean_q: 19.988550\n",
            " 128274/150000: episode: 18685, duration: 0.068s, episode steps:   7, steps per second: 103, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.310835, mae: 13.653552, mean_q: 20.005138\n",
            " 128282/150000: episode: 18686, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.525072, mae: 13.707084, mean_q: 20.129166\n",
            " 128289/150000: episode: 18687, duration: 0.077s, episode steps:   7, steps per second:  91, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.417269, mae: 13.709264, mean_q: 20.119383\n",
            " 128297/150000: episode: 18688, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.485353, mae: 14.054417, mean_q: 20.058708\n",
            " 128306/150000: episode: 18689, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.234274, mae: 13.763618, mean_q: 20.006466\n",
            " 128313/150000: episode: 18690, duration: 0.099s, episode steps:   7, steps per second:  71, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.571 [0.000, 7.000],  loss: 0.216830, mae: 13.722020, mean_q: 20.018795\n",
            " 128318/150000: episode: 18691, duration: 0.057s, episode steps:   5, steps per second:  88, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [1.000, 8.000],  loss: 0.382943, mae: 13.721316, mean_q: 19.930225\n",
            " 128325/150000: episode: 18692, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.429 [0.000, 8.000],  loss: 0.448850, mae: 13.841285, mean_q: 19.980909\n",
            " 128331/150000: episode: 18693, duration: 0.056s, episode steps:   6, steps per second: 108, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.281662, mae: 14.059911, mean_q: 20.079344\n",
            " 128340/150000: episode: 18694, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.300753, mae: 13.499155, mean_q: 20.074446\n",
            " 128347/150000: episode: 18695, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.224377, mae: 13.673345, mean_q: 20.104214\n",
            " 128355/150000: episode: 18696, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.400423, mae: 13.807928, mean_q: 19.944355\n",
            " 128364/150000: episode: 18697, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.273443, mae: 13.737059, mean_q: 20.073046\n",
            " 128373/150000: episode: 18698, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.215738, mae: 13.928665, mean_q: 20.030079\n",
            " 128379/150000: episode: 18699, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.167 [1.000, 6.000],  loss: 0.172699, mae: 13.940674, mean_q: 20.088081\n",
            " 128384/150000: episode: 18700, duration: 0.051s, episode steps:   5, steps per second:  97, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.600 [2.000, 8.000],  loss: 0.559876, mae: 13.759125, mean_q: 20.162327\n",
            " 128389/150000: episode: 18701, duration: 0.059s, episode steps:   5, steps per second:  85, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.200 [0.000, 8.000],  loss: 0.284699, mae: 13.860127, mean_q: 20.007563\n",
            " 128393/150000: episode: 18702, duration: 0.042s, episode steps:   4, steps per second:  94, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.250 [1.000, 6.000],  loss: 0.307630, mae: 13.896158, mean_q: 19.903976\n",
            " 128400/150000: episode: 18703, duration: 0.067s, episode steps:   7, steps per second: 104, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.400075, mae: 13.792336, mean_q: 20.065046\n",
            " 128407/150000: episode: 18704, duration: 0.084s, episode steps:   7, steps per second:  83, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.310027, mae: 13.975388, mean_q: 19.952057\n",
            " 128416/150000: episode: 18705, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.602718, mae: 13.812304, mean_q: 19.958252\n",
            " 128422/150000: episode: 18706, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.449954, mae: 14.115559, mean_q: 20.089930\n",
            " 128431/150000: episode: 18707, duration: 0.095s, episode steps:   9, steps per second:  94, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.535038, mae: 13.893045, mean_q: 20.112679\n",
            " 128436/150000: episode: 18708, duration: 0.048s, episode steps:   5, steps per second: 105, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.000 [3.000, 7.000],  loss: 0.483351, mae: 13.874243, mean_q: 20.073641\n",
            " 128444/150000: episode: 18709, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.357468, mae: 13.536439, mean_q: 20.029625\n",
            " 128452/150000: episode: 18710, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.300012, mae: 14.134903, mean_q: 20.284340\n",
            " 128460/150000: episode: 18711, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.478794, mae: 13.858002, mean_q: 19.882479\n",
            " 128467/150000: episode: 18712, duration: 0.063s, episode steps:   7, steps per second: 111, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.375067, mae: 14.044362, mean_q: 19.986374\n",
            " 128473/150000: episode: 18713, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.394710, mae: 14.029805, mean_q: 20.172186\n",
            " 128478/150000: episode: 18714, duration: 0.047s, episode steps:   5, steps per second: 107, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.400 [0.000, 5.000],  loss: 0.379758, mae: 14.051486, mean_q: 19.952305\n",
            " 128486/150000: episode: 18715, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.289840, mae: 14.000254, mean_q: 20.080561\n",
            " 128492/150000: episode: 18716, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.000 [0.000, 8.000],  loss: 0.470032, mae: 14.207639, mean_q: 20.114988\n",
            " 128498/150000: episode: 18717, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.634059, mae: 13.763878, mean_q: 19.823557\n",
            " 128504/150000: episode: 18718, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [1.000, 8.000],  loss: 0.413553, mae: 13.621845, mean_q: 20.130543\n",
            " 128509/150000: episode: 18719, duration: 0.067s, episode steps:   5, steps per second:  74, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.800 [0.000, 5.000],  loss: 0.715588, mae: 13.798945, mean_q: 20.047136\n",
            " 128517/150000: episode: 18720, duration: 0.089s, episode steps:   8, steps per second:  89, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.277502, mae: 13.880435, mean_q: 20.103043\n",
            " 128524/150000: episode: 18721, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.233411, mae: 13.682109, mean_q: 20.045956\n",
            " 128531/150000: episode: 18722, duration: 0.111s, episode steps:   7, steps per second:  63, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.182686, mae: 14.001142, mean_q: 20.025949\n",
            " 128537/150000: episode: 18723, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [2.000, 7.000],  loss: 0.438390, mae: 13.872449, mean_q: 19.864439\n",
            " 128540/150000: episode: 18724, duration: 0.054s, episode steps:   3, steps per second:  56, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 3.000 [2.000, 5.000],  loss: 0.152923, mae: 13.923726, mean_q: 20.111170\n",
            " 128542/150000: episode: 18725, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 0.000 [0.000, 0.000],  loss: 0.339190, mae: 14.389320, mean_q: 20.167540\n",
            " 128547/150000: episode: 18726, duration: 0.092s, episode steps:   5, steps per second:  54, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.200 [0.000, 8.000],  loss: 0.283462, mae: 13.273311, mean_q: 20.028528\n",
            " 128555/150000: episode: 18727, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.396253, mae: 14.035975, mean_q: 19.900339\n",
            " 128563/150000: episode: 18728, duration: 0.146s, episode steps:   8, steps per second:  55, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.367573, mae: 13.839518, mean_q: 19.958321\n",
            " 128570/150000: episode: 18729, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 5.286 [0.000, 8.000],  loss: 0.217902, mae: 13.529071, mean_q: 19.974539\n",
            " 128577/150000: episode: 18730, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.297202, mae: 13.934584, mean_q: 20.060450\n",
            " 128584/150000: episode: 18731, duration: 0.122s, episode steps:   7, steps per second:  58, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 5.143 [0.000, 8.000],  loss: 0.251718, mae: 13.918699, mean_q: 19.953897\n",
            " 128592/150000: episode: 18732, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.125 [0.000, 8.000],  loss: 0.175398, mae: 13.825848, mean_q: 19.966864\n",
            " 128601/150000: episode: 18733, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.238527, mae: 13.862200, mean_q: 20.210789\n",
            " 128609/150000: episode: 18734, duration: 0.127s, episode steps:   8, steps per second:  63, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.553285, mae: 13.624730, mean_q: 19.902140\n",
            " 128617/150000: episode: 18735, duration: 0.126s, episode steps:   8, steps per second:  63, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.245481, mae: 13.834224, mean_q: 20.217480\n",
            " 128623/150000: episode: 18736, duration: 0.087s, episode steps:   6, steps per second:  69, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [0.000, 7.000],  loss: 0.222104, mae: 13.949291, mean_q: 20.023006\n",
            " 128630/150000: episode: 18737, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 5.286 [2.000, 8.000],  loss: 0.495890, mae: 14.167336, mean_q: 20.145639\n",
            " 128636/150000: episode: 18738, duration: 0.102s, episode steps:   6, steps per second:  59, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [1.000, 8.000],  loss: 0.265563, mae: 13.784280, mean_q: 20.061115\n",
            " 128639/150000: episode: 18739, duration: 0.054s, episode steps:   3, steps per second:  55, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 5.667 [5.000, 7.000],  loss: 0.349911, mae: 13.931458, mean_q: 19.867056\n",
            " 128646/150000: episode: 18740, duration: 0.118s, episode steps:   7, steps per second:  59, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.434372, mae: 13.613907, mean_q: 20.157711\n",
            " 128652/150000: episode: 18741, duration: 0.097s, episode steps:   6, steps per second:  62, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 7.000],  loss: 0.182309, mae: 13.781326, mean_q: 20.135822\n",
            " 128658/150000: episode: 18742, duration: 0.091s, episode steps:   6, steps per second:  66, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.000 [0.000, 8.000],  loss: 0.423954, mae: 13.983184, mean_q: 19.813663\n",
            " 128666/150000: episode: 18743, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.551619, mae: 14.007136, mean_q: 20.023273\n",
            " 128675/150000: episode: 18744, duration: 0.136s, episode steps:   9, steps per second:  66, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.271834, mae: 13.880371, mean_q: 19.787039\n",
            " 128682/150000: episode: 18745, duration: 0.102s, episode steps:   7, steps per second:  69, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.714 [0.000, 8.000],  loss: 0.348084, mae: 13.979566, mean_q: 20.133760\n",
            " 128686/150000: episode: 18746, duration: 0.062s, episode steps:   4, steps per second:  65, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.250 [0.000, 8.000],  loss: 0.202185, mae: 13.991003, mean_q: 20.121067\n",
            " 128694/150000: episode: 18747, duration: 0.110s, episode steps:   8, steps per second:  72, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.350376, mae: 13.805355, mean_q: 20.090487\n",
            " 128699/150000: episode: 18748, duration: 0.076s, episode steps:   5, steps per second:  66, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.600 [2.000, 7.000],  loss: 0.236359, mae: 13.789798, mean_q: 20.284172\n",
            " 128708/150000: episode: 18749, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.312017, mae: 13.938029, mean_q: 19.892548\n",
            " 128713/150000: episode: 18750, duration: 0.081s, episode steps:   5, steps per second:  62, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [0.000, 8.000],  loss: 0.256737, mae: 14.040808, mean_q: 20.201569\n",
            " 128722/150000: episode: 18751, duration: 0.121s, episode steps:   9, steps per second:  74, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.220431, mae: 13.918035, mean_q: 20.099150\n",
            " 128731/150000: episode: 18752, duration: 0.132s, episode steps:   9, steps per second:  68, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.192271, mae: 14.083118, mean_q: 20.092211\n",
            " 128739/150000: episode: 18753, duration: 0.109s, episode steps:   8, steps per second:  74, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.250 [0.000, 8.000],  loss: 0.168932, mae: 14.015351, mean_q: 19.953585\n",
            " 128747/150000: episode: 18754, duration: 0.117s, episode steps:   8, steps per second:  69, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.206639, mae: 14.093582, mean_q: 20.050888\n",
            " 128752/150000: episode: 18755, duration: 0.075s, episode steps:   5, steps per second:  66, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.000 [0.000, 5.000],  loss: 0.220046, mae: 13.924260, mean_q: 19.952396\n",
            " 128760/150000: episode: 18756, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.250 [0.000, 8.000],  loss: 0.487645, mae: 14.264539, mean_q: 20.028316\n",
            " 128767/150000: episode: 18757, duration: 0.087s, episode steps:   7, steps per second:  80, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [1.000, 8.000],  loss: 0.328397, mae: 13.832444, mean_q: 19.887318\n",
            " 128772/150000: episode: 18758, duration: 0.049s, episode steps:   5, steps per second: 102, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.800 [2.000, 7.000],  loss: 0.197401, mae: 14.204338, mean_q: 20.278088\n",
            " 128780/150000: episode: 18759, duration: 0.070s, episode steps:   8, steps per second: 114, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.346091, mae: 14.016499, mean_q: 20.048777\n",
            " 128789/150000: episode: 18760, duration: 0.089s, episode steps:   9, steps per second: 102, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.398699, mae: 14.110474, mean_q: 20.035309\n",
            " 128798/150000: episode: 18761, duration: 0.102s, episode steps:   9, steps per second:  89, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.164925, mae: 13.831328, mean_q: 20.178280\n",
            " 128804/150000: episode: 18762, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 7.000],  loss: 0.223838, mae: 14.042171, mean_q: 20.162138\n",
            " 128810/150000: episode: 18763, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2.833 [0.000, 6.000],  loss: 0.530621, mae: 13.732907, mean_q: 19.994635\n",
            " 128819/150000: episode: 18764, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.464192, mae: 13.957418, mean_q: 20.038986\n",
            " 128826/150000: episode: 18765, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.227192, mae: 13.850383, mean_q: 20.190619\n",
            " 128831/150000: episode: 18766, duration: 0.045s, episode steps:   5, steps per second: 112, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.181504, mae: 14.057089, mean_q: 20.040777\n",
            " 128834/150000: episode: 18767, duration: 0.043s, episode steps:   3, steps per second:  69, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 3.333 [2.000, 6.000],  loss: 0.194527, mae: 13.985705, mean_q: 20.048037\n",
            " 128841/150000: episode: 18768, duration: 0.070s, episode steps:   7, steps per second: 100, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.334014, mae: 13.909570, mean_q: 20.018988\n",
            " 128850/150000: episode: 18769, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.191782, mae: 14.069299, mean_q: 20.204710\n",
            " 128855/150000: episode: 18770, duration: 0.046s, episode steps:   5, steps per second: 109, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.800 [2.000, 8.000],  loss: 0.233822, mae: 14.008204, mean_q: 20.047480\n",
            " 128858/150000: episode: 18771, duration: 0.043s, episode steps:   3, steps per second:  70, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 4.667 [0.000, 7.000],  loss: 0.406146, mae: 14.013560, mean_q: 20.300417\n",
            " 128867/150000: episode: 18772, duration: 0.077s, episode steps:   9, steps per second: 117, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.318711, mae: 14.104597, mean_q: 20.201202\n",
            " 128871/150000: episode: 18773, duration: 0.039s, episode steps:   4, steps per second: 103, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 2.750 [0.000, 6.000],  loss: 0.310665, mae: 13.530787, mean_q: 19.994837\n",
            " 128875/150000: episode: 18774, duration: 0.039s, episode steps:   4, steps per second: 103, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.250 [1.000, 6.000],  loss: 0.171038, mae: 14.019732, mean_q: 20.192505\n",
            " 128883/150000: episode: 18775, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.268834, mae: 13.818705, mean_q: 20.140793\n",
            " 128891/150000: episode: 18776, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.377276, mae: 13.815039, mean_q: 20.018112\n",
            " 128898/150000: episode: 18777, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.204382, mae: 13.816323, mean_q: 20.016115\n",
            " 128907/150000: episode: 18778, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.372008, mae: 13.973066, mean_q: 20.027039\n",
            " 128914/150000: episode: 18779, duration: 0.078s, episode steps:   7, steps per second:  90, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.596733, mae: 13.951838, mean_q: 20.214878\n",
            " 128920/150000: episode: 18780, duration: 0.055s, episode steps:   6, steps per second: 108, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [1.000, 7.000],  loss: 0.205671, mae: 13.838039, mean_q: 20.124029\n",
            " 128928/150000: episode: 18781, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.209330, mae: 14.028920, mean_q: 20.225410\n",
            " 128937/150000: episode: 18782, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.222 [0.000, 8.000],  loss: 0.313186, mae: 13.990510, mean_q: 20.179232\n",
            " 128943/150000: episode: 18783, duration: 0.054s, episode steps:   6, steps per second: 111, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 7.000],  loss: 0.354299, mae: 13.483356, mean_q: 20.036985\n",
            " 128951/150000: episode: 18784, duration: 0.081s, episode steps:   8, steps per second:  98, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.750 [1.000, 7.000],  loss: 0.235085, mae: 14.071690, mean_q: 20.124699\n",
            " 128959/150000: episode: 18785, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.898883, mae: 13.953564, mean_q: 19.894680\n",
            " 128966/150000: episode: 18786, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.263643, mae: 14.029941, mean_q: 20.241383\n",
            " 128974/150000: episode: 18787, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.500 [0.000, 8.000],  loss: 0.243799, mae: 13.999355, mean_q: 20.146915\n",
            " 128978/150000: episode: 18788, duration: 0.042s, episode steps:   4, steps per second:  96, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.000 [2.000, 7.000],  loss: 0.455586, mae: 13.693653, mean_q: 19.813435\n",
            " 128985/150000: episode: 18789, duration: 0.080s, episode steps:   7, steps per second:  88, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.714 [2.000, 8.000],  loss: 0.271491, mae: 13.894846, mean_q: 20.232208\n",
            " 128994/150000: episode: 18790, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.308546, mae: 13.893140, mean_q: 20.120123\n",
            " 128999/150000: episode: 18791, duration: 0.057s, episode steps:   5, steps per second:  87, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.600 [2.000, 8.000],  loss: 0.506072, mae: 13.814615, mean_q: 20.031506\n",
            " 129002/150000: episode: 18792, duration: 0.032s, episode steps:   3, steps per second:  94, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 2.000 [0.000, 6.000],  loss: 0.595335, mae: 13.948041, mean_q: 19.557158\n",
            " 129008/150000: episode: 18793, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.500 [0.000, 8.000],  loss: 0.427735, mae: 14.053719, mean_q: 20.072609\n",
            " 129013/150000: episode: 18794, duration: 0.047s, episode steps:   5, steps per second: 107, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.349369, mae: 13.918938, mean_q: 20.127964\n",
            " 129021/150000: episode: 18795, duration: 0.099s, episode steps:   8, steps per second:  80, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.237014, mae: 13.981581, mean_q: 20.213261\n",
            " 129027/150000: episode: 18796, duration: 0.054s, episode steps:   6, steps per second: 111, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.391969, mae: 13.978031, mean_q: 19.861635\n",
            " 129030/150000: episode: 18797, duration: 0.031s, episode steps:   3, steps per second:  96, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 5.000 [3.000, 6.000],  loss: 0.267109, mae: 13.816998, mean_q: 20.242922\n",
            " 129039/150000: episode: 18798, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.369181, mae: 13.677797, mean_q: 20.187281\n",
            " 129047/150000: episode: 18799, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.750 [1.000, 8.000],  loss: 0.348486, mae: 14.097320, mean_q: 19.997259\n",
            " 129052/150000: episode: 18800, duration: 0.055s, episode steps:   5, steps per second:  91, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 8.000],  loss: 0.248466, mae: 13.891187, mean_q: 20.230288\n",
            " 129059/150000: episode: 18801, duration: 0.062s, episode steps:   7, steps per second: 113, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.360814, mae: 13.617163, mean_q: 20.032301\n",
            " 129067/150000: episode: 18802, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.239307, mae: 14.033239, mean_q: 20.304752\n",
            " 129074/150000: episode: 18803, duration: 0.077s, episode steps:   7, steps per second:  90, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.190664, mae: 13.926111, mean_q: 20.168945\n",
            " 129079/150000: episode: 18804, duration: 0.049s, episode steps:   5, steps per second: 103, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.400 [2.000, 8.000],  loss: 0.252740, mae: 13.764715, mean_q: 20.134405\n",
            " 129081/150000: episode: 18805, duration: 0.026s, episode steps:   2, steps per second:  76, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 7.000 [7.000, 7.000],  loss: 0.336535, mae: 13.982628, mean_q: 19.628967\n",
            " 129088/150000: episode: 18806, duration: 0.070s, episode steps:   7, steps per second: 100, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.233849, mae: 14.054460, mean_q: 19.982939\n",
            " 129097/150000: episode: 18807, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.778 [0.000, 8.000],  loss: 0.219139, mae: 13.650355, mean_q: 20.236595\n",
            " 129102/150000: episode: 18808, duration: 0.047s, episode steps:   5, steps per second: 106, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.251406, mae: 14.122904, mean_q: 19.866232\n",
            " 129109/150000: episode: 18809, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 7.000],  loss: 0.464077, mae: 13.961622, mean_q: 20.033503\n",
            " 129112/150000: episode: 18810, duration: 0.034s, episode steps:   3, steps per second:  88, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 4.000 [2.000, 5.000],  loss: 0.230263, mae: 14.101962, mean_q: 19.802238\n",
            " 129120/150000: episode: 18811, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.625 [0.000, 8.000],  loss: 0.432775, mae: 13.661033, mean_q: 19.955200\n",
            " 129126/150000: episode: 18812, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [1.000, 7.000],  loss: 0.219636, mae: 14.136718, mean_q: 20.014729\n",
            " 129133/150000: episode: 18813, duration: 0.070s, episode steps:   7, steps per second:  99, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 7.000],  loss: 0.176056, mae: 14.107773, mean_q: 20.248175\n",
            " 129142/150000: episode: 18814, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.372063, mae: 13.970213, mean_q: 20.039211\n",
            " 129150/150000: episode: 18815, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.250 [0.000, 7.000],  loss: 0.208972, mae: 14.081361, mean_q: 20.085482\n",
            " 129158/150000: episode: 18816, duration: 0.072s, episode steps:   8, steps per second: 112, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.429624, mae: 14.124277, mean_q: 19.970730\n",
            " 129164/150000: episode: 18817, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.234503, mae: 14.034455, mean_q: 20.013144\n",
            " 129171/150000: episode: 18818, duration: 0.073s, episode steps:   7, steps per second:  96, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.198119, mae: 14.195921, mean_q: 20.271130\n",
            " 129178/150000: episode: 18819, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.507253, mae: 13.998795, mean_q: 19.895472\n",
            " 129187/150000: episode: 18820, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.290947, mae: 14.218772, mean_q: 20.061424\n",
            " 129191/150000: episode: 18821, duration: 0.040s, episode steps:   4, steps per second:  99, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.250 [3.000, 8.000],  loss: 0.704873, mae: 14.186926, mean_q: 19.974237\n",
            " 129200/150000: episode: 18822, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.269406, mae: 14.084146, mean_q: 20.102810\n",
            " 129204/150000: episode: 18823, duration: 0.042s, episode steps:   4, steps per second:  96, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 2.250 [0.000, 5.000],  loss: 0.571012, mae: 14.123549, mean_q: 20.406898\n",
            " 129212/150000: episode: 18824, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.371185, mae: 14.313108, mean_q: 20.070713\n",
            " 129215/150000: episode: 18825, duration: 0.032s, episode steps:   3, steps per second:  94, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 5.333 [4.000, 6.000],  loss: 0.397260, mae: 13.870293, mean_q: 20.125856\n",
            " 129219/150000: episode: 18826, duration: 0.049s, episode steps:   4, steps per second:  82, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.500 [0.000, 6.000],  loss: 0.268257, mae: 14.010123, mean_q: 20.412548\n",
            " 129228/150000: episode: 18827, duration: 0.080s, episode steps:   9, steps per second: 112, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.547455, mae: 13.789567, mean_q: 19.921986\n",
            " 129236/150000: episode: 18828, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.342546, mae: 14.391899, mean_q: 20.184967\n",
            " 129245/150000: episode: 18829, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.357148, mae: 14.042435, mean_q: 20.087044\n",
            " 129254/150000: episode: 18830, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.556 [1.000, 8.000],  loss: 0.353980, mae: 14.043161, mean_q: 20.092926\n",
            " 129263/150000: episode: 18831, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.464925, mae: 14.061172, mean_q: 20.017433\n",
            " 129270/150000: episode: 18832, duration: 0.065s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.267191, mae: 13.993272, mean_q: 19.983145\n",
            " 129275/150000: episode: 18833, duration: 0.054s, episode steps:   5, steps per second:  92, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.200 [0.000, 8.000],  loss: 1.078913, mae: 14.053372, mean_q: 20.103273\n",
            " 129282/150000: episode: 18834, duration: 0.076s, episode steps:   7, steps per second:  92, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.244200, mae: 13.830061, mean_q: 20.092661\n",
            " 129290/150000: episode: 18835, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.194655, mae: 13.946596, mean_q: 20.158169\n",
            " 129298/150000: episode: 18836, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.187754, mae: 13.610971, mean_q: 19.961544\n",
            " 129303/150000: episode: 18837, duration: 0.049s, episode steps:   5, steps per second: 102, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.154553, mae: 14.315584, mean_q: 20.183369\n",
            " 129311/150000: episode: 18838, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.255839, mae: 14.293291, mean_q: 19.972990\n",
            " 129317/150000: episode: 18839, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 6.000],  loss: 0.261470, mae: 13.913743, mean_q: 20.191324\n",
            " 129322/150000: episode: 18840, duration: 0.052s, episode steps:   5, steps per second:  96, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.789745, mae: 14.309186, mean_q: 19.991369\n",
            " 129330/150000: episode: 18841, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.213498, mae: 14.275519, mean_q: 20.075977\n",
            " 129338/150000: episode: 18842, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.478315, mae: 13.833170, mean_q: 19.843439\n",
            " 129343/150000: episode: 18843, duration: 0.048s, episode steps:   5, steps per second: 104, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.000 [3.000, 7.000],  loss: 0.258662, mae: 14.076056, mean_q: 20.075054\n",
            " 129352/150000: episode: 18844, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.284719, mae: 13.614230, mean_q: 20.109867\n",
            " 129360/150000: episode: 18845, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.275146, mae: 13.881850, mean_q: 20.094881\n",
            " 129368/150000: episode: 18846, duration: 0.085s, episode steps:   8, steps per second:  95, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.250 [0.000, 8.000],  loss: 0.642664, mae: 13.790344, mean_q: 20.090618\n",
            " 129374/150000: episode: 18847, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.333 [2.000, 8.000],  loss: 0.392447, mae: 13.643691, mean_q: 19.921652\n",
            " 129383/150000: episode: 18848, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.610516, mae: 13.834933, mean_q: 20.093019\n",
            " 129390/150000: episode: 18849, duration: 0.066s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.940846, mae: 13.993320, mean_q: 19.994043\n",
            " 129396/150000: episode: 18850, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [2.000, 8.000],  loss: 0.466758, mae: 13.776019, mean_q: 20.138062\n",
            " 129404/150000: episode: 18851, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.279856, mae: 13.889835, mean_q: 20.152782\n",
            " 129410/150000: episode: 18852, duration: 0.076s, episode steps:   6, steps per second:  79, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 8.000],  loss: 0.391306, mae: 13.966817, mean_q: 19.919296\n",
            " 129413/150000: episode: 18853, duration: 0.033s, episode steps:   3, steps per second:  90, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 3.667 [3.000, 5.000],  loss: 0.333044, mae: 13.759200, mean_q: 19.928837\n",
            " 129419/150000: episode: 18854, duration: 0.071s, episode steps:   6, steps per second:  85, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.667 [3.000, 8.000],  loss: 0.476238, mae: 14.369385, mean_q: 20.108149\n",
            " 129424/150000: episode: 18855, duration: 0.060s, episode steps:   5, steps per second:  83, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.503355, mae: 13.752153, mean_q: 19.923676\n",
            " 129432/150000: episode: 18856, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.000 [0.000, 7.000],  loss: 0.248966, mae: 13.710145, mean_q: 20.288982\n",
            " 129438/150000: episode: 18857, duration: 0.055s, episode steps:   6, steps per second: 110, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.000 [0.000, 7.000],  loss: 0.199202, mae: 13.858425, mean_q: 19.932608\n",
            " 129444/150000: episode: 18858, duration: 0.056s, episode steps:   6, steps per second: 108, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.259830, mae: 13.598820, mean_q: 19.982046\n",
            " 129447/150000: episode: 18859, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 6.333 [5.000, 7.000],  loss: 0.836346, mae: 13.624260, mean_q: 19.788952\n",
            " 129454/150000: episode: 18860, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.516586, mae: 14.208313, mean_q: 19.908657\n",
            " 129463/150000: episode: 18861, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.348386, mae: 14.024472, mean_q: 20.338442\n",
            " 129468/150000: episode: 18862, duration: 0.048s, episode steps:   5, steps per second: 104, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 6.000],  loss: 0.309768, mae: 13.647720, mean_q: 20.210653\n",
            " 129474/150000: episode: 18863, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 7.000],  loss: 0.233214, mae: 13.819084, mean_q: 19.977434\n",
            " 129482/150000: episode: 18864, duration: 0.070s, episode steps:   8, steps per second: 114, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.257723, mae: 13.814755, mean_q: 20.104780\n",
            " 129488/150000: episode: 18865, duration: 0.057s, episode steps:   6, steps per second: 106, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.333 [2.000, 8.000],  loss: 0.303837, mae: 14.126187, mean_q: 20.198709\n",
            " 129494/150000: episode: 18866, duration: 0.072s, episode steps:   6, steps per second:  84, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 7.000],  loss: 0.513271, mae: 14.071713, mean_q: 20.008051\n",
            " 129500/150000: episode: 18867, duration: 0.072s, episode steps:   6, steps per second:  83, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 7.000],  loss: 0.210401, mae: 13.901282, mean_q: 20.225636\n",
            " 129505/150000: episode: 18868, duration: 0.049s, episode steps:   5, steps per second: 101, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.000 [0.000, 5.000],  loss: 0.190247, mae: 14.247305, mean_q: 20.143837\n",
            " 129511/150000: episode: 18869, duration: 0.056s, episode steps:   6, steps per second: 108, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 7.000],  loss: 0.514135, mae: 13.752564, mean_q: 20.063477\n",
            " 129518/150000: episode: 18870, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.857 [1.000, 7.000],  loss: 0.433113, mae: 13.819689, mean_q: 19.987793\n",
            " 129524/150000: episode: 18871, duration: 0.055s, episode steps:   6, steps per second: 108, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [2.000, 8.000],  loss: 0.230489, mae: 13.974636, mean_q: 20.049812\n",
            " 129531/150000: episode: 18872, duration: 0.062s, episode steps:   7, steps per second: 112, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.517922, mae: 13.637649, mean_q: 20.047796\n",
            " 129537/150000: episode: 18873, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 7.000],  loss: 0.175769, mae: 13.995328, mean_q: 20.202225\n",
            " 129543/150000: episode: 18874, duration: 0.073s, episode steps:   6, steps per second:  82, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [1.000, 8.000],  loss: 0.296408, mae: 13.734428, mean_q: 20.062113\n",
            " 129552/150000: episode: 18875, duration: 0.080s, episode steps:   9, steps per second: 112, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.323895, mae: 13.826122, mean_q: 20.068316\n",
            " 129557/150000: episode: 18876, duration: 0.050s, episode steps:   5, steps per second:  99, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.800 [1.000, 5.000],  loss: 0.260480, mae: 14.044452, mean_q: 20.124655\n",
            " 129565/150000: episode: 18877, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.364965, mae: 14.141731, mean_q: 20.114708\n",
            " 129571/150000: episode: 18878, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [2.000, 7.000],  loss: 0.268054, mae: 14.130509, mean_q: 20.035292\n",
            " 129580/150000: episode: 18879, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.889 [1.000, 8.000],  loss: 0.251237, mae: 14.188609, mean_q: 20.068613\n",
            " 129586/150000: episode: 18880, duration: 0.067s, episode steps:   6, steps per second:  90, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 7.000],  loss: 0.362464, mae: 14.037467, mean_q: 19.843908\n",
            " 129591/150000: episode: 18881, duration: 0.057s, episode steps:   5, steps per second:  87, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.348430, mae: 13.874022, mean_q: 19.943470\n",
            " 129599/150000: episode: 18882, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.275986, mae: 14.034552, mean_q: 20.010298\n",
            " 129601/150000: episode: 18883, duration: 0.025s, episode steps:   2, steps per second:  79, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 5.000 [5.000, 5.000],  loss: 0.622251, mae: 13.802351, mean_q: 19.791557\n",
            " 129610/150000: episode: 18884, duration: 0.109s, episode steps:   9, steps per second:  83, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.318704, mae: 13.880795, mean_q: 20.193707\n",
            " 129615/150000: episode: 18885, duration: 0.058s, episode steps:   5, steps per second:  86, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.315643, mae: 14.358342, mean_q: 20.119534\n",
            " 129624/150000: episode: 18886, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.210184, mae: 13.919863, mean_q: 20.115990\n",
            " 129632/150000: episode: 18887, duration: 0.087s, episode steps:   8, steps per second:  91, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.250 [0.000, 8.000],  loss: 0.236496, mae: 14.129097, mean_q: 20.093353\n",
            " 129639/150000: episode: 18888, duration: 0.071s, episode steps:   7, steps per second:  99, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.233842, mae: 13.886956, mean_q: 19.956142\n",
            " 129647/150000: episode: 18889, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.176870, mae: 14.307356, mean_q: 20.101734\n",
            " 129649/150000: episode: 18890, duration: 0.025s, episode steps:   2, steps per second:  81, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 3.000 [3.000, 3.000],  loss: 0.108236, mae: 13.946722, mean_q: 19.948366\n",
            " 129654/150000: episode: 18891, duration: 0.062s, episode steps:   5, steps per second:  80, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 8.000],  loss: 0.270415, mae: 13.883985, mean_q: 19.969864\n",
            " 129662/150000: episode: 18892, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.519695, mae: 14.147399, mean_q: 20.126053\n",
            " 129670/150000: episode: 18893, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.249910, mae: 13.947733, mean_q: 19.955490\n",
            " 129675/150000: episode: 18894, duration: 0.062s, episode steps:   5, steps per second:  80, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.000 [0.000, 5.000],  loss: 0.180319, mae: 14.404818, mean_q: 20.237049\n",
            " 129684/150000: episode: 18895, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.209595, mae: 14.054638, mean_q: 20.267626\n",
            " 129693/150000: episode: 18896, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.282746, mae: 13.995382, mean_q: 19.872068\n",
            " 129701/150000: episode: 18897, duration: 0.095s, episode steps:   8, steps per second:  84, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.291011, mae: 14.158816, mean_q: 20.000286\n",
            " 129708/150000: episode: 18898, duration: 0.072s, episode steps:   7, steps per second:  97, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 7.000],  loss: 0.255819, mae: 13.942954, mean_q: 19.940722\n",
            " 129713/150000: episode: 18899, duration: 0.059s, episode steps:   5, steps per second:  85, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.600 [2.000, 8.000],  loss: 0.149673, mae: 14.182570, mean_q: 20.047962\n",
            " 129720/150000: episode: 18900, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.316658, mae: 13.739449, mean_q: 19.938402\n",
            " 129729/150000: episode: 18901, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.278545, mae: 13.903863, mean_q: 20.009010\n",
            " 129735/150000: episode: 18902, duration: 0.093s, episode steps:   6, steps per second:  64, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 8.000],  loss: 0.393400, mae: 13.951110, mean_q: 20.205751\n",
            " 129743/150000: episode: 18903, duration: 0.120s, episode steps:   8, steps per second:  67, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.394986, mae: 14.040214, mean_q: 19.882559\n",
            " 129752/150000: episode: 18904, duration: 0.142s, episode steps:   9, steps per second:  64, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.939557, mae: 13.826834, mean_q: 19.956072\n",
            " 129759/150000: episode: 18905, duration: 0.102s, episode steps:   7, steps per second:  69, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.341500, mae: 14.262956, mean_q: 20.195545\n",
            " 129764/150000: episode: 18906, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.600 [2.000, 8.000],  loss: 0.464848, mae: 13.571376, mean_q: 20.239233\n",
            " 129768/150000: episode: 18907, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 2.000 [0.000, 7.000],  loss: 0.223599, mae: 14.254455, mean_q: 19.973503\n",
            " 129775/150000: episode: 18908, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.378584, mae: 14.042491, mean_q: 19.946873\n",
            " 129782/150000: episode: 18909, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.308224, mae: 13.643207, mean_q: 20.064142\n",
            " 129791/150000: episode: 18910, duration: 0.126s, episode steps:   9, steps per second:  72, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.190124, mae: 14.140293, mean_q: 20.084932\n",
            " 129799/150000: episode: 18911, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.237924, mae: 13.766657, mean_q: 20.005186\n",
            " 129807/150000: episode: 18912, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.988593, mae: 13.817366, mean_q: 19.955692\n",
            " 129813/150000: episode: 18913, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.636049, mae: 14.238656, mean_q: 20.002352\n",
            " 129818/150000: episode: 18914, duration: 0.067s, episode steps:   5, steps per second:  74, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.800 [0.000, 8.000],  loss: 0.669482, mae: 13.993306, mean_q: 20.300098\n",
            " 129824/150000: episode: 18915, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.433593, mae: 13.914790, mean_q: 19.907146\n",
            " 129831/150000: episode: 18916, duration: 0.087s, episode steps:   7, steps per second:  81, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.408713, mae: 13.724528, mean_q: 20.097187\n",
            " 129837/150000: episode: 18917, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.167 [0.000, 8.000],  loss: 0.361735, mae: 13.959728, mean_q: 20.084736\n",
            " 129843/150000: episode: 18918, duration: 0.087s, episode steps:   6, steps per second:  69, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.230631, mae: 14.115265, mean_q: 20.115047\n",
            " 129851/150000: episode: 18919, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.819095, mae: 13.852112, mean_q: 19.988958\n",
            " 129857/150000: episode: 18920, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.167 [1.000, 8.000],  loss: 0.606838, mae: 14.172828, mean_q: 19.988298\n",
            " 129864/150000: episode: 18921, duration: 0.102s, episode steps:   7, steps per second:  69, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.441611, mae: 13.972719, mean_q: 20.285803\n",
            " 129870/150000: episode: 18922, duration: 0.092s, episode steps:   6, steps per second:  65, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.000 [0.000, 8.000],  loss: 0.407227, mae: 14.096173, mean_q: 20.214888\n",
            " 129879/150000: episode: 18923, duration: 0.138s, episode steps:   9, steps per second:  65, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.350541, mae: 13.620619, mean_q: 19.951437\n",
            " 129888/150000: episode: 18924, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.419024, mae: 13.859567, mean_q: 19.913076\n",
            " 129896/150000: episode: 18925, duration: 0.112s, episode steps:   8, steps per second:  72, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.355577, mae: 14.166598, mean_q: 20.125668\n",
            " 129899/150000: episode: 18926, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 4.000 [2.000, 8.000],  loss: 0.445390, mae: 13.864701, mean_q: 19.996788\n",
            " 129907/150000: episode: 18927, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.611500, mae: 13.909818, mean_q: 20.048344\n",
            " 129913/150000: episode: 18928, duration: 0.085s, episode steps:   6, steps per second:  70, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 6.000],  loss: 0.301020, mae: 13.872875, mean_q: 20.057770\n",
            " 129918/150000: episode: 18929, duration: 0.068s, episode steps:   5, steps per second:  74, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.555284, mae: 13.954135, mean_q: 20.105793\n",
            " 129925/150000: episode: 18930, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.368854, mae: 13.783928, mean_q: 19.937363\n",
            " 129934/150000: episode: 18931, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.432997, mae: 13.760661, mean_q: 20.294914\n",
            " 129940/150000: episode: 18932, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [1.000, 8.000],  loss: 0.222502, mae: 13.996184, mean_q: 19.893234\n",
            " 129948/150000: episode: 18933, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 5.000 [0.000, 8.000],  loss: 0.297744, mae: 13.951547, mean_q: 20.241474\n",
            " 129955/150000: episode: 18934, duration: 0.096s, episode steps:   7, steps per second:  73, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.857 [0.000, 8.000],  loss: 0.360865, mae: 13.764216, mean_q: 19.847027\n",
            " 129962/150000: episode: 18935, duration: 0.096s, episode steps:   7, steps per second:  73, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.467544, mae: 14.071006, mean_q: 20.014380\n",
            " 129968/150000: episode: 18936, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.194393, mae: 14.103237, mean_q: 20.061087\n",
            " 129974/150000: episode: 18937, duration: 0.094s, episode steps:   6, steps per second:  64, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.191373, mae: 13.718789, mean_q: 20.095314\n",
            " 129982/150000: episode: 18938, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.590759, mae: 13.938839, mean_q: 19.958977\n",
            " 129987/150000: episode: 18939, duration: 0.048s, episode steps:   5, steps per second: 103, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.324683, mae: 14.034528, mean_q: 20.072300\n",
            " 129995/150000: episode: 18940, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.258879, mae: 13.781845, mean_q: 19.950979\n",
            " 129998/150000: episode: 18941, duration: 0.036s, episode steps:   3, steps per second:  84, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 3.333 [2.000, 6.000],  loss: 0.561751, mae: 14.639539, mean_q: 19.827528\n",
            " 130006/150000: episode: 18942, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.750 [0.000, 8.000],  loss: 0.961328, mae: 14.049966, mean_q: 20.071949\n",
            " 130014/150000: episode: 18943, duration: 0.080s, episode steps:   8, steps per second: 101, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.277134, mae: 14.047879, mean_q: 20.064316\n",
            " 130019/150000: episode: 18944, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.385557, mae: 13.818036, mean_q: 20.088919\n",
            " 130025/150000: episode: 18945, duration: 0.071s, episode steps:   6, steps per second:  85, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.667 [1.000, 6.000],  loss: 0.236997, mae: 14.021297, mean_q: 19.834059\n",
            " 130033/150000: episode: 18946, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.269048, mae: 13.867247, mean_q: 20.084366\n",
            " 130041/150000: episode: 18947, duration: 0.085s, episode steps:   8, steps per second:  95, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.258665, mae: 13.783103, mean_q: 20.104122\n",
            " 130048/150000: episode: 18948, duration: 0.080s, episode steps:   7, steps per second:  87, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.281604, mae: 13.894037, mean_q: 20.117880\n",
            " 130053/150000: episode: 18949, duration: 0.055s, episode steps:   5, steps per second:  91, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.304797, mae: 13.716046, mean_q: 19.955044\n",
            " 130056/150000: episode: 18950, duration: 0.034s, episode steps:   3, steps per second:  89, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 6.667 [6.000, 7.000],  loss: 0.173264, mae: 14.554805, mean_q: 20.073523\n",
            " 130060/150000: episode: 18951, duration: 0.052s, episode steps:   4, steps per second:  78, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 2.250 [0.000, 7.000],  loss: 0.322422, mae: 14.030289, mean_q: 20.022430\n",
            " 130067/150000: episode: 18952, duration: 0.075s, episode steps:   7, steps per second:  93, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.173576, mae: 13.980249, mean_q: 19.994190\n",
            " 130075/150000: episode: 18953, duration: 0.077s, episode steps:   8, steps per second: 105, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.503681, mae: 14.075594, mean_q: 19.774860\n",
            " 130081/150000: episode: 18954, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 7.000],  loss: 0.582798, mae: 13.899227, mean_q: 20.086054\n",
            " 130090/150000: episode: 18955, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.392892, mae: 13.788058, mean_q: 19.911367\n",
            " 130098/150000: episode: 18956, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.541330, mae: 13.689554, mean_q: 20.005037\n",
            " 130105/150000: episode: 18957, duration: 0.080s, episode steps:   7, steps per second:  88, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 7.000],  loss: 0.583995, mae: 14.076343, mean_q: 20.028763\n",
            " 130114/150000: episode: 18958, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.449960, mae: 13.810076, mean_q: 19.744986\n",
            " 130121/150000: episode: 18959, duration: 0.078s, episode steps:   7, steps per second:  90, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.322039, mae: 13.992262, mean_q: 20.020395\n",
            " 130128/150000: episode: 18960, duration: 0.082s, episode steps:   7, steps per second:  85, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.482560, mae: 14.222616, mean_q: 20.141668\n",
            " 130136/150000: episode: 18961, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.264259, mae: 13.986975, mean_q: 19.968149\n",
            " 130145/150000: episode: 18962, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.454262, mae: 13.769677, mean_q: 20.015236\n",
            " 130153/150000: episode: 18963, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.250 [0.000, 8.000],  loss: 0.242539, mae: 14.150609, mean_q: 20.028223\n",
            " 130159/150000: episode: 18964, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.333 [2.000, 8.000],  loss: 0.329506, mae: 14.303986, mean_q: 19.966978\n",
            " 130164/150000: episode: 18965, duration: 0.051s, episode steps:   5, steps per second:  98, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.508200, mae: 14.000284, mean_q: 19.839813\n",
            " 130172/150000: episode: 18966, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.462414, mae: 13.724158, mean_q: 19.835409\n",
            " 130180/150000: episode: 18967, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.358345, mae: 14.012367, mean_q: 19.979500\n",
            " 130187/150000: episode: 18968, duration: 0.073s, episode steps:   7, steps per second:  96, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.421685, mae: 14.095199, mean_q: 20.111607\n",
            " 130196/150000: episode: 18969, duration: 0.088s, episode steps:   9, steps per second: 103, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.305296, mae: 13.921999, mean_q: 19.997099\n",
            " 130200/150000: episode: 18970, duration: 0.043s, episode steps:   4, steps per second:  94, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 1.500 [0.000, 4.000],  loss: 0.416012, mae: 13.642681, mean_q: 19.966467\n",
            " 130207/150000: episode: 18971, duration: 0.079s, episode steps:   7, steps per second:  89, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.549518, mae: 13.614676, mean_q: 20.048014\n",
            " 130214/150000: episode: 18972, duration: 0.071s, episode steps:   7, steps per second:  99, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 5.143 [1.000, 8.000],  loss: 0.261925, mae: 13.781616, mean_q: 19.966867\n",
            " 130223/150000: episode: 18973, duration: 0.096s, episode steps:   9, steps per second:  93, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.231877, mae: 13.876493, mean_q: 20.094975\n",
            " 130232/150000: episode: 18974, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.216705, mae: 13.908794, mean_q: 20.014471\n",
            " 130235/150000: episode: 18975, duration: 0.036s, episode steps:   3, steps per second:  84, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 4.667 [2.000, 6.000],  loss: 0.179132, mae: 14.036293, mean_q: 19.981852\n",
            " 130242/150000: episode: 18976, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.336094, mae: 13.838517, mean_q: 19.906183\n",
            " 130249/150000: episode: 18977, duration: 0.079s, episode steps:   7, steps per second:  88, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.456719, mae: 13.761253, mean_q: 20.018770\n",
            " 130255/150000: episode: 18978, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.500 [0.000, 8.000],  loss: 0.302597, mae: 13.818390, mean_q: 20.074509\n",
            " 130263/150000: episode: 18979, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.453265, mae: 13.950666, mean_q: 20.062725\n",
            " 130272/150000: episode: 18980, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.298408, mae: 13.852075, mean_q: 20.031075\n",
            " 130276/150000: episode: 18981, duration: 0.049s, episode steps:   4, steps per second:  82, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.500 [0.000, 8.000],  loss: 0.842378, mae: 13.801580, mean_q: 19.952309\n",
            " 130283/150000: episode: 18982, duration: 0.068s, episode steps:   7, steps per second: 104, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 2.714 [0.000, 6.000],  loss: 0.339788, mae: 13.783235, mean_q: 19.988556\n",
            " 130291/150000: episode: 18983, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.695065, mae: 14.163544, mean_q: 20.066746\n",
            " 130300/150000: episode: 18984, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.584333, mae: 14.159394, mean_q: 19.941477\n",
            " 130305/150000: episode: 18985, duration: 0.051s, episode steps:   5, steps per second:  98, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.600 [2.000, 8.000],  loss: 0.329363, mae: 13.799817, mean_q: 19.875721\n",
            " 130312/150000: episode: 18986, duration: 0.066s, episode steps:   7, steps per second: 107, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.571 [2.000, 8.000],  loss: 0.379702, mae: 13.717574, mean_q: 20.132113\n",
            " 130317/150000: episode: 18987, duration: 0.061s, episode steps:   5, steps per second:  82, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.400 [0.000, 6.000],  loss: 0.249346, mae: 13.996439, mean_q: 20.067158\n",
            " 130326/150000: episode: 18988, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.333 [0.000, 8.000],  loss: 0.410160, mae: 14.307775, mean_q: 19.893181\n",
            " 130332/150000: episode: 18989, duration: 0.062s, episode steps:   6, steps per second:  96, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.292490, mae: 14.360934, mean_q: 19.880053\n",
            " 130341/150000: episode: 18990, duration: 0.082s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.330180, mae: 13.902174, mean_q: 19.935284\n",
            " 130347/150000: episode: 18991, duration: 0.060s, episode steps:   6, steps per second: 101, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [1.000, 8.000],  loss: 0.228793, mae: 14.020554, mean_q: 20.031853\n",
            " 130355/150000: episode: 18992, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.246556, mae: 14.000296, mean_q: 20.071140\n",
            " 130364/150000: episode: 18993, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.333 [0.000, 8.000],  loss: 0.481092, mae: 14.083457, mean_q: 19.968958\n",
            " 130372/150000: episode: 18994, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.875 [0.000, 8.000],  loss: 0.654061, mae: 14.086250, mean_q: 20.023785\n",
            " 130378/150000: episode: 18995, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 7.000],  loss: 0.255132, mae: 14.092808, mean_q: 20.215178\n",
            " 130381/150000: episode: 18996, duration: 0.034s, episode steps:   3, steps per second:  89, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 6.333 [6.000, 7.000],  loss: 0.257306, mae: 13.799770, mean_q: 20.179323\n",
            " 130388/150000: episode: 18997, duration: 0.070s, episode steps:   7, steps per second: 100, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.496553, mae: 13.980051, mean_q: 20.097515\n",
            " 130393/150000: episode: 18998, duration: 0.059s, episode steps:   5, steps per second:  85, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.201121, mae: 14.329798, mean_q: 20.104290\n",
            " 130400/150000: episode: 18999, duration: 0.071s, episode steps:   7, steps per second:  98, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.857 [2.000, 7.000],  loss: 0.244129, mae: 13.831863, mean_q: 20.145044\n",
            " 130407/150000: episode: 19000, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.194814, mae: 13.717392, mean_q: 19.995672\n",
            " 130412/150000: episode: 19001, duration: 0.052s, episode steps:   5, steps per second:  96, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [0.000, 8.000],  loss: 0.308190, mae: 13.795329, mean_q: 19.925848\n",
            " 130420/150000: episode: 19002, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.863961, mae: 13.843412, mean_q: 19.874531\n",
            " 130428/150000: episode: 19003, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.321859, mae: 13.807611, mean_q: 20.217186\n",
            " 130436/150000: episode: 19004, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.125 [1.000, 8.000],  loss: 0.432514, mae: 14.110647, mean_q: 20.147495\n",
            " 130445/150000: episode: 19005, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.817293, mae: 13.700224, mean_q: 19.901863\n",
            " 130454/150000: episode: 19006, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.381266, mae: 13.967594, mean_q: 20.089285\n",
            " 130460/150000: episode: 19007, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.333 [1.000, 8.000],  loss: 0.266505, mae: 14.162842, mean_q: 20.341358\n",
            " 130465/150000: episode: 19008, duration: 0.049s, episode steps:   5, steps per second: 103, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [1.000, 8.000],  loss: 0.245959, mae: 14.001704, mean_q: 19.877396\n",
            " 130472/150000: episode: 19009, duration: 0.075s, episode steps:   7, steps per second:  93, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.243274, mae: 14.069667, mean_q: 20.170477\n",
            " 130475/150000: episode: 19010, duration: 0.033s, episode steps:   3, steps per second:  90, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 4.333 [3.000, 7.000],  loss: 0.304901, mae: 14.096629, mean_q: 20.236902\n",
            " 130480/150000: episode: 19011, duration: 0.047s, episode steps:   5, steps per second: 105, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.800 [1.000, 7.000],  loss: 0.229603, mae: 14.047266, mean_q: 20.059561\n",
            " 130488/150000: episode: 19012, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.250 [0.000, 8.000],  loss: 0.351613, mae: 14.047382, mean_q: 20.068676\n",
            " 130496/150000: episode: 19013, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.250 [0.000, 8.000],  loss: 0.313409, mae: 14.107695, mean_q: 20.071873\n",
            " 130500/150000: episode: 19014, duration: 0.041s, episode steps:   4, steps per second:  98, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.750 [0.000, 8.000],  loss: 0.513198, mae: 14.146423, mean_q: 20.092529\n",
            " 130506/150000: episode: 19015, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 8.000],  loss: 0.176766, mae: 13.811241, mean_q: 20.089617\n",
            " 130513/150000: episode: 19016, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.477096, mae: 13.874529, mean_q: 19.946127\n",
            " 130520/150000: episode: 19017, duration: 0.082s, episode steps:   7, steps per second:  86, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.143 [0.000, 8.000],  loss: 0.219395, mae: 14.257797, mean_q: 20.185394\n",
            " 130528/150000: episode: 19018, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.436743, mae: 14.018843, mean_q: 20.037868\n",
            " 130531/150000: episode: 19019, duration: 0.032s, episode steps:   3, steps per second:  93, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.323002, mae: 13.387329, mean_q: 19.968580\n",
            " 130538/150000: episode: 19020, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.284519, mae: 14.073277, mean_q: 20.078064\n",
            " 130540/150000: episode: 19021, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 6.000 [6.000, 6.000],  loss: 0.513599, mae: 14.412577, mean_q: 19.857281\n",
            " 130546/150000: episode: 19022, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.000 [0.000, 8.000],  loss: 0.260104, mae: 13.731921, mean_q: 19.792374\n",
            " 130555/150000: episode: 19023, duration: 0.076s, episode steps:   9, steps per second: 119, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.289640, mae: 13.778553, mean_q: 19.923462\n",
            " 130562/150000: episode: 19024, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.339625, mae: 14.241855, mean_q: 20.111252\n",
            " 130570/150000: episode: 19025, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.229632, mae: 14.005865, mean_q: 19.915140\n",
            " 130577/150000: episode: 19026, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.225848, mae: 14.295870, mean_q: 20.158899\n",
            " 130581/150000: episode: 19027, duration: 0.044s, episode steps:   4, steps per second:  90, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.750 [0.000, 8.000],  loss: 0.225994, mae: 14.093737, mean_q: 19.780640\n",
            " 130586/150000: episode: 19028, duration: 0.048s, episode steps:   5, steps per second: 104, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.800 [0.000, 8.000],  loss: 0.597192, mae: 13.868883, mean_q: 20.000446\n",
            " 130591/150000: episode: 19029, duration: 0.051s, episode steps:   5, steps per second:  98, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 6.000],  loss: 0.243996, mae: 13.936232, mean_q: 20.026417\n",
            " 130596/150000: episode: 19030, duration: 0.063s, episode steps:   5, steps per second:  80, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.478135, mae: 13.987765, mean_q: 19.916965\n",
            " 130605/150000: episode: 19031, duration: 0.084s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.398257, mae: 14.102910, mean_q: 19.955090\n",
            " 130611/150000: episode: 19032, duration: 0.071s, episode steps:   6, steps per second:  84, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 8.000],  loss: 0.238297, mae: 14.121203, mean_q: 20.020203\n",
            " 130614/150000: episode: 19033, duration: 0.039s, episode steps:   3, steps per second:  76, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 4.667 [0.000, 7.000],  loss: 0.330279, mae: 13.767399, mean_q: 20.254093\n",
            " 130619/150000: episode: 19034, duration: 0.064s, episode steps:   5, steps per second:  78, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.200 [1.000, 7.000],  loss: 0.392507, mae: 14.033101, mean_q: 20.025162\n",
            " 130627/150000: episode: 19035, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 1.135093, mae: 14.015645, mean_q: 19.721273\n",
            " 130629/150000: episode: 19036, duration: 0.031s, episode steps:   2, steps per second:  65, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 0.000 [0.000, 0.000],  loss: 0.253312, mae: 14.298703, mean_q: 20.397533\n",
            " 130638/150000: episode: 19037, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.760600, mae: 14.098610, mean_q: 20.307350\n",
            " 130646/150000: episode: 19038, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.597967, mae: 13.826217, mean_q: 20.109119\n",
            " 130650/150000: episode: 19039, duration: 0.048s, episode steps:   4, steps per second:  84, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 2.500 [0.000, 7.000],  loss: 0.388241, mae: 14.064434, mean_q: 20.156569\n",
            " 130655/150000: episode: 19040, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.800 [0.000, 8.000],  loss: 0.272614, mae: 14.210955, mean_q: 20.135942\n",
            " 130660/150000: episode: 19041, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.400 [2.000, 8.000],  loss: 0.495587, mae: 13.961108, mean_q: 20.098841\n",
            " 130668/150000: episode: 19042, duration: 0.122s, episode steps:   8, steps per second:  66, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.462223, mae: 14.238456, mean_q: 19.903992\n",
            " 130672/150000: episode: 19043, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.312437, mae: 14.105913, mean_q: 20.284071\n",
            " 130679/150000: episode: 19044, duration: 0.100s, episode steps:   7, steps per second:  70, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.291101, mae: 13.981985, mean_q: 20.042528\n",
            " 130687/150000: episode: 19045, duration: 0.110s, episode steps:   8, steps per second:  73, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.266580, mae: 13.957417, mean_q: 20.110970\n",
            " 130695/150000: episode: 19046, duration: 0.096s, episode steps:   8, steps per second:  84, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.193706, mae: 13.825966, mean_q: 20.023563\n",
            " 130699/150000: episode: 19047, duration: 0.070s, episode steps:   4, steps per second:  57, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.250 [2.000, 7.000],  loss: 0.218522, mae: 13.916067, mean_q: 19.830433\n",
            " 130708/150000: episode: 19048, duration: 0.131s, episode steps:   9, steps per second:  69, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.203877, mae: 14.111766, mean_q: 20.036137\n",
            " 130717/150000: episode: 19049, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.605691, mae: 14.065718, mean_q: 19.764860\n",
            " 130726/150000: episode: 19050, duration: 0.126s, episode steps:   9, steps per second:  71, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.434885, mae: 14.012585, mean_q: 20.001701\n",
            " 130734/150000: episode: 19051, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.334248, mae: 13.964914, mean_q: 19.904652\n",
            " 130743/150000: episode: 19052, duration: 0.146s, episode steps:   9, steps per second:  61, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.381183, mae: 13.635841, mean_q: 20.046240\n",
            " 130752/150000: episode: 19053, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.218080, mae: 13.788511, mean_q: 20.082487\n",
            " 130761/150000: episode: 19054, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.322636, mae: 13.718927, mean_q: 19.920506\n",
            " 130768/150000: episode: 19055, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.430591, mae: 13.753392, mean_q: 20.170443\n",
            " 130777/150000: episode: 19056, duration: 0.151s, episode steps:   9, steps per second:  59, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.339602, mae: 13.982852, mean_q: 20.137352\n",
            " 130785/150000: episode: 19057, duration: 0.116s, episode steps:   8, steps per second:  69, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.709471, mae: 13.934630, mean_q: 19.801865\n",
            " 130791/150000: episode: 19058, duration: 0.074s, episode steps:   6, steps per second:  81, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [0.000, 8.000],  loss: 0.334307, mae: 14.233562, mean_q: 20.277529\n",
            " 130796/150000: episode: 19059, duration: 0.082s, episode steps:   5, steps per second:  61, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.400 [0.000, 7.000],  loss: 0.271172, mae: 14.060904, mean_q: 19.905823\n",
            " 130804/150000: episode: 19060, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.875 [1.000, 7.000],  loss: 0.350002, mae: 14.038577, mean_q: 19.856949\n",
            " 130812/150000: episode: 19061, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.185069, mae: 13.992455, mean_q: 20.072077\n",
            " 130819/150000: episode: 19062, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.208656, mae: 14.152396, mean_q: 20.018040\n",
            " 130824/150000: episode: 19063, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.200 [0.000, 7.000],  loss: 0.347959, mae: 13.884146, mean_q: 19.951843\n",
            " 130833/150000: episode: 19064, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.296336, mae: 13.872478, mean_q: 19.836479\n",
            " 130840/150000: episode: 19065, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.487439, mae: 14.110335, mean_q: 20.058207\n",
            " 130846/150000: episode: 19066, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [1.000, 8.000],  loss: 0.355388, mae: 13.845014, mean_q: 19.877928\n",
            " 130852/150000: episode: 19067, duration: 0.101s, episode steps:   6, steps per second:  59, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.667 [0.000, 8.000],  loss: 0.196211, mae: 13.720944, mean_q: 19.971161\n",
            " 130860/150000: episode: 19068, duration: 0.109s, episode steps:   8, steps per second:  74, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.483807, mae: 14.073925, mean_q: 20.107544\n",
            " 130869/150000: episode: 19069, duration: 0.149s, episode steps:   9, steps per second:  60, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.326004, mae: 14.190355, mean_q: 19.952255\n",
            " 130875/150000: episode: 19070, duration: 0.096s, episode steps:   6, steps per second:  62, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.833 [1.000, 7.000],  loss: 0.815400, mae: 13.686591, mean_q: 19.922592\n",
            " 130883/150000: episode: 19071, duration: 0.133s, episode steps:   8, steps per second:  60, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.277366, mae: 13.743069, mean_q: 19.958658\n",
            " 130890/150000: episode: 19072, duration: 0.121s, episode steps:   7, steps per second:  58, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.000 [1.000, 8.000],  loss: 0.288926, mae: 13.993922, mean_q: 20.130758\n",
            " 130898/150000: episode: 19073, duration: 0.144s, episode steps:   8, steps per second:  56, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.209539, mae: 14.048100, mean_q: 20.033867\n",
            " 130905/150000: episode: 19074, duration: 0.114s, episode steps:   7, steps per second:  61, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.223790, mae: 13.750951, mean_q: 19.882416\n",
            " 130913/150000: episode: 19075, duration: 0.130s, episode steps:   8, steps per second:  62, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.242995, mae: 13.941954, mean_q: 19.959875\n",
            " 130918/150000: episode: 19076, duration: 0.074s, episode steps:   5, steps per second:  68, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.400 [1.000, 5.000],  loss: 0.506094, mae: 13.774594, mean_q: 19.958572\n",
            " 130927/150000: episode: 19077, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.889 [0.000, 8.000],  loss: 0.219213, mae: 13.914790, mean_q: 19.976284\n",
            " 130934/150000: episode: 19078, duration: 0.114s, episode steps:   7, steps per second:  61, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [1.000, 8.000],  loss: 0.465534, mae: 13.990270, mean_q: 19.956995\n",
            " 130942/150000: episode: 19079, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.538472, mae: 13.918282, mean_q: 19.805237\n",
            " 130948/150000: episode: 19080, duration: 0.077s, episode steps:   6, steps per second:  77, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.330218, mae: 13.676896, mean_q: 19.977980\n",
            " 130956/150000: episode: 19081, duration: 0.115s, episode steps:   8, steps per second:  69, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.329661, mae: 14.235684, mean_q: 20.062590\n",
            " 130965/150000: episode: 19082, duration: 0.115s, episode steps:   9, steps per second:  78, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.385846, mae: 13.963618, mean_q: 20.066223\n",
            " 130973/150000: episode: 19083, duration: 0.136s, episode steps:   8, steps per second:  59, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.633028, mae: 13.805257, mean_q: 19.846706\n",
            " 130977/150000: episode: 19084, duration: 0.061s, episode steps:   4, steps per second:  66, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 6.250 [3.000, 8.000],  loss: 0.579303, mae: 13.548360, mean_q: 19.957949\n",
            " 130981/150000: episode: 19085, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.250 [1.000, 6.000],  loss: 0.357116, mae: 13.941301, mean_q: 19.956268\n",
            " 130989/150000: episode: 19086, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.318171, mae: 13.886215, mean_q: 20.082275\n",
            " 130996/150000: episode: 19087, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [1.000, 8.000],  loss: 0.291064, mae: 14.147107, mean_q: 19.948915\n",
            " 131004/150000: episode: 19088, duration: 0.122s, episode steps:   8, steps per second:  65, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.330096, mae: 13.642103, mean_q: 20.039057\n",
            " 131013/150000: episode: 19089, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.648891, mae: 13.696047, mean_q: 19.843264\n",
            " 131019/150000: episode: 19090, duration: 0.095s, episode steps:   6, steps per second:  63, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2.667 [0.000, 6.000],  loss: 0.311210, mae: 14.108714, mean_q: 20.041510\n",
            " 131022/150000: episode: 19091, duration: 0.048s, episode steps:   3, steps per second:  62, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 4.000 [2.000, 5.000],  loss: 0.267550, mae: 13.457290, mean_q: 20.009371\n",
            " 131027/150000: episode: 19092, duration: 0.078s, episode steps:   5, steps per second:  64, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [2.000, 6.000],  loss: 0.563264, mae: 13.739764, mean_q: 19.883162\n",
            " 131032/150000: episode: 19093, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 7.000],  loss: 0.466829, mae: 13.730803, mean_q: 20.068739\n",
            " 131037/150000: episode: 19094, duration: 0.096s, episode steps:   5, steps per second:  52, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.943519, mae: 14.065732, mean_q: 19.816870\n",
            " 131042/150000: episode: 19095, duration: 0.073s, episode steps:   5, steps per second:  68, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.553003, mae: 13.934708, mean_q: 19.875088\n",
            " 131047/150000: episode: 19096, duration: 0.070s, episode steps:   5, steps per second:  72, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.400 [2.000, 8.000],  loss: 0.228170, mae: 14.020460, mean_q: 19.932343\n",
            " 131054/150000: episode: 19097, duration: 0.113s, episode steps:   7, steps per second:  62, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.315072, mae: 13.780236, mean_q: 19.852354\n",
            " 131063/150000: episode: 19098, duration: 0.126s, episode steps:   9, steps per second:  72, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.356977, mae: 13.543073, mean_q: 19.912436\n",
            " 131067/150000: episode: 19099, duration: 0.061s, episode steps:   4, steps per second:  66, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 2.750 [0.000, 5.000],  loss: 0.344199, mae: 14.019276, mean_q: 19.962217\n",
            " 131074/150000: episode: 19100, duration: 0.093s, episode steps:   7, steps per second:  76, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.225191, mae: 13.829661, mean_q: 20.029114\n",
            " 131083/150000: episode: 19101, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.296189, mae: 13.803833, mean_q: 19.931013\n",
            " 131088/150000: episode: 19102, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.800 [0.000, 8.000],  loss: 0.546685, mae: 14.104448, mean_q: 19.961073\n",
            " 131096/150000: episode: 19103, duration: 0.115s, episode steps:   8, steps per second:  70, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.872606, mae: 13.463642, mean_q: 19.808138\n",
            " 131105/150000: episode: 19104, duration: 0.131s, episode steps:   9, steps per second:  68, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.444 [0.000, 8.000],  loss: 0.896656, mae: 14.016468, mean_q: 19.746908\n",
            " 131110/150000: episode: 19105, duration: 0.077s, episode steps:   5, steps per second:  65, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.332808, mae: 13.710459, mean_q: 20.104010\n",
            " 131117/150000: episode: 19106, duration: 0.105s, episode steps:   7, steps per second:  67, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.429 [0.000, 7.000],  loss: 0.525058, mae: 14.025848, mean_q: 20.426268\n",
            " 131126/150000: episode: 19107, duration: 0.141s, episode steps:   9, steps per second:  64, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.383075, mae: 13.810845, mean_q: 19.950480\n",
            " 131131/150000: episode: 19108, duration: 0.060s, episode steps:   5, steps per second:  84, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.233034, mae: 13.972615, mean_q: 20.151413\n",
            " 131139/150000: episode: 19109, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.505512, mae: 13.804593, mean_q: 19.885166\n",
            " 131148/150000: episode: 19110, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.456609, mae: 13.773865, mean_q: 19.921482\n",
            " 131156/150000: episode: 19111, duration: 0.074s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.418796, mae: 13.615759, mean_q: 19.832001\n",
            " 131163/150000: episode: 19112, duration: 0.067s, episode steps:   7, steps per second: 104, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.320486, mae: 13.964879, mean_q: 20.082010\n",
            " 131170/150000: episode: 19113, duration: 0.068s, episode steps:   7, steps per second: 103, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.392666, mae: 13.733369, mean_q: 19.730392\n",
            " 131179/150000: episode: 19114, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.468872, mae: 14.344166, mean_q: 19.865974\n",
            " 131187/150000: episode: 19115, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.430692, mae: 14.255852, mean_q: 19.828991\n",
            " 131195/150000: episode: 19116, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.214470, mae: 14.052557, mean_q: 19.917477\n",
            " 131201/150000: episode: 19117, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.000 [1.000, 8.000],  loss: 0.255069, mae: 13.759807, mean_q: 20.208002\n",
            " 131210/150000: episode: 19118, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.361990, mae: 13.778134, mean_q: 19.917627\n",
            " 131217/150000: episode: 19119, duration: 0.074s, episode steps:   7, steps per second:  95, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.404148, mae: 14.097977, mean_q: 19.906023\n",
            " 131226/150000: episode: 19120, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.339043, mae: 13.832943, mean_q: 19.876225\n",
            " 131231/150000: episode: 19121, duration: 0.062s, episode steps:   5, steps per second:  81, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 1.800 [0.000, 5.000],  loss: 0.310485, mae: 14.097468, mean_q: 20.094578\n",
            " 131238/150000: episode: 19122, duration: 0.081s, episode steps:   7, steps per second:  87, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.219987, mae: 13.498168, mean_q: 19.908878\n",
            " 131246/150000: episode: 19123, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.295024, mae: 13.569471, mean_q: 19.969343\n",
            " 131255/150000: episode: 19124, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.392113, mae: 13.719556, mean_q: 19.835117\n",
            " 131264/150000: episode: 19125, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.211774, mae: 13.707683, mean_q: 19.899139\n",
            " 131269/150000: episode: 19126, duration: 0.053s, episode steps:   5, steps per second:  94, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.800 [0.000, 5.000],  loss: 0.225750, mae: 13.784633, mean_q: 19.898695\n",
            " 131276/150000: episode: 19127, duration: 0.066s, episode steps:   7, steps per second: 105, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.143 [0.000, 8.000],  loss: 0.218498, mae: 13.901141, mean_q: 19.901201\n",
            " 131281/150000: episode: 19128, duration: 0.052s, episode steps:   5, steps per second:  96, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.800 [0.000, 8.000],  loss: 0.207437, mae: 13.682017, mean_q: 19.815769\n",
            " 131289/150000: episode: 19129, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.275337, mae: 13.919714, mean_q: 19.930870\n",
            " 131298/150000: episode: 19130, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.219471, mae: 13.610988, mean_q: 19.768284\n",
            " 131305/150000: episode: 19131, duration: 0.080s, episode steps:   7, steps per second:  88, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.533852, mae: 13.561897, mean_q: 19.846785\n",
            " 131314/150000: episode: 19132, duration: 0.103s, episode steps:   9, steps per second:  87, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.667 [1.000, 8.000],  loss: 0.477357, mae: 13.930970, mean_q: 19.886925\n",
            " 131319/150000: episode: 19133, duration: 0.054s, episode steps:   5, steps per second:  93, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 6.400 [3.000, 8.000],  loss: 1.126880, mae: 13.649887, mean_q: 19.704906\n",
            " 131328/150000: episode: 19134, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.889 [0.000, 7.000],  loss: 0.736791, mae: 13.586902, mean_q: 20.160662\n",
            " 131334/150000: episode: 19135, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [2.000, 7.000],  loss: 0.238393, mae: 13.642982, mean_q: 19.874334\n",
            " 131342/150000: episode: 19136, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.512324, mae: 13.748409, mean_q: 20.055191\n",
            " 131348/150000: episode: 19137, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [2.000, 8.000],  loss: 0.382527, mae: 13.653493, mean_q: 19.815519\n",
            " 131354/150000: episode: 19138, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.000 [2.000, 8.000],  loss: 0.345692, mae: 13.733783, mean_q: 19.838356\n",
            " 131363/150000: episode: 19139, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.335484, mae: 13.693401, mean_q: 19.884897\n",
            " 131370/150000: episode: 19140, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.637963, mae: 13.626432, mean_q: 19.813253\n",
            " 131379/150000: episode: 19141, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.542997, mae: 13.853870, mean_q: 19.751808\n",
            " 131384/150000: episode: 19142, duration: 0.052s, episode steps:   5, steps per second:  97, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.390192, mae: 13.682065, mean_q: 19.992687\n",
            " 131392/150000: episode: 19143, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.406319, mae: 13.655937, mean_q: 19.880230\n",
            " 131400/150000: episode: 19144, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.290257, mae: 13.811423, mean_q: 19.917818\n",
            " 131408/150000: episode: 19145, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.240509, mae: 13.676759, mean_q: 20.007133\n",
            " 131414/150000: episode: 19146, duration: 0.061s, episode steps:   6, steps per second:  99, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 7.000],  loss: 0.348881, mae: 14.056447, mean_q: 19.849487\n",
            " 131418/150000: episode: 19147, duration: 0.047s, episode steps:   4, steps per second:  85, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.000 [1.000, 7.000],  loss: 0.529466, mae: 13.976965, mean_q: 19.500576\n",
            " 131425/150000: episode: 19148, duration: 0.072s, episode steps:   7, steps per second:  97, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.429 [0.000, 7.000],  loss: 0.231240, mae: 14.426446, mean_q: 19.825472\n",
            " 131433/150000: episode: 19149, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.258345, mae: 13.921656, mean_q: 19.963409\n",
            " 131441/150000: episode: 19150, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.369202, mae: 13.889534, mean_q: 19.847437\n",
            " 131446/150000: episode: 19151, duration: 0.051s, episode steps:   5, steps per second:  98, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.229037, mae: 13.589121, mean_q: 19.738674\n",
            " 131454/150000: episode: 19152, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.500 [0.000, 8.000],  loss: 0.352373, mae: 13.748346, mean_q: 20.039396\n",
            " 131463/150000: episode: 19153, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.396121, mae: 13.930186, mean_q: 19.878679\n",
            " 131469/150000: episode: 19154, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 7.000],  loss: 0.481973, mae: 13.869464, mean_q: 19.897858\n",
            " 131475/150000: episode: 19155, duration: 0.054s, episode steps:   6, steps per second: 111, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.333 [2.000, 8.000],  loss: 0.336844, mae: 14.068868, mean_q: 20.090117\n",
            " 131480/150000: episode: 19156, duration: 0.052s, episode steps:   5, steps per second:  97, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 6.200 [4.000, 8.000],  loss: 0.465522, mae: 13.605169, mean_q: 19.972656\n",
            " 131489/150000: episode: 19157, duration: 0.098s, episode steps:   9, steps per second:  91, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.370611, mae: 14.050735, mean_q: 20.246817\n",
            " 131492/150000: episode: 19158, duration: 0.033s, episode steps:   3, steps per second:  91, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 2.667 [0.000, 8.000],  loss: 0.346874, mae: 13.959309, mean_q: 20.007311\n",
            " 131499/150000: episode: 19159, duration: 0.065s, episode steps:   7, steps per second: 107, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 5.143 [0.000, 8.000],  loss: 0.565242, mae: 13.974374, mean_q: 19.818441\n",
            " 131506/150000: episode: 19160, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.451055, mae: 13.986186, mean_q: 20.000034\n",
            " 131514/150000: episode: 19161, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.291026, mae: 14.176628, mean_q: 20.129974\n",
            " 131521/150000: episode: 19162, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.333877, mae: 14.117423, mean_q: 19.681532\n",
            " 131528/150000: episode: 19163, duration: 0.082s, episode steps:   7, steps per second:  85, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.571 [0.000, 8.000],  loss: 0.400196, mae: 14.109610, mean_q: 20.219646\n",
            " 131533/150000: episode: 19164, duration: 0.051s, episode steps:   5, steps per second:  97, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.800 [0.000, 7.000],  loss: 0.672102, mae: 13.728557, mean_q: 19.901066\n",
            " 131541/150000: episode: 19165, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.812207, mae: 13.785248, mean_q: 19.948915\n",
            " 131546/150000: episode: 19166, duration: 0.051s, episode steps:   5, steps per second:  97, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.800 [2.000, 7.000],  loss: 0.419275, mae: 13.954755, mean_q: 19.996796\n",
            " 131554/150000: episode: 19167, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.533308, mae: 13.608974, mean_q: 20.171551\n",
            " 131563/150000: episode: 19168, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.292564, mae: 13.823795, mean_q: 20.014616\n",
            " 131572/150000: episode: 19169, duration: 0.082s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.522763, mae: 13.709492, mean_q: 19.794794\n",
            " 131580/150000: episode: 19170, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.562569, mae: 13.962516, mean_q: 20.103100\n",
            " 131587/150000: episode: 19171, duration: 0.082s, episode steps:   7, steps per second:  85, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.330022, mae: 13.991701, mean_q: 20.046644\n",
            " 131592/150000: episode: 19172, duration: 0.050s, episode steps:   5, steps per second: 101, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.200 [0.000, 8.000],  loss: 0.350812, mae: 13.895983, mean_q: 20.000065\n",
            " 131598/150000: episode: 19173, duration: 0.055s, episode steps:   6, steps per second: 110, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [2.000, 7.000],  loss: 0.620491, mae: 13.903801, mean_q: 19.942474\n",
            " 131605/150000: episode: 19174, duration: 0.065s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.386322, mae: 13.613503, mean_q: 20.131504\n",
            " 131610/150000: episode: 19175, duration: 0.057s, episode steps:   5, steps per second:  87, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.200 [2.000, 7.000],  loss: 0.294807, mae: 13.838203, mean_q: 19.890631\n",
            " 131617/150000: episode: 19176, duration: 0.070s, episode steps:   7, steps per second:  99, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.143 [2.000, 7.000],  loss: 0.477386, mae: 13.748210, mean_q: 20.114908\n",
            " 131624/150000: episode: 19177, duration: 0.063s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.848893, mae: 13.806231, mean_q: 19.779629\n",
            " 131632/150000: episode: 19178, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.472469, mae: 13.994171, mean_q: 20.345074\n",
            " 131639/150000: episode: 19179, duration: 0.069s, episode steps:   7, steps per second: 102, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.568151, mae: 13.942085, mean_q: 19.995588\n",
            " 131647/150000: episode: 19180, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.242995, mae: 13.902143, mean_q: 19.995995\n",
            " 131649/150000: episode: 19181, duration: 0.028s, episode steps:   2, steps per second:  71, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.196846, mae: 14.653299, mean_q: 19.970512\n",
            " 131657/150000: episode: 19182, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.500 [0.000, 8.000],  loss: 0.257155, mae: 14.075981, mean_q: 19.962173\n",
            " 131665/150000: episode: 19183, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.317494, mae: 14.024233, mean_q: 20.003315\n",
            " 131673/150000: episode: 19184, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.288775, mae: 13.746859, mean_q: 19.974808\n",
            " 131682/150000: episode: 19185, duration: 0.097s, episode steps:   9, steps per second:  92, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.463787, mae: 14.045971, mean_q: 19.991501\n",
            " 131689/150000: episode: 19186, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.429 [0.000, 8.000],  loss: 0.468158, mae: 13.882632, mean_q: 20.036854\n",
            " 131694/150000: episode: 19187, duration: 0.047s, episode steps:   5, steps per second: 107, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.800 [1.000, 7.000],  loss: 0.191211, mae: 13.831345, mean_q: 20.004593\n",
            " 131703/150000: episode: 19188, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.431985, mae: 13.744328, mean_q: 20.000633\n",
            " 131712/150000: episode: 19189, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.556 [0.000, 8.000],  loss: 0.627717, mae: 13.736979, mean_q: 19.853718\n",
            " 131721/150000: episode: 19190, duration: 0.081s, episode steps:   9, steps per second: 112, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.290128, mae: 13.732250, mean_q: 20.475044\n",
            " 131727/150000: episode: 19191, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.333 [2.000, 8.000],  loss: 0.380687, mae: 13.806376, mean_q: 19.864122\n",
            " 131735/150000: episode: 19192, duration: 0.097s, episode steps:   8, steps per second:  82, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.451879, mae: 14.015237, mean_q: 20.151115\n",
            " 131740/150000: episode: 19193, duration: 0.049s, episode steps:   5, steps per second: 102, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.303503, mae: 13.739429, mean_q: 19.861681\n",
            " 131746/150000: episode: 19194, duration: 0.059s, episode steps:   6, steps per second: 101, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [0.000, 7.000],  loss: 0.223140, mae: 14.356748, mean_q: 20.013838\n",
            " 131752/150000: episode: 19195, duration: 0.062s, episode steps:   6, steps per second:  97, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.167 [0.000, 6.000],  loss: 0.399714, mae: 13.750491, mean_q: 19.943151\n",
            " 131754/150000: episode: 19196, duration: 0.024s, episode steps:   2, steps per second:  84, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 5.000 [5.000, 5.000],  loss: 0.239210, mae: 14.057222, mean_q: 19.760639\n",
            " 131763/150000: episode: 19197, duration: 0.092s, episode steps:   9, steps per second:  97, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.540269, mae: 13.611808, mean_q: 19.935591\n",
            " 131772/150000: episode: 19198, duration: 0.078s, episode steps:   9, steps per second: 116, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.337828, mae: 13.732299, mean_q: 19.956764\n",
            " 131781/150000: episode: 19199, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.421434, mae: 13.781930, mean_q: 20.006559\n",
            " 131786/150000: episode: 19200, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.400 [3.000, 8.000],  loss: 0.441769, mae: 13.650354, mean_q: 19.960064\n",
            " 131795/150000: episode: 19201, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.326659, mae: 13.820515, mean_q: 19.890341\n",
            " 131800/150000: episode: 19202, duration: 0.047s, episode steps:   5, steps per second: 106, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.200 [0.000, 5.000],  loss: 0.166541, mae: 14.190249, mean_q: 20.177761\n",
            " 131809/150000: episode: 19203, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.344686, mae: 13.656136, mean_q: 20.011990\n",
            " 131814/150000: episode: 19204, duration: 0.062s, episode steps:   5, steps per second:  80, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.200 [1.000, 8.000],  loss: 0.135368, mae: 13.903033, mean_q: 20.024837\n",
            " 131821/150000: episode: 19205, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.353804, mae: 13.808328, mean_q: 20.050333\n",
            " 131829/150000: episode: 19206, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.172359, mae: 13.845493, mean_q: 20.012543\n",
            " 131836/150000: episode: 19207, duration: 0.094s, episode steps:   7, steps per second:  74, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.857 [1.000, 8.000],  loss: 0.184628, mae: 13.992556, mean_q: 20.095583\n",
            " 131844/150000: episode: 19208, duration: 0.070s, episode steps:   8, steps per second: 115, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.261542, mae: 14.039087, mean_q: 19.938868\n",
            " 131851/150000: episode: 19209, duration: 0.065s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.210646, mae: 13.795316, mean_q: 19.997026\n",
            " 131857/150000: episode: 19210, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.167 [3.000, 7.000],  loss: 0.412454, mae: 13.733472, mean_q: 19.941065\n",
            " 131865/150000: episode: 19211, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.442253, mae: 14.225586, mean_q: 20.071529\n",
            " 131871/150000: episode: 19212, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.305268, mae: 13.659786, mean_q: 19.805859\n",
            " 131880/150000: episode: 19213, duration: 0.085s, episode steps:   9, steps per second: 105, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.652949, mae: 13.735970, mean_q: 20.015343\n",
            " 131885/150000: episode: 19214, duration: 0.048s, episode steps:   5, steps per second: 105, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.800 [1.000, 5.000],  loss: 0.342853, mae: 13.937200, mean_q: 20.096525\n",
            " 131892/150000: episode: 19215, duration: 0.080s, episode steps:   7, steps per second:  87, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.289039, mae: 14.110038, mean_q: 20.239969\n",
            " 131900/150000: episode: 19216, duration: 0.070s, episode steps:   8, steps per second: 114, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.215840, mae: 13.908032, mean_q: 19.962269\n",
            " 131908/150000: episode: 19217, duration: 0.070s, episode steps:   8, steps per second: 114, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.319711, mae: 13.688980, mean_q: 19.880123\n",
            " 131914/150000: episode: 19218, duration: 0.052s, episode steps:   6, steps per second: 114, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 7.000],  loss: 0.311569, mae: 13.739929, mean_q: 19.839537\n",
            " 131919/150000: episode: 19219, duration: 0.060s, episode steps:   5, steps per second:  83, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.203901, mae: 13.927548, mean_q: 20.129993\n",
            " 131924/150000: episode: 19220, duration: 0.050s, episode steps:   5, steps per second: 101, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.800 [1.000, 7.000],  loss: 0.419841, mae: 13.750807, mean_q: 19.813223\n",
            " 131933/150000: episode: 19221, duration: 0.080s, episode steps:   9, steps per second: 112, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.243190, mae: 13.932046, mean_q: 19.996914\n",
            " 131938/150000: episode: 19222, duration: 0.052s, episode steps:   5, steps per second:  96, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.200 [0.000, 7.000],  loss: 0.272850, mae: 14.102455, mean_q: 20.079819\n",
            " 131944/150000: episode: 19223, duration: 0.074s, episode steps:   6, steps per second:  81, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.928257, mae: 13.835366, mean_q: 19.926895\n",
            " 131952/150000: episode: 19224, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.394354, mae: 13.838732, mean_q: 20.099110\n",
            " 131960/150000: episode: 19225, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.535272, mae: 13.740841, mean_q: 19.992716\n",
            " 131968/150000: episode: 19226, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.358120, mae: 14.001683, mean_q: 19.800776\n",
            " 131974/150000: episode: 19227, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [0.000, 8.000],  loss: 0.250904, mae: 13.935715, mean_q: 20.333542\n",
            " 131979/150000: episode: 19228, duration: 0.054s, episode steps:   5, steps per second:  93, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.200 [0.000, 8.000],  loss: 0.636958, mae: 14.025851, mean_q: 20.008448\n",
            " 131985/150000: episode: 19229, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.286606, mae: 13.932251, mean_q: 19.817154\n",
            " 131993/150000: episode: 19230, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.664800, mae: 13.829628, mean_q: 20.007751\n",
            " 131998/150000: episode: 19231, duration: 0.048s, episode steps:   5, steps per second: 105, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.400 [2.000, 8.000],  loss: 0.383958, mae: 13.936501, mean_q: 19.867773\n",
            " 132003/150000: episode: 19232, duration: 0.051s, episode steps:   5, steps per second:  98, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.200 [0.000, 7.000],  loss: 0.309617, mae: 13.924748, mean_q: 20.156773\n",
            " 132010/150000: episode: 19233, duration: 0.067s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.190793, mae: 14.144079, mean_q: 19.977724\n",
            " 132016/150000: episode: 19234, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 7.000],  loss: 0.245359, mae: 13.900265, mean_q: 19.925430\n",
            " 132025/150000: episode: 19235, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.424379, mae: 13.869843, mean_q: 20.118837\n",
            " 132034/150000: episode: 19236, duration: 0.080s, episode steps:   9, steps per second: 112, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.419375, mae: 13.900207, mean_q: 19.774710\n",
            " 132042/150000: episode: 19237, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.230488, mae: 13.836399, mean_q: 20.099464\n",
            " 132049/150000: episode: 19238, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [1.000, 8.000],  loss: 0.180316, mae: 14.007940, mean_q: 20.138201\n",
            " 132056/150000: episode: 19239, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.193844, mae: 13.881206, mean_q: 20.021748\n",
            " 132065/150000: episode: 19240, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.222 [0.000, 8.000],  loss: 0.587800, mae: 13.883900, mean_q: 19.870235\n",
            " 132071/150000: episode: 19241, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.625668, mae: 13.887188, mean_q: 19.737207\n",
            " 132079/150000: episode: 19242, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.474964, mae: 14.018886, mean_q: 20.081457\n",
            " 132083/150000: episode: 19243, duration: 0.039s, episode steps:   4, steps per second: 103, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.750 [0.000, 7.000],  loss: 0.936956, mae: 14.187367, mean_q: 20.231627\n",
            " 132090/150000: episode: 19244, duration: 0.076s, episode steps:   7, steps per second:  92, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 1.008088, mae: 13.828873, mean_q: 19.830517\n",
            " 132097/150000: episode: 19245, duration: 0.061s, episode steps:   7, steps per second: 115, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.571 [0.000, 8.000],  loss: 0.576851, mae: 13.498320, mean_q: 20.132650\n",
            " 132104/150000: episode: 19246, duration: 0.061s, episode steps:   7, steps per second: 114, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.407677, mae: 13.812985, mean_q: 19.865015\n",
            " 132110/150000: episode: 19247, duration: 0.054s, episode steps:   6, steps per second: 111, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.000 [3.000, 8.000],  loss: 1.229150, mae: 13.909842, mean_q: 20.053129\n",
            " 132119/150000: episode: 19248, duration: 0.123s, episode steps:   9, steps per second:  73, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.657650, mae: 14.183432, mean_q: 20.203339\n",
            " 132127/150000: episode: 19249, duration: 0.112s, episode steps:   8, steps per second:  71, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.374194, mae: 13.693672, mean_q: 20.064066\n",
            " 132134/150000: episode: 19250, duration: 0.101s, episode steps:   7, steps per second:  69, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.300743, mae: 14.009210, mean_q: 20.236116\n",
            " 132139/150000: episode: 19251, duration: 0.087s, episode steps:   5, steps per second:  57, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.600 [2.000, 8.000],  loss: 0.406253, mae: 13.559153, mean_q: 19.947424\n",
            " 132144/150000: episode: 19252, duration: 0.076s, episode steps:   5, steps per second:  66, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.400 [0.000, 6.000],  loss: 0.651447, mae: 13.782390, mean_q: 19.926928\n",
            " 132152/150000: episode: 19253, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.267627, mae: 13.938385, mean_q: 20.084404\n",
            " 132155/150000: episode: 19254, duration: 0.043s, episode steps:   3, steps per second:  69, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.312249, mae: 14.018096, mean_q: 19.829967\n",
            " 132160/150000: episode: 19255, duration: 0.072s, episode steps:   5, steps per second:  69, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.800 [0.000, 6.000],  loss: 0.353468, mae: 14.106130, mean_q: 19.833679\n",
            " 132168/150000: episode: 19256, duration: 0.131s, episode steps:   8, steps per second:  61, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.319769, mae: 13.701334, mean_q: 19.892372\n",
            " 132176/150000: episode: 19257, duration: 0.117s, episode steps:   8, steps per second:  68, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.398983, mae: 13.692947, mean_q: 19.830814\n",
            " 132184/150000: episode: 19258, duration: 0.125s, episode steps:   8, steps per second:  64, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.625 [0.000, 8.000],  loss: 0.256719, mae: 14.137983, mean_q: 19.876535\n",
            " 132189/150000: episode: 19259, duration: 0.076s, episode steps:   5, steps per second:  66, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [1.000, 8.000],  loss: 0.303150, mae: 13.830984, mean_q: 19.943186\n",
            " 132196/150000: episode: 19260, duration: 0.087s, episode steps:   7, steps per second:  80, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.380822, mae: 13.729615, mean_q: 19.792192\n",
            " 132201/150000: episode: 19261, duration: 0.093s, episode steps:   5, steps per second:  54, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.578885, mae: 13.746445, mean_q: 19.871325\n",
            " 132209/150000: episode: 19262, duration: 0.130s, episode steps:   8, steps per second:  62, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.277446, mae: 13.928205, mean_q: 19.941490\n",
            " 132217/150000: episode: 19263, duration: 0.120s, episode steps:   8, steps per second:  67, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.353758, mae: 13.788727, mean_q: 20.066156\n",
            " 132226/150000: episode: 19264, duration: 0.148s, episode steps:   9, steps per second:  61, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.635960, mae: 13.751278, mean_q: 19.901075\n",
            " 132230/150000: episode: 19265, duration: 0.074s, episode steps:   4, steps per second:  54, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.750 [2.000, 6.000],  loss: 0.340773, mae: 13.736298, mean_q: 20.001328\n",
            " 132238/150000: episode: 19266, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.254928, mae: 13.888106, mean_q: 20.212240\n",
            " 132245/150000: episode: 19267, duration: 0.101s, episode steps:   7, steps per second:  70, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.234259, mae: 13.896472, mean_q: 20.125772\n",
            " 132250/150000: episode: 19268, duration: 0.083s, episode steps:   5, steps per second:  60, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.200 [1.000, 8.000],  loss: 0.278340, mae: 13.735361, mean_q: 20.007528\n",
            " 132256/150000: episode: 19269, duration: 0.092s, episode steps:   6, steps per second:  65, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 8.000],  loss: 0.261061, mae: 13.632108, mean_q: 19.893057\n",
            " 132265/150000: episode: 19270, duration: 0.146s, episode steps:   9, steps per second:  62, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.274840, mae: 14.045439, mean_q: 19.990252\n",
            " 132273/150000: episode: 19271, duration: 0.128s, episode steps:   8, steps per second:  63, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.261045, mae: 13.789849, mean_q: 19.997267\n",
            " 132281/150000: episode: 19272, duration: 0.127s, episode steps:   8, steps per second:  63, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.298986, mae: 13.732845, mean_q: 20.043774\n",
            " 132287/150000: episode: 19273, duration: 0.105s, episode steps:   6, steps per second:  57, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.000 [0.000, 8.000],  loss: 0.602531, mae: 13.981389, mean_q: 19.950741\n",
            " 132293/150000: episode: 19274, duration: 0.090s, episode steps:   6, steps per second:  67, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.000 [0.000, 8.000],  loss: 0.772205, mae: 14.186409, mean_q: 19.988485\n",
            " 132301/150000: episode: 19275, duration: 0.128s, episode steps:   8, steps per second:  62, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.439078, mae: 13.892497, mean_q: 20.090002\n",
            " 132308/150000: episode: 19276, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.388618, mae: 13.623706, mean_q: 19.961777\n",
            " 132315/150000: episode: 19277, duration: 0.073s, episode steps:   7, steps per second:  95, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.282546, mae: 13.931514, mean_q: 19.839621\n",
            " 132323/150000: episode: 19278, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.440853, mae: 14.051009, mean_q: 20.045372\n",
            " 132329/150000: episode: 19279, duration: 0.061s, episode steps:   6, steps per second:  98, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 2.833 [0.000, 7.000],  loss: 0.384279, mae: 13.870027, mean_q: 19.945820\n",
            " 132334/150000: episode: 19280, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [2.000, 6.000],  loss: 0.745449, mae: 13.916400, mean_q: 19.809208\n",
            " 132342/150000: episode: 19281, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.280074, mae: 13.870152, mean_q: 19.955889\n",
            " 132350/150000: episode: 19282, duration: 0.091s, episode steps:   8, steps per second:  87, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.276337, mae: 14.030124, mean_q: 20.017050\n",
            " 132358/150000: episode: 19283, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.378984, mae: 13.890916, mean_q: 19.874788\n",
            " 132365/150000: episode: 19284, duration: 0.069s, episode steps:   7, steps per second: 102, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.269548, mae: 14.183760, mean_q: 20.030172\n",
            " 132371/150000: episode: 19285, duration: 0.094s, episode steps:   6, steps per second:  64, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.167 [0.000, 8.000],  loss: 0.289668, mae: 13.582776, mean_q: 20.003912\n",
            " 132379/150000: episode: 19286, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.286943, mae: 13.917630, mean_q: 19.954472\n",
            " 132385/150000: episode: 19287, duration: 0.064s, episode steps:   6, steps per second:  94, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [1.000, 8.000],  loss: 0.227296, mae: 13.556133, mean_q: 19.946909\n",
            " 132392/150000: episode: 19288, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.232062, mae: 14.132994, mean_q: 20.020779\n",
            " 132398/150000: episode: 19289, duration: 0.071s, episode steps:   6, steps per second:  84, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [1.000, 8.000],  loss: 0.953131, mae: 13.902596, mean_q: 19.888620\n",
            " 132400/150000: episode: 19290, duration: 0.026s, episode steps:   2, steps per second:  76, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 7.000 [7.000, 7.000],  loss: 0.516105, mae: 13.489157, mean_q: 19.920303\n",
            " 132407/150000: episode: 19291, duration: 0.068s, episode steps:   7, steps per second: 103, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 7.000],  loss: 0.475276, mae: 14.142901, mean_q: 19.932652\n",
            " 132416/150000: episode: 19292, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.229542, mae: 13.974429, mean_q: 19.978088\n",
            " 132421/150000: episode: 19293, duration: 0.052s, episode steps:   5, steps per second:  96, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.600 [2.000, 8.000],  loss: 0.227974, mae: 13.901445, mean_q: 20.068182\n",
            " 132428/150000: episode: 19294, duration: 0.072s, episode steps:   7, steps per second:  97, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.223605, mae: 13.973702, mean_q: 20.054489\n",
            " 132434/150000: episode: 19295, duration: 0.061s, episode steps:   6, steps per second:  98, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.424644, mae: 13.906056, mean_q: 19.915640\n",
            " 132442/150000: episode: 19296, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.125 [0.000, 8.000],  loss: 0.278038, mae: 13.958254, mean_q: 19.756239\n",
            " 132450/150000: episode: 19297, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.353615, mae: 13.916513, mean_q: 19.895996\n",
            " 132457/150000: episode: 19298, duration: 0.076s, episode steps:   7, steps per second:  92, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.143 [0.000, 8.000],  loss: 0.286661, mae: 14.114897, mean_q: 19.839552\n",
            " 132463/150000: episode: 19299, duration: 0.072s, episode steps:   6, steps per second:  84, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 8.000],  loss: 0.278411, mae: 13.996223, mean_q: 20.096910\n",
            " 132469/150000: episode: 19300, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 2.833 [0.000, 8.000],  loss: 0.638668, mae: 14.002708, mean_q: 19.927048\n",
            " 132475/150000: episode: 19301, duration: 0.060s, episode steps:   6, steps per second: 101, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.167 [2.000, 8.000],  loss: 0.388038, mae: 13.780103, mean_q: 20.007391\n",
            " 132482/150000: episode: 19302, duration: 0.072s, episode steps:   7, steps per second:  98, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.321140, mae: 13.842610, mean_q: 20.261318\n",
            " 132489/150000: episode: 19303, duration: 0.079s, episode steps:   7, steps per second:  88, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.349552, mae: 13.876641, mean_q: 19.932444\n",
            " 132496/150000: episode: 19304, duration: 0.063s, episode steps:   7, steps per second: 111, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.286 [0.000, 8.000],  loss: 0.367546, mae: 13.789248, mean_q: 19.939974\n",
            " 132502/150000: episode: 19305, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [1.000, 8.000],  loss: 0.673644, mae: 13.869919, mean_q: 20.016317\n",
            " 132511/150000: episode: 19306, duration: 0.088s, episode steps:   9, steps per second: 103, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.290866, mae: 13.775580, mean_q: 20.074377\n",
            " 132520/150000: episode: 19307, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.237069, mae: 14.083917, mean_q: 20.053093\n",
            " 132529/150000: episode: 19308, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.222 [0.000, 8.000],  loss: 0.559805, mae: 13.769011, mean_q: 20.087965\n",
            " 132534/150000: episode: 19309, duration: 0.061s, episode steps:   5, steps per second:  82, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.400 [0.000, 8.000],  loss: 0.380213, mae: 13.554853, mean_q: 19.847523\n",
            " 132540/150000: episode: 19310, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.372496, mae: 13.673195, mean_q: 20.045572\n",
            " 132545/150000: episode: 19311, duration: 0.064s, episode steps:   5, steps per second:  78, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [0.000, 8.000],  loss: 0.292817, mae: 13.680821, mean_q: 20.028912\n",
            " 132554/150000: episode: 19312, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.111 [0.000, 8.000],  loss: 0.255368, mae: 14.021509, mean_q: 20.025026\n",
            " 132560/150000: episode: 19313, duration: 0.062s, episode steps:   6, steps per second:  97, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [1.000, 7.000],  loss: 0.497327, mae: 13.856364, mean_q: 20.042419\n",
            " 132567/150000: episode: 19314, duration: 0.065s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.444899, mae: 13.895433, mean_q: 19.910671\n",
            " 132572/150000: episode: 19315, duration: 0.050s, episode steps:   5, steps per second:  99, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.284018, mae: 13.993449, mean_q: 20.139328\n",
            " 132579/150000: episode: 19316, duration: 0.081s, episode steps:   7, steps per second:  87, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.000 [0.000, 8.000],  loss: 0.196615, mae: 13.756177, mean_q: 20.113220\n",
            " 132588/150000: episode: 19317, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.343358, mae: 13.664900, mean_q: 19.874044\n",
            " 132594/150000: episode: 19318, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.282458, mae: 13.416821, mean_q: 19.966988\n",
            " 132599/150000: episode: 19319, duration: 0.053s, episode steps:   5, steps per second:  95, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 1.800 [0.000, 5.000],  loss: 0.265782, mae: 13.380219, mean_q: 19.853371\n",
            " 132605/150000: episode: 19320, duration: 0.085s, episode steps:   6, steps per second:  71, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.167 [0.000, 7.000],  loss: 0.249857, mae: 13.748867, mean_q: 19.781576\n",
            " 132612/150000: episode: 19321, duration: 0.067s, episode steps:   7, steps per second: 104, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 5.143 [0.000, 8.000],  loss: 0.197108, mae: 14.081573, mean_q: 20.042828\n",
            " 132619/150000: episode: 19322, duration: 0.067s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.368037, mae: 14.070119, mean_q: 20.084719\n",
            " 132625/150000: episode: 19323, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.000 [0.000, 7.000],  loss: 0.501651, mae: 13.671673, mean_q: 20.071577\n",
            " 132633/150000: episode: 19324, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 1.012596, mae: 14.014768, mean_q: 19.915882\n",
            " 132639/150000: episode: 19325, duration: 0.067s, episode steps:   6, steps per second:  89, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.833 [1.000, 8.000],  loss: 0.277498, mae: 14.058933, mean_q: 20.019222\n",
            " 132645/150000: episode: 19326, duration: 0.069s, episode steps:   6, steps per second:  87, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [1.000, 8.000],  loss: 0.641505, mae: 13.776687, mean_q: 19.818153\n",
            " 132654/150000: episode: 19327, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.249560, mae: 13.978233, mean_q: 20.270294\n",
            " 132661/150000: episode: 19328, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 2.571 [0.000, 6.000],  loss: 0.486004, mae: 13.977979, mean_q: 19.907013\n",
            " 132667/150000: episode: 19329, duration: 0.064s, episode steps:   6, steps per second:  94, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.662494, mae: 13.362366, mean_q: 19.901802\n",
            " 132674/150000: episode: 19330, duration: 0.068s, episode steps:   7, steps per second: 103, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.256987, mae: 13.854239, mean_q: 20.029093\n",
            " 132680/150000: episode: 19331, duration: 0.071s, episode steps:   6, steps per second:  84, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.833 [0.000, 8.000],  loss: 0.263342, mae: 13.807106, mean_q: 20.124542\n",
            " 132688/150000: episode: 19332, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.375 [1.000, 8.000],  loss: 0.292023, mae: 13.608738, mean_q: 19.937990\n",
            " 132697/150000: episode: 19333, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.287326, mae: 14.106702, mean_q: 20.075138\n",
            " 132706/150000: episode: 19334, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.239866, mae: 13.950195, mean_q: 19.962099\n",
            " 132714/150000: episode: 19335, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.266535, mae: 13.701942, mean_q: 19.977392\n",
            " 132722/150000: episode: 19336, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.750 [1.000, 8.000],  loss: 0.165689, mae: 14.078779, mean_q: 19.972374\n",
            " 132728/150000: episode: 19337, duration: 0.061s, episode steps:   6, steps per second:  99, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.000 [2.000, 7.000],  loss: 0.440215, mae: 13.860081, mean_q: 19.834290\n",
            " 132733/150000: episode: 19338, duration: 0.062s, episode steps:   5, steps per second:  81, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.400 [0.000, 8.000],  loss: 0.288374, mae: 13.750269, mean_q: 20.085819\n",
            " 132738/150000: episode: 19339, duration: 0.054s, episode steps:   5, steps per second:  92, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.800 [1.000, 6.000],  loss: 0.328240, mae: 13.795530, mean_q: 19.966091\n",
            " 132743/150000: episode: 19340, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.600 [2.000, 8.000],  loss: 1.020965, mae: 14.217203, mean_q: 19.819965\n",
            " 132750/150000: episode: 19341, duration: 0.076s, episode steps:   7, steps per second:  92, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.143 [0.000, 8.000],  loss: 0.657991, mae: 13.845482, mean_q: 20.181326\n",
            " 132759/150000: episode: 19342, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.300708, mae: 13.663174, mean_q: 20.144508\n",
            " 132767/150000: episode: 19343, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.263293, mae: 13.862214, mean_q: 20.105156\n",
            " 132776/150000: episode: 19344, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.586737, mae: 13.701268, mean_q: 19.981077\n",
            " 132780/150000: episode: 19345, duration: 0.040s, episode steps:   4, steps per second: 100, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.500 [3.000, 7.000],  loss: 0.264824, mae: 13.624372, mean_q: 19.935726\n",
            " 132788/150000: episode: 19346, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.446220, mae: 13.983524, mean_q: 20.156395\n",
            " 132797/150000: episode: 19347, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.778 [0.000, 8.000],  loss: 0.257446, mae: 13.971868, mean_q: 20.149372\n",
            " 132803/150000: episode: 19348, duration: 0.071s, episode steps:   6, steps per second:  85, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.667 [2.000, 8.000],  loss: 0.237424, mae: 13.972510, mean_q: 20.295347\n",
            " 132812/150000: episode: 19349, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.507496, mae: 13.898372, mean_q: 20.119282\n",
            " 132821/150000: episode: 19350, duration: 0.095s, episode steps:   9, steps per second:  94, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.383874, mae: 13.803908, mean_q: 19.926863\n",
            " 132826/150000: episode: 19351, duration: 0.049s, episode steps:   5, steps per second: 102, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 6.000],  loss: 0.196654, mae: 13.923512, mean_q: 20.062378\n",
            " 132834/150000: episode: 19352, duration: 0.074s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.542873, mae: 13.842249, mean_q: 20.088146\n",
            " 132840/150000: episode: 19353, duration: 0.062s, episode steps:   6, steps per second:  96, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.295475, mae: 14.017457, mean_q: 20.167387\n",
            " 132843/150000: episode: 19354, duration: 0.052s, episode steps:   3, steps per second:  58, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 4.667 [2.000, 6.000],  loss: 0.226504, mae: 13.762073, mean_q: 20.411692\n",
            " 132849/150000: episode: 19355, duration: 0.057s, episode steps:   6, steps per second: 106, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [0.000, 7.000],  loss: 0.447549, mae: 13.880206, mean_q: 20.064781\n",
            " 132856/150000: episode: 19356, duration: 0.063s, episode steps:   7, steps per second: 112, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.281019, mae: 14.180837, mean_q: 20.048235\n",
            " 132861/150000: episode: 19357, duration: 0.047s, episode steps:   5, steps per second: 106, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.400 [0.000, 6.000],  loss: 0.216086, mae: 14.033237, mean_q: 20.069345\n",
            " 132869/150000: episode: 19358, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.511087, mae: 13.717564, mean_q: 20.032967\n",
            " 132874/150000: episode: 19359, duration: 0.050s, episode steps:   5, steps per second:  99, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.638748, mae: 14.039647, mean_q: 20.129612\n",
            " 132880/150000: episode: 19360, duration: 0.062s, episode steps:   6, steps per second:  96, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 6.000],  loss: 0.467700, mae: 14.058713, mean_q: 20.087816\n",
            " 132888/150000: episode: 19361, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.281644, mae: 13.712575, mean_q: 20.091618\n",
            " 132894/150000: episode: 19362, duration: 0.069s, episode steps:   6, steps per second:  86, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.667 [1.000, 8.000],  loss: 0.516863, mae: 13.547456, mean_q: 20.001385\n",
            " 132900/150000: episode: 19363, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.333 [2.000, 8.000],  loss: 0.285637, mae: 13.935330, mean_q: 20.065434\n",
            " 132907/150000: episode: 19364, duration: 0.071s, episode steps:   7, steps per second:  98, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [0.000, 8.000],  loss: 0.220199, mae: 13.766571, mean_q: 20.022388\n",
            " 132913/150000: episode: 19365, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.833 [3.000, 8.000],  loss: 0.590882, mae: 13.957299, mean_q: 20.200869\n",
            " 132920/150000: episode: 19366, duration: 0.073s, episode steps:   7, steps per second:  96, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.239888, mae: 14.017051, mean_q: 20.103207\n",
            " 132925/150000: episode: 19367, duration: 0.051s, episode steps:   5, steps per second:  98, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.400 [1.000, 8.000],  loss: 0.296559, mae: 14.076129, mean_q: 20.243418\n",
            " 132932/150000: episode: 19368, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.429 [0.000, 8.000],  loss: 0.246092, mae: 13.876307, mean_q: 20.047588\n",
            " 132936/150000: episode: 19369, duration: 0.043s, episode steps:   4, steps per second:  94, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.245584, mae: 14.129797, mean_q: 20.187311\n",
            " 132942/150000: episode: 19370, duration: 0.084s, episode steps:   6, steps per second:  72, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 8.000],  loss: 0.313624, mae: 13.989970, mean_q: 20.011986\n",
            " 132950/150000: episode: 19371, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.311658, mae: 14.116621, mean_q: 19.995325\n",
            " 132956/150000: episode: 19372, duration: 0.056s, episode steps:   6, steps per second: 106, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 6.000],  loss: 0.246294, mae: 14.090875, mean_q: 20.034010\n",
            " 132960/150000: episode: 19373, duration: 0.041s, episode steps:   4, steps per second:  98, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.000 [2.000, 8.000],  loss: 0.179805, mae: 14.091684, mean_q: 20.124664\n",
            " 132969/150000: episode: 19374, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.296255, mae: 13.829964, mean_q: 19.879993\n",
            " 132975/150000: episode: 19375, duration: 0.063s, episode steps:   6, steps per second:  96, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.399667, mae: 13.866664, mean_q: 19.994349\n",
            " 132983/150000: episode: 19376, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.305760, mae: 13.950302, mean_q: 20.036665\n",
            " 132991/150000: episode: 19377, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.758576, mae: 13.842220, mean_q: 19.762695\n",
            " 132998/150000: episode: 19378, duration: 0.069s, episode steps:   7, steps per second: 102, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.292280, mae: 14.032579, mean_q: 20.330853\n",
            " 133003/150000: episode: 19379, duration: 0.047s, episode steps:   5, steps per second: 105, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.400 [0.000, 6.000],  loss: 0.325709, mae: 13.498073, mean_q: 19.982962\n",
            " 133009/150000: episode: 19380, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 7.000],  loss: 0.423489, mae: 14.114486, mean_q: 20.014856\n",
            " 133015/150000: episode: 19381, duration: 0.072s, episode steps:   6, steps per second:  84, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.527643, mae: 13.734235, mean_q: 19.558065\n",
            " 133023/150000: episode: 19382, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.249711, mae: 14.050246, mean_q: 20.457350\n",
            " 133032/150000: episode: 19383, duration: 0.078s, episode steps:   9, steps per second: 115, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.585612, mae: 14.106005, mean_q: 19.837944\n",
            " 133037/150000: episode: 19384, duration: 0.056s, episode steps:   5, steps per second:  90, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [1.000, 8.000],  loss: 0.300489, mae: 14.055333, mean_q: 20.046728\n",
            " 133043/150000: episode: 19385, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 7.000],  loss: 0.664610, mae: 13.715623, mean_q: 19.863167\n",
            " 133050/150000: episode: 19386, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.307693, mae: 13.709311, mean_q: 19.942141\n",
            " 133056/150000: episode: 19387, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 2.500 [0.000, 6.000],  loss: 0.305469, mae: 14.174317, mean_q: 20.114777\n",
            " 133062/150000: episode: 19388, duration: 0.071s, episode steps:   6, steps per second:  85, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 1.057770, mae: 14.040835, mean_q: 20.028797\n",
            " 133067/150000: episode: 19389, duration: 0.047s, episode steps:   5, steps per second: 105, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.800 [0.000, 8.000],  loss: 0.360635, mae: 13.778272, mean_q: 20.187771\n",
            " 133075/150000: episode: 19390, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.500 [0.000, 8.000],  loss: 0.403346, mae: 14.051788, mean_q: 20.179462\n",
            " 133083/150000: episode: 19391, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.376602, mae: 13.842052, mean_q: 20.008575\n",
            " 133089/150000: episode: 19392, duration: 0.070s, episode steps:   6, steps per second:  85, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.167 [2.000, 8.000],  loss: 0.296991, mae: 14.033587, mean_q: 20.025675\n",
            " 133096/150000: episode: 19393, duration: 0.066s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.243811, mae: 13.636583, mean_q: 19.867113\n",
            " 133101/150000: episode: 19394, duration: 0.049s, episode steps:   5, steps per second: 103, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.000 [1.000, 8.000],  loss: 0.800954, mae: 13.690135, mean_q: 19.627064\n",
            " 133104/150000: episode: 19395, duration: 0.033s, episode steps:   3, steps per second:  90, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 5.333 [0.000, 8.000],  loss: 0.260380, mae: 14.142059, mean_q: 20.154949\n",
            " 133110/150000: episode: 19396, duration: 0.064s, episode steps:   6, steps per second:  94, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 1.185992, mae: 13.853236, mean_q: 19.714907\n",
            " 133116/150000: episode: 19397, duration: 0.068s, episode steps:   6, steps per second:  88, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.667 [0.000, 7.000],  loss: 0.256615, mae: 14.049197, mean_q: 20.158720\n",
            " 133123/150000: episode: 19398, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.633257, mae: 13.961641, mean_q: 20.105774\n",
            " 133132/150000: episode: 19399, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.237031, mae: 14.082903, mean_q: 20.103859\n",
            " 133140/150000: episode: 19400, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.688365, mae: 14.055828, mean_q: 20.057365\n",
            " 133148/150000: episode: 19401, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.400584, mae: 14.036125, mean_q: 19.873323\n",
            " 133156/150000: episode: 19402, duration: 0.081s, episode steps:   8, steps per second:  98, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.331129, mae: 13.987560, mean_q: 20.304695\n",
            " 133161/150000: episode: 19403, duration: 0.047s, episode steps:   5, steps per second: 106, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.200 [2.000, 8.000],  loss: 0.385232, mae: 13.802259, mean_q: 19.857107\n",
            " 133168/150000: episode: 19404, duration: 0.063s, episode steps:   7, steps per second: 112, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.259963, mae: 13.942543, mean_q: 20.034168\n",
            " 133174/150000: episode: 19405, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 6.000],  loss: 0.256302, mae: 14.045651, mean_q: 20.030502\n",
            " 133182/150000: episode: 19406, duration: 0.085s, episode steps:   8, steps per second:  95, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.457478, mae: 13.842336, mean_q: 19.877483\n",
            " 133190/150000: episode: 19407, duration: 0.071s, episode steps:   8, steps per second: 112, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.316500, mae: 13.732267, mean_q: 20.084156\n",
            " 133199/150000: episode: 19408, duration: 0.078s, episode steps:   9, steps per second: 116, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.556384, mae: 13.950112, mean_q: 19.846489\n",
            " 133206/150000: episode: 19409, duration: 0.079s, episode steps:   7, steps per second:  89, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.263402, mae: 13.823502, mean_q: 20.061270\n",
            " 133211/150000: episode: 19410, duration: 0.048s, episode steps:   5, steps per second: 103, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.400 [3.000, 7.000],  loss: 0.297944, mae: 13.628775, mean_q: 19.745098\n",
            " 133220/150000: episode: 19411, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.246327, mae: 14.128314, mean_q: 19.995811\n",
            " 133228/150000: episode: 19412, duration: 0.067s, episode steps:   8, steps per second: 119, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.656933, mae: 13.613707, mean_q: 19.866302\n",
            " 133234/150000: episode: 19413, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.290538, mae: 13.963577, mean_q: 19.843340\n",
            " 133241/150000: episode: 19414, duration: 0.063s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.470735, mae: 13.786858, mean_q: 20.150143\n",
            " 133250/150000: episode: 19415, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.293129, mae: 14.029960, mean_q: 20.040184\n",
            " 133257/150000: episode: 19416, duration: 0.066s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.301486, mae: 13.704226, mean_q: 20.166027\n",
            " 133262/150000: episode: 19417, duration: 0.049s, episode steps:   5, steps per second: 102, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.200 [2.000, 7.000],  loss: 0.275900, mae: 13.626022, mean_q: 20.023396\n",
            " 133267/150000: episode: 19418, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.600 [0.000, 6.000],  loss: 0.234675, mae: 13.789228, mean_q: 19.970058\n",
            " 133272/150000: episode: 19419, duration: 0.081s, episode steps:   5, steps per second:  61, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [0.000, 8.000],  loss: 0.450491, mae: 13.817142, mean_q: 19.966703\n",
            " 133278/150000: episode: 19420, duration: 0.093s, episode steps:   6, steps per second:  65, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [1.000, 8.000],  loss: 0.221458, mae: 13.842471, mean_q: 19.823736\n",
            " 133285/150000: episode: 19421, duration: 0.098s, episode steps:   7, steps per second:  71, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.303241, mae: 14.072078, mean_q: 19.968662\n",
            " 133294/150000: episode: 19422, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.392979, mae: 13.899839, mean_q: 19.902163\n",
            " 133301/150000: episode: 19423, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [0.000, 8.000],  loss: 0.359065, mae: 13.859746, mean_q: 19.854601\n",
            " 133307/150000: episode: 19424, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [1.000, 7.000],  loss: 0.443044, mae: 13.956143, mean_q: 20.019209\n",
            " 133315/150000: episode: 19425, duration: 0.135s, episode steps:   8, steps per second:  59, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.398812, mae: 13.948997, mean_q: 20.117184\n",
            " 133322/150000: episode: 19426, duration: 0.090s, episode steps:   7, steps per second:  78, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [1.000, 8.000],  loss: 0.215880, mae: 13.990976, mean_q: 20.115742\n",
            " 133327/150000: episode: 19427, duration: 0.077s, episode steps:   5, steps per second:  65, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.000 [2.000, 7.000],  loss: 0.325381, mae: 13.902494, mean_q: 20.076374\n",
            " 133335/150000: episode: 19428, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.319401, mae: 13.796513, mean_q: 19.857138\n",
            " 133343/150000: episode: 19429, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.284665, mae: 13.666487, mean_q: 19.917530\n",
            " 133350/150000: episode: 19430, duration: 0.090s, episode steps:   7, steps per second:  77, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.476739, mae: 13.755730, mean_q: 19.866934\n",
            " 133359/150000: episode: 19431, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.514035, mae: 13.978397, mean_q: 20.165092\n",
            " 133366/150000: episode: 19432, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.413533, mae: 13.848744, mean_q: 19.893538\n",
            " 133373/150000: episode: 19433, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.804949, mae: 14.169425, mean_q: 20.058912\n",
            " 133382/150000: episode: 19434, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.222 [0.000, 8.000],  loss: 0.318450, mae: 14.008440, mean_q: 19.995335\n",
            " 133389/150000: episode: 19435, duration: 0.123s, episode steps:   7, steps per second:  57, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.429 [0.000, 8.000],  loss: 0.348406, mae: 13.936206, mean_q: 20.455978\n",
            " 133397/150000: episode: 19436, duration: 0.126s, episode steps:   8, steps per second:  63, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.875 [0.000, 8.000],  loss: 0.320572, mae: 13.884043, mean_q: 19.911642\n",
            " 133402/150000: episode: 19437, duration: 0.084s, episode steps:   5, steps per second:  60, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.000 [0.000, 8.000],  loss: 0.478471, mae: 13.747789, mean_q: 20.082880\n",
            " 133411/150000: episode: 19438, duration: 0.135s, episode steps:   9, steps per second:  67, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.413028, mae: 13.950077, mean_q: 19.967131\n",
            " 133418/150000: episode: 19439, duration: 0.105s, episode steps:   7, steps per second:  67, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.626897, mae: 13.767604, mean_q: 20.134501\n",
            " 133426/150000: episode: 19440, duration: 0.131s, episode steps:   8, steps per second:  61, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.434143, mae: 13.736066, mean_q: 20.019077\n",
            " 133433/150000: episode: 19441, duration: 0.107s, episode steps:   7, steps per second:  66, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.262579, mae: 14.121190, mean_q: 20.008942\n",
            " 133440/150000: episode: 19442, duration: 0.108s, episode steps:   7, steps per second:  65, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.353080, mae: 13.924890, mean_q: 19.866405\n",
            " 133445/150000: episode: 19443, duration: 0.093s, episode steps:   5, steps per second:  54, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.400 [0.000, 6.000],  loss: 0.379085, mae: 14.027356, mean_q: 19.915005\n",
            " 133451/150000: episode: 19444, duration: 0.090s, episode steps:   6, steps per second:  67, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.290509, mae: 14.051562, mean_q: 20.165163\n",
            " 133456/150000: episode: 19445, duration: 0.078s, episode steps:   5, steps per second:  64, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.400 [2.000, 7.000],  loss: 0.208272, mae: 13.727819, mean_q: 19.907701\n",
            " 133463/150000: episode: 19446, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.571 [0.000, 8.000],  loss: 0.913759, mae: 13.662105, mean_q: 20.052273\n",
            " 133472/150000: episode: 19447, duration: 0.153s, episode steps:   9, steps per second:  59, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.250771, mae: 13.914513, mean_q: 19.927919\n",
            " 133479/150000: episode: 19448, duration: 0.108s, episode steps:   7, steps per second:  65, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.520077, mae: 13.723401, mean_q: 19.876802\n",
            " 133485/150000: episode: 19449, duration: 0.069s, episode steps:   6, steps per second:  87, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.167 [2.000, 8.000],  loss: 0.237079, mae: 13.750258, mean_q: 19.977501\n",
            " 133492/150000: episode: 19450, duration: 0.080s, episode steps:   7, steps per second:  87, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.502367, mae: 13.811198, mean_q: 19.940384\n",
            " 133499/150000: episode: 19451, duration: 0.067s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.457193, mae: 14.038733, mean_q: 20.204435\n",
            " 133508/150000: episode: 19452, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.290660, mae: 14.022753, mean_q: 19.858389\n",
            " 133516/150000: episode: 19453, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.200381, mae: 14.090984, mean_q: 20.375647\n",
            " 133525/150000: episode: 19454, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.458416, mae: 13.819442, mean_q: 19.827545\n",
            " 133530/150000: episode: 19455, duration: 0.055s, episode steps:   5, steps per second:  91, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.400 [0.000, 6.000],  loss: 0.228992, mae: 14.017609, mean_q: 20.006533\n",
            " 133539/150000: episode: 19456, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.222 [0.000, 7.000],  loss: 0.234321, mae: 13.880720, mean_q: 20.206089\n",
            " 133547/150000: episode: 19457, duration: 0.074s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.243312, mae: 13.957599, mean_q: 20.117851\n",
            " 133553/150000: episode: 19458, duration: 0.065s, episode steps:   6, steps per second:  92, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.167 [0.000, 8.000],  loss: 0.155822, mae: 14.058179, mean_q: 19.959242\n",
            " 133562/150000: episode: 19459, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.230710, mae: 14.006832, mean_q: 20.169083\n",
            " 133571/150000: episode: 19460, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.249268, mae: 14.036035, mean_q: 20.068848\n",
            " 133578/150000: episode: 19461, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.374621, mae: 13.975510, mean_q: 19.912914\n",
            " 133584/150000: episode: 19462, duration: 0.074s, episode steps:   6, steps per second:  81, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.175117, mae: 13.914943, mean_q: 20.066544\n",
            " 133593/150000: episode: 19463, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.320253, mae: 14.003174, mean_q: 19.962553\n",
            " 133600/150000: episode: 19464, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.494589, mae: 13.664416, mean_q: 19.754147\n",
            " 133607/150000: episode: 19465, duration: 0.077s, episode steps:   7, steps per second:  90, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.314212, mae: 13.694911, mean_q: 20.150311\n",
            " 133616/150000: episode: 19466, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.259259, mae: 14.011289, mean_q: 20.015902\n",
            " 133621/150000: episode: 19467, duration: 0.051s, episode steps:   5, steps per second:  97, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.247469, mae: 14.006493, mean_q: 20.007076\n",
            " 133629/150000: episode: 19468, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.187631, mae: 14.204325, mean_q: 20.174429\n",
            " 133634/150000: episode: 19469, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.600 [2.000, 8.000],  loss: 0.285171, mae: 14.106252, mean_q: 19.826155\n",
            " 133642/150000: episode: 19470, duration: 0.077s, episode steps:   8, steps per second: 103, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.114908, mae: 14.306083, mean_q: 20.072750\n",
            " 133646/150000: episode: 19471, duration: 0.041s, episode steps:   4, steps per second:  98, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.750 [3.000, 8.000],  loss: 0.319224, mae: 14.034809, mean_q: 19.945774\n",
            " 133655/150000: episode: 19472, duration: 0.104s, episode steps:   9, steps per second:  87, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.232405, mae: 13.856957, mean_q: 19.962624\n",
            " 133659/150000: episode: 19473, duration: 0.040s, episode steps:   4, steps per second: 101, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.750 [2.000, 7.000],  loss: 0.222118, mae: 13.727612, mean_q: 20.008230\n",
            " 133666/150000: episode: 19474, duration: 0.065s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.200043, mae: 13.830419, mean_q: 19.918909\n",
            " 133674/150000: episode: 19475, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.236277, mae: 13.855700, mean_q: 19.982338\n",
            " 133679/150000: episode: 19476, duration: 0.061s, episode steps:   5, steps per second:  82, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 8.000],  loss: 0.333966, mae: 13.727140, mean_q: 19.957523\n",
            " 133686/150000: episode: 19477, duration: 0.063s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.429 [0.000, 7.000],  loss: 0.160098, mae: 14.015260, mean_q: 19.995121\n",
            " 133693/150000: episode: 19478, duration: 0.067s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.549101, mae: 14.056632, mean_q: 19.830578\n",
            " 133700/150000: episode: 19479, duration: 0.072s, episode steps:   7, steps per second:  98, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.281537, mae: 14.119141, mean_q: 20.277302\n",
            " 133708/150000: episode: 19480, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.291166, mae: 13.878599, mean_q: 19.979042\n",
            " 133717/150000: episode: 19481, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.418477, mae: 13.792888, mean_q: 19.932079\n",
            " 133725/150000: episode: 19482, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.564965, mae: 13.848227, mean_q: 19.970053\n",
            " 133732/150000: episode: 19483, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.328940, mae: 13.684204, mean_q: 20.167233\n",
            " 133737/150000: episode: 19484, duration: 0.049s, episode steps:   5, steps per second: 101, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.600 [0.000, 8.000],  loss: 0.275592, mae: 14.113823, mean_q: 19.921991\n",
            " 133742/150000: episode: 19485, duration: 0.049s, episode steps:   5, steps per second: 103, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.355906, mae: 13.991724, mean_q: 20.149418\n",
            " 133747/150000: episode: 19486, duration: 0.058s, episode steps:   5, steps per second:  87, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.400 [0.000, 8.000],  loss: 0.213714, mae: 13.815035, mean_q: 20.133234\n",
            " 133752/150000: episode: 19487, duration: 0.057s, episode steps:   5, steps per second:  88, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.197248, mae: 14.024393, mean_q: 20.133408\n",
            " 133759/150000: episode: 19488, duration: 0.080s, episode steps:   7, steps per second:  88, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.367242, mae: 13.673777, mean_q: 19.921629\n",
            " 133765/150000: episode: 19489, duration: 0.062s, episode steps:   6, steps per second:  97, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.667 [2.000, 7.000],  loss: 0.212241, mae: 14.317767, mean_q: 20.109529\n",
            " 133772/150000: episode: 19490, duration: 0.078s, episode steps:   7, steps per second:  90, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.571 [0.000, 6.000],  loss: 0.392698, mae: 13.865164, mean_q: 19.947149\n",
            " 133779/150000: episode: 19491, duration: 0.065s, episode steps:   7, steps per second: 107, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.857 [0.000, 7.000],  loss: 0.479814, mae: 14.097679, mean_q: 19.962353\n",
            " 133785/150000: episode: 19492, duration: 0.064s, episode steps:   6, steps per second:  94, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2.667 [0.000, 6.000],  loss: 0.273013, mae: 14.043843, mean_q: 20.137146\n",
            " 133793/150000: episode: 19493, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.453643, mae: 13.739450, mean_q: 19.931908\n",
            " 133798/150000: episode: 19494, duration: 0.050s, episode steps:   5, steps per second: 101, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.400 [2.000, 7.000],  loss: 0.513202, mae: 13.826796, mean_q: 20.010841\n",
            " 133806/150000: episode: 19495, duration: 0.080s, episode steps:   8, steps per second: 101, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.499819, mae: 13.878513, mean_q: 19.908457\n",
            " 133815/150000: episode: 19496, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.238532, mae: 13.950756, mean_q: 20.001965\n",
            " 133823/150000: episode: 19497, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.500 [0.000, 8.000],  loss: 0.162905, mae: 13.947783, mean_q: 20.083961\n",
            " 133826/150000: episode: 19498, duration: 0.042s, episode steps:   3, steps per second:  72, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 5.333 [4.000, 6.000],  loss: 0.227683, mae: 14.051021, mean_q: 20.117161\n",
            " 133833/150000: episode: 19499, duration: 0.072s, episode steps:   7, steps per second:  97, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.559638, mae: 13.848547, mean_q: 19.955923\n",
            " 133842/150000: episode: 19500, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.486426, mae: 14.031933, mean_q: 19.969017\n",
            " 133850/150000: episode: 19501, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.260252, mae: 13.848467, mean_q: 20.096985\n",
            " 133858/150000: episode: 19502, duration: 0.077s, episode steps:   8, steps per second: 103, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.257666, mae: 14.209417, mean_q: 20.159742\n",
            " 133864/150000: episode: 19503, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.167 [0.000, 7.000],  loss: 0.206465, mae: 14.251212, mean_q: 20.049936\n",
            " 133873/150000: episode: 19504, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.234750, mae: 14.019520, mean_q: 20.179787\n",
            " 133879/150000: episode: 19505, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 7.000],  loss: 0.483161, mae: 13.970615, mean_q: 19.900852\n",
            " 133887/150000: episode: 19506, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.239232, mae: 14.162846, mean_q: 20.086472\n",
            " 133896/150000: episode: 19507, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.516776, mae: 13.895607, mean_q: 20.060066\n",
            " 133903/150000: episode: 19508, duration: 0.073s, episode steps:   7, steps per second:  95, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.439222, mae: 13.785573, mean_q: 19.881250\n",
            " 133910/150000: episode: 19509, duration: 0.077s, episode steps:   7, steps per second:  91, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.477288, mae: 14.105085, mean_q: 20.064581\n",
            " 133915/150000: episode: 19510, duration: 0.055s, episode steps:   5, steps per second:  91, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.800 [3.000, 8.000],  loss: 0.750781, mae: 13.986061, mean_q: 19.776377\n",
            " 133924/150000: episode: 19511, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.222 [0.000, 7.000],  loss: 0.348038, mae: 13.997562, mean_q: 20.327387\n",
            " 133931/150000: episode: 19512, duration: 0.068s, episode steps:   7, steps per second: 103, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.754960, mae: 13.613393, mean_q: 19.852936\n",
            " 133939/150000: episode: 19513, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.357432, mae: 13.835354, mean_q: 20.068958\n",
            " 133947/150000: episode: 19514, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.628879, mae: 13.776142, mean_q: 20.004513\n",
            " 133953/150000: episode: 19515, duration: 0.069s, episode steps:   6, steps per second:  87, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.500 [0.000, 8.000],  loss: 0.417511, mae: 13.623610, mean_q: 19.834764\n",
            " 133960/150000: episode: 19516, duration: 0.088s, episode steps:   7, steps per second:  80, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.143 [0.000, 8.000],  loss: 0.382154, mae: 14.049896, mean_q: 20.321981\n",
            " 133966/150000: episode: 19517, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.167 [0.000, 8.000],  loss: 0.394497, mae: 13.959577, mean_q: 19.934393\n",
            " 133971/150000: episode: 19518, duration: 0.048s, episode steps:   5, steps per second: 104, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [1.000, 8.000],  loss: 0.397807, mae: 14.322804, mean_q: 19.965939\n",
            " 133979/150000: episode: 19519, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.683469, mae: 13.841101, mean_q: 20.083469\n",
            " 133986/150000: episode: 19520, duration: 0.079s, episode steps:   7, steps per second:  89, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.286 [0.000, 8.000],  loss: 0.398800, mae: 13.942482, mean_q: 19.923170\n",
            " 133994/150000: episode: 19521, duration: 0.074s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.315649, mae: 13.971140, mean_q: 20.048546\n",
            " 133999/150000: episode: 19522, duration: 0.050s, episode steps:   5, steps per second: 101, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.000 [2.000, 8.000],  loss: 0.723092, mae: 13.979541, mean_q: 20.043674\n",
            " 134008/150000: episode: 19523, duration: 0.092s, episode steps:   9, steps per second:  97, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.286493, mae: 13.956196, mean_q: 19.917377\n",
            " 134017/150000: episode: 19524, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.257631, mae: 13.950021, mean_q: 20.040615\n",
            " 134022/150000: episode: 19525, duration: 0.056s, episode steps:   5, steps per second:  89, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 6.000],  loss: 0.774841, mae: 14.033595, mean_q: 19.824673\n",
            " 134031/150000: episode: 19526, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.396870, mae: 13.693492, mean_q: 20.111145\n",
            " 134033/150000: episode: 19527, duration: 0.024s, episode steps:   2, steps per second:  83, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.638025, mae: 13.836498, mean_q: 19.811733\n",
            " 134041/150000: episode: 19528, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.472699, mae: 13.647036, mean_q: 20.027557\n",
            " 134050/150000: episode: 19529, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.532059, mae: 13.862828, mean_q: 20.096920\n",
            " 134058/150000: episode: 19530, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.319436, mae: 13.796759, mean_q: 19.942455\n",
            " 134066/150000: episode: 19531, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.290067, mae: 13.940508, mean_q: 20.130592\n",
            " 134075/150000: episode: 19532, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.231750, mae: 13.717465, mean_q: 20.065765\n",
            " 134084/150000: episode: 19533, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.346612, mae: 13.856091, mean_q: 19.890476\n",
            " 134091/150000: episode: 19534, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.574000, mae: 13.828361, mean_q: 19.952562\n",
            " 134098/150000: episode: 19535, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.243440, mae: 13.943611, mean_q: 20.181911\n",
            " 134107/150000: episode: 19536, duration: 0.105s, episode steps:   9, steps per second:  86, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.198158, mae: 14.001320, mean_q: 20.081961\n",
            " 134116/150000: episode: 19537, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.260708, mae: 13.317247, mean_q: 20.037821\n",
            " 134124/150000: episode: 19538, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.268292, mae: 13.955688, mean_q: 19.991085\n",
            " 134130/150000: episode: 19539, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 7.000],  loss: 0.217557, mae: 14.012764, mean_q: 19.944260\n",
            " 134138/150000: episode: 19540, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.256610, mae: 13.897123, mean_q: 20.184891\n",
            " 134143/150000: episode: 19541, duration: 0.050s, episode steps:   5, steps per second: 101, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.400 [0.000, 6.000],  loss: 0.246683, mae: 13.883905, mean_q: 19.996262\n",
            " 134149/150000: episode: 19542, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 7.000],  loss: 0.503200, mae: 13.891208, mean_q: 19.936716\n",
            " 134156/150000: episode: 19543, duration: 0.094s, episode steps:   7, steps per second:  74, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.494140, mae: 13.794977, mean_q: 20.029400\n",
            " 134164/150000: episode: 19544, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.403117, mae: 14.129891, mean_q: 19.911678\n",
            " 134171/150000: episode: 19545, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.291217, mae: 14.257451, mean_q: 20.131811\n",
            " 134180/150000: episode: 19546, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.249921, mae: 13.904305, mean_q: 20.050474\n",
            " 134189/150000: episode: 19547, duration: 0.081s, episode steps:   9, steps per second: 112, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.438503, mae: 14.169955, mean_q: 19.941023\n",
            " 134198/150000: episode: 19548, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.269584, mae: 13.971930, mean_q: 20.050407\n",
            " 134203/150000: episode: 19549, duration: 0.061s, episode steps:   5, steps per second:  82, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.400 [1.000, 7.000],  loss: 0.358328, mae: 13.645338, mean_q: 19.854361\n",
            " 134211/150000: episode: 19550, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.574708, mae: 14.057890, mean_q: 19.829895\n",
            " 134220/150000: episode: 19551, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.667 [0.000, 8.000],  loss: 0.609894, mae: 13.843624, mean_q: 19.970917\n",
            " 134227/150000: episode: 19552, duration: 0.082s, episode steps:   7, steps per second:  86, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.917815, mae: 13.908922, mean_q: 20.099993\n",
            " 134234/150000: episode: 19553, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.580869, mae: 13.725579, mean_q: 19.942926\n",
            " 134243/150000: episode: 19554, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.371128, mae: 13.580342, mean_q: 20.156471\n",
            " 134251/150000: episode: 19555, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.756015, mae: 14.059008, mean_q: 19.855299\n",
            " 134257/150000: episode: 19556, duration: 0.068s, episode steps:   6, steps per second:  89, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 7.000],  loss: 0.271114, mae: 14.081506, mean_q: 20.083910\n",
            " 134264/150000: episode: 19557, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.446503, mae: 13.755485, mean_q: 19.765123\n",
            " 134269/150000: episode: 19558, duration: 0.050s, episode steps:   5, steps per second:  99, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.200 [2.000, 7.000],  loss: 0.250783, mae: 13.648341, mean_q: 19.998280\n",
            " 134277/150000: episode: 19559, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.247308, mae: 13.486824, mean_q: 20.002172\n",
            " 134280/150000: episode: 19560, duration: 0.038s, episode steps:   3, steps per second:  78, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 1.000 [0.000, 3.000],  loss: 0.514443, mae: 13.941505, mean_q: 19.971292\n",
            " 134289/150000: episode: 19561, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.296108, mae: 14.106719, mean_q: 19.860888\n",
            " 134293/150000: episode: 19562, duration: 0.041s, episode steps:   4, steps per second:  96, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.750 [3.000, 8.000],  loss: 0.721917, mae: 14.251956, mean_q: 19.977829\n",
            " 134296/150000: episode: 19563, duration: 0.034s, episode steps:   3, steps per second:  87, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 6.000 [2.000, 8.000],  loss: 0.173254, mae: 13.996613, mean_q: 20.046820\n",
            " 134301/150000: episode: 19564, duration: 0.073s, episode steps:   5, steps per second:  68, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.800 [0.000, 8.000],  loss: 0.415404, mae: 14.300557, mean_q: 20.141943\n",
            " 134305/150000: episode: 19565, duration: 0.042s, episode steps:   4, steps per second:  95, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.000 [1.000, 4.000],  loss: 0.540660, mae: 14.033360, mean_q: 20.376480\n",
            " 134313/150000: episode: 19566, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.303834, mae: 14.053942, mean_q: 20.095156\n",
            " 134320/150000: episode: 19567, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.225104, mae: 13.740424, mean_q: 20.069977\n",
            " 134328/150000: episode: 19568, duration: 0.085s, episode steps:   8, steps per second:  95, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.000 [0.000, 8.000],  loss: 0.580530, mae: 13.736103, mean_q: 19.946548\n",
            " 134336/150000: episode: 19569, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.875 [0.000, 7.000],  loss: 0.221301, mae: 13.949415, mean_q: 19.985233\n",
            " 134343/150000: episode: 19570, duration: 0.069s, episode steps:   7, steps per second: 101, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 2.857 [0.000, 6.000],  loss: 0.416102, mae: 14.022727, mean_q: 19.969662\n",
            " 134352/150000: episode: 19571, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.471053, mae: 13.894694, mean_q: 19.902437\n",
            " 134359/150000: episode: 19572, duration: 0.069s, episode steps:   7, steps per second: 102, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.286 [1.000, 7.000],  loss: 0.538034, mae: 14.118501, mean_q: 19.939272\n",
            " 134366/150000: episode: 19573, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.494191, mae: 13.999192, mean_q: 19.827616\n",
            " 134371/150000: episode: 19574, duration: 0.048s, episode steps:   5, steps per second: 103, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.800 [0.000, 8.000],  loss: 0.745416, mae: 13.906507, mean_q: 20.524515\n",
            " 134380/150000: episode: 19575, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.489967, mae: 13.677023, mean_q: 20.093723\n",
            " 134385/150000: episode: 19576, duration: 0.049s, episode steps:   5, steps per second: 101, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.400 [0.000, 6.000],  loss: 0.397223, mae: 14.071356, mean_q: 20.348333\n",
            " 134392/150000: episode: 19577, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.373171, mae: 14.014556, mean_q: 20.026327\n",
            " 134400/150000: episode: 19578, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.391401, mae: 13.967690, mean_q: 20.138176\n",
            " 134409/150000: episode: 19579, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.447661, mae: 14.069610, mean_q: 19.972305\n",
            " 134415/150000: episode: 19580, duration: 0.059s, episode steps:   6, steps per second: 101, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 8.000],  loss: 0.402225, mae: 14.054962, mean_q: 20.086510\n",
            " 134419/150000: episode: 19581, duration: 0.040s, episode steps:   4, steps per second:  99, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.500 [2.000, 6.000],  loss: 0.246511, mae: 13.600780, mean_q: 19.992468\n",
            " 134424/150000: episode: 19582, duration: 0.047s, episode steps:   5, steps per second: 106, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.400 [3.000, 7.000],  loss: 0.487575, mae: 14.041654, mean_q: 19.920267\n",
            " 134433/150000: episode: 19583, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.453631, mae: 13.889556, mean_q: 20.015081\n",
            " 134442/150000: episode: 19584, duration: 0.079s, episode steps:   9, steps per second: 113, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.414807, mae: 14.384456, mean_q: 20.146322\n",
            " 134448/150000: episode: 19585, duration: 0.062s, episode steps:   6, steps per second:  97, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.508058, mae: 13.740787, mean_q: 19.936407\n",
            " 134455/150000: episode: 19586, duration: 0.094s, episode steps:   7, steps per second:  74, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.231121, mae: 14.155624, mean_q: 19.912720\n",
            " 134462/150000: episode: 19587, duration: 0.111s, episode steps:   7, steps per second:  63, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.262192, mae: 14.091962, mean_q: 20.121334\n",
            " 134469/150000: episode: 19588, duration: 0.106s, episode steps:   7, steps per second:  66, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.607932, mae: 13.761027, mean_q: 19.920673\n",
            " 134475/150000: episode: 19589, duration: 0.090s, episode steps:   6, steps per second:  67, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.884229, mae: 14.001908, mean_q: 19.960957\n",
            " 134481/150000: episode: 19590, duration: 0.086s, episode steps:   6, steps per second:  69, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.553988, mae: 13.949348, mean_q: 20.499619\n",
            " 134489/150000: episode: 19591, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.523368, mae: 13.978060, mean_q: 19.919422\n",
            " 134496/150000: episode: 19592, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 5.286 [1.000, 8.000],  loss: 0.254041, mae: 14.443097, mean_q: 20.384199\n",
            " 134504/150000: episode: 19593, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.500 [0.000, 8.000],  loss: 0.472407, mae: 14.049753, mean_q: 20.052706\n",
            " 134509/150000: episode: 19594, duration: 0.079s, episode steps:   5, steps per second:  63, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.000 [1.000, 7.000],  loss: 0.365438, mae: 13.850718, mean_q: 20.003040\n",
            " 134515/150000: episode: 19595, duration: 0.085s, episode steps:   6, steps per second:  71, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.479912, mae: 13.826999, mean_q: 20.239830\n",
            " 134523/150000: episode: 19596, duration: 0.103s, episode steps:   8, steps per second:  77, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.375 [0.000, 7.000],  loss: 0.291162, mae: 13.920193, mean_q: 19.952156\n",
            " 134531/150000: episode: 19597, duration: 0.118s, episode steps:   8, steps per second:  68, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.390558, mae: 13.862345, mean_q: 20.096500\n",
            " 134539/150000: episode: 19598, duration: 0.099s, episode steps:   8, steps per second:  81, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.465944, mae: 13.854771, mean_q: 19.712902\n",
            " 134548/150000: episode: 19599, duration: 0.109s, episode steps:   9, steps per second:  83, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.295954, mae: 13.907323, mean_q: 20.224165\n",
            " 134555/150000: episode: 19600, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.560796, mae: 13.742785, mean_q: 19.735693\n",
            " 134563/150000: episode: 19601, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.291957, mae: 13.892152, mean_q: 19.653908\n",
            " 134566/150000: episode: 19602, duration: 0.065s, episode steps:   3, steps per second:  46, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 5.333 [2.000, 7.000],  loss: 0.205477, mae: 14.341237, mean_q: 20.273527\n",
            " 134575/150000: episode: 19603, duration: 0.121s, episode steps:   9, steps per second:  75, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.209798, mae: 13.964727, mean_q: 19.911900\n",
            " 134581/150000: episode: 19604, duration: 0.106s, episode steps:   6, steps per second:  57, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.500 [1.000, 7.000],  loss: 0.403450, mae: 13.774465, mean_q: 19.757490\n",
            " 134589/150000: episode: 19605, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.609054, mae: 14.294891, mean_q: 20.030239\n",
            " 134597/150000: episode: 19606, duration: 0.128s, episode steps:   8, steps per second:  62, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.339233, mae: 13.790771, mean_q: 19.925919\n",
            " 134605/150000: episode: 19607, duration: 0.138s, episode steps:   8, steps per second:  58, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.815056, mae: 13.768583, mean_q: 20.050667\n",
            " 134610/150000: episode: 19608, duration: 0.085s, episode steps:   5, steps per second:  59, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.600 [2.000, 7.000],  loss: 0.324230, mae: 13.333822, mean_q: 20.147903\n",
            " 134615/150000: episode: 19609, duration: 0.079s, episode steps:   5, steps per second:  63, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.278114, mae: 14.066371, mean_q: 20.500980\n",
            " 134624/150000: episode: 19610, duration: 0.138s, episode steps:   9, steps per second:  65, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.341209, mae: 13.961575, mean_q: 19.952904\n",
            " 134630/150000: episode: 19611, duration: 0.099s, episode steps:   6, steps per second:  61, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.434033, mae: 14.012767, mean_q: 20.084715\n",
            " 134639/150000: episode: 19612, duration: 0.141s, episode steps:   9, steps per second:  64, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.268102, mae: 13.807269, mean_q: 20.007896\n",
            " 134647/150000: episode: 19613, duration: 0.118s, episode steps:   8, steps per second:  68, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.266299, mae: 13.860329, mean_q: 20.027554\n",
            " 134653/150000: episode: 19614, duration: 0.101s, episode steps:   6, steps per second:  59, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.333 [2.000, 7.000],  loss: 0.327850, mae: 13.711219, mean_q: 19.963686\n",
            " 134661/150000: episode: 19615, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.456763, mae: 14.066330, mean_q: 19.960972\n",
            " 134670/150000: episode: 19616, duration: 0.132s, episode steps:   9, steps per second:  68, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.387280, mae: 14.123125, mean_q: 20.100891\n",
            " 134676/150000: episode: 19617, duration: 0.096s, episode steps:   6, steps per second:  63, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.651663, mae: 13.984375, mean_q: 19.922915\n",
            " 134683/150000: episode: 19618, duration: 0.097s, episode steps:   7, steps per second:  73, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.303340, mae: 14.125157, mean_q: 20.066727\n",
            " 134691/150000: episode: 19619, duration: 0.123s, episode steps:   8, steps per second:  65, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.548904, mae: 13.632988, mean_q: 19.931789\n",
            " 134697/150000: episode: 19620, duration: 0.086s, episode steps:   6, steps per second:  69, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.000 [0.000, 8.000],  loss: 0.288663, mae: 14.195190, mean_q: 19.970947\n",
            " 134703/150000: episode: 19621, duration: 0.061s, episode steps:   6, steps per second:  98, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.679698, mae: 13.854660, mean_q: 20.074846\n",
            " 134712/150000: episode: 19622, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.359238, mae: 13.985239, mean_q: 20.050606\n",
            " 134718/150000: episode: 19623, duration: 0.056s, episode steps:   6, steps per second: 108, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.833 [1.000, 6.000],  loss: 0.209371, mae: 14.029122, mean_q: 20.179733\n",
            " 134723/150000: episode: 19624, duration: 0.075s, episode steps:   5, steps per second:  67, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.479780, mae: 13.950659, mean_q: 20.172728\n",
            " 134732/150000: episode: 19625, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.329721, mae: 13.603023, mean_q: 19.843801\n",
            " 134738/150000: episode: 19626, duration: 0.063s, episode steps:   6, steps per second:  96, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [1.000, 8.000],  loss: 0.342623, mae: 13.791718, mean_q: 20.114973\n",
            " 134744/150000: episode: 19627, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 2.667 [0.000, 8.000],  loss: 0.295107, mae: 13.860799, mean_q: 19.946640\n",
            " 134753/150000: episode: 19628, duration: 0.093s, episode steps:   9, steps per second:  96, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.330088, mae: 13.513195, mean_q: 19.788822\n",
            " 134759/150000: episode: 19629, duration: 0.073s, episode steps:   6, steps per second:  83, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.167 [0.000, 8.000],  loss: 0.264525, mae: 13.309166, mean_q: 20.168432\n",
            " 134768/150000: episode: 19630, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.195207, mae: 14.054485, mean_q: 20.036373\n",
            " 134777/150000: episode: 19631, duration: 0.103s, episode steps:   9, steps per second:  87, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.318391, mae: 13.759966, mean_q: 19.921156\n",
            " 134785/150000: episode: 19632, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.305910, mae: 13.634140, mean_q: 19.936646\n",
            " 134789/150000: episode: 19633, duration: 0.042s, episode steps:   4, steps per second:  95, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.750 [2.000, 8.000],  loss: 0.342094, mae: 13.893805, mean_q: 20.171215\n",
            " 134796/150000: episode: 19634, duration: 0.068s, episode steps:   7, steps per second: 102, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.545512, mae: 13.907399, mean_q: 19.863880\n",
            " 134803/150000: episode: 19635, duration: 0.079s, episode steps:   7, steps per second:  88, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.347796, mae: 13.897952, mean_q: 20.001511\n",
            " 134809/150000: episode: 19636, duration: 0.062s, episode steps:   6, steps per second:  97, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.500 [0.000, 8.000],  loss: 0.254319, mae: 13.889079, mean_q: 20.100031\n",
            " 134817/150000: episode: 19637, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.616871, mae: 13.765815, mean_q: 19.889738\n",
            " 134824/150000: episode: 19638, duration: 0.082s, episode steps:   7, steps per second:  85, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.332131, mae: 13.773606, mean_q: 19.775417\n",
            " 134829/150000: episode: 19639, duration: 0.055s, episode steps:   5, steps per second:  91, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.000 [2.000, 8.000],  loss: 0.402148, mae: 13.826048, mean_q: 19.932384\n",
            " 134836/150000: episode: 19640, duration: 0.067s, episode steps:   7, steps per second: 104, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.654682, mae: 13.858644, mean_q: 20.032324\n",
            " 134845/150000: episode: 19641, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.342130, mae: 13.944942, mean_q: 20.073553\n",
            " 134849/150000: episode: 19642, duration: 0.047s, episode steps:   4, steps per second:  86, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.250 [1.000, 7.000],  loss: 0.191835, mae: 14.381931, mean_q: 20.353111\n",
            " 134855/150000: episode: 19643, duration: 0.072s, episode steps:   6, steps per second:  84, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.167 [2.000, 8.000],  loss: 0.582963, mae: 13.756701, mean_q: 19.881275\n",
            " 134862/150000: episode: 19644, duration: 0.073s, episode steps:   7, steps per second:  95, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.337480, mae: 13.791689, mean_q: 19.902090\n",
            " 134871/150000: episode: 19645, duration: 0.121s, episode steps:   9, steps per second:  74, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.111 [0.000, 8.000],  loss: 0.212037, mae: 14.096194, mean_q: 20.084318\n",
            " 134880/150000: episode: 19646, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.247006, mae: 14.123339, mean_q: 20.109896\n",
            " 134886/150000: episode: 19647, duration: 0.061s, episode steps:   6, steps per second:  98, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [1.000, 8.000],  loss: 0.658226, mae: 14.048982, mean_q: 19.745110\n",
            " 134892/150000: episode: 19648, duration: 0.064s, episode steps:   6, steps per second:  94, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 6.000],  loss: 0.389486, mae: 14.101337, mean_q: 20.161419\n",
            " 134900/150000: episode: 19649, duration: 0.086s, episode steps:   8, steps per second:  94, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.411605, mae: 13.824228, mean_q: 20.004734\n",
            " 134905/150000: episode: 19650, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [1.000, 8.000],  loss: 0.412982, mae: 14.211726, mean_q: 20.100840\n",
            " 134914/150000: episode: 19651, duration: 0.094s, episode steps:   9, steps per second:  95, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.397789, mae: 13.877253, mean_q: 20.147831\n",
            " 134921/150000: episode: 19652, duration: 0.068s, episode steps:   7, steps per second: 103, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.460535, mae: 13.920772, mean_q: 19.914642\n",
            " 134925/150000: episode: 19653, duration: 0.046s, episode steps:   4, steps per second:  86, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.250 [1.000, 7.000],  loss: 0.224707, mae: 14.277048, mean_q: 20.225342\n",
            " 134933/150000: episode: 19654, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.667810, mae: 13.959770, mean_q: 19.742430\n",
            " 134941/150000: episode: 19655, duration: 0.085s, episode steps:   8, steps per second:  95, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.543775, mae: 14.015331, mean_q: 20.321930\n",
            " 134949/150000: episode: 19656, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.250 [0.000, 8.000],  loss: 0.267136, mae: 13.758222, mean_q: 20.033813\n",
            " 134954/150000: episode: 19657, duration: 0.059s, episode steps:   5, steps per second:  85, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.800 [0.000, 7.000],  loss: 0.576207, mae: 14.011879, mean_q: 19.980957\n",
            " 134963/150000: episode: 19658, duration: 0.101s, episode steps:   9, steps per second:  90, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.487084, mae: 13.920652, mean_q: 19.930458\n",
            " 134968/150000: episode: 19659, duration: 0.053s, episode steps:   5, steps per second:  95, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.000 [3.000, 7.000],  loss: 0.301795, mae: 14.008097, mean_q: 20.047619\n",
            " 134977/150000: episode: 19660, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.522334, mae: 13.883690, mean_q: 20.358212\n",
            " 134985/150000: episode: 19661, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.478736, mae: 13.777521, mean_q: 19.905676\n",
            " 134992/150000: episode: 19662, duration: 0.071s, episode steps:   7, steps per second:  99, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.286 [1.000, 7.000],  loss: 0.806656, mae: 13.811559, mean_q: 20.025404\n",
            " 135001/150000: episode: 19663, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.446413, mae: 13.857590, mean_q: 20.075331\n",
            " 135010/150000: episode: 19664, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.567682, mae: 13.867724, mean_q: 20.057184\n",
            " 135015/150000: episode: 19665, duration: 0.054s, episode steps:   5, steps per second:  93, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.242199, mae: 13.962537, mean_q: 20.149801\n",
            " 135022/150000: episode: 19666, duration: 0.071s, episode steps:   7, steps per second:  98, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 2.571 [0.000, 6.000],  loss: 0.411381, mae: 13.826172, mean_q: 20.260387\n",
            " 135029/150000: episode: 19667, duration: 0.067s, episode steps:   7, steps per second: 104, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.456744, mae: 13.822497, mean_q: 19.787491\n",
            " 135037/150000: episode: 19668, duration: 0.087s, episode steps:   8, steps per second:  91, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.526353, mae: 13.742314, mean_q: 20.117359\n",
            " 135044/150000: episode: 19669, duration: 0.067s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.340612, mae: 13.791228, mean_q: 19.910788\n",
            " 135052/150000: episode: 19670, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.203846, mae: 13.932348, mean_q: 20.002081\n",
            " 135060/150000: episode: 19671, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.217033, mae: 14.090120, mean_q: 20.043804\n",
            " 135066/150000: episode: 19672, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [1.000, 8.000],  loss: 0.232278, mae: 13.567964, mean_q: 20.025772\n",
            " 135069/150000: episode: 19673, duration: 0.033s, episode steps:   3, steps per second:  92, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.343475, mae: 13.521363, mean_q: 20.061388\n",
            " 135072/150000: episode: 19674, duration: 0.033s, episode steps:   3, steps per second:  92, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.228586, mae: 13.940322, mean_q: 19.974169\n",
            " 135077/150000: episode: 19675, duration: 0.048s, episode steps:   5, steps per second: 103, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.800 [0.000, 8.000],  loss: 0.171277, mae: 14.026179, mean_q: 20.128613\n",
            " 135083/150000: episode: 19676, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 6.000],  loss: 0.387234, mae: 14.327998, mean_q: 19.997440\n",
            " 135092/150000: episode: 19677, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.215988, mae: 14.051285, mean_q: 19.952129\n",
            " 135100/150000: episode: 19678, duration: 0.077s, episode steps:   8, steps per second: 103, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.496392, mae: 14.135419, mean_q: 19.952591\n",
            " 135107/150000: episode: 19679, duration: 0.079s, episode steps:   7, steps per second:  89, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.857 [0.000, 8.000],  loss: 0.214740, mae: 13.910372, mean_q: 20.022680\n",
            " 135115/150000: episode: 19680, duration: 0.074s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.492882, mae: 13.883545, mean_q: 20.124420\n",
            " 135123/150000: episode: 19681, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.483461, mae: 13.656961, mean_q: 20.001167\n",
            " 135130/150000: episode: 19682, duration: 0.082s, episode steps:   7, steps per second:  85, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.143 [0.000, 8.000],  loss: 0.352900, mae: 13.696754, mean_q: 20.034372\n",
            " 135138/150000: episode: 19683, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.326885, mae: 13.671140, mean_q: 20.083382\n",
            " 135147/150000: episode: 19684, duration: 0.096s, episode steps:   9, steps per second:  93, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.362185, mae: 13.948133, mean_q: 20.088654\n",
            " 135155/150000: episode: 19685, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.466469, mae: 14.014793, mean_q: 19.926399\n",
            " 135159/150000: episode: 19686, duration: 0.043s, episode steps:   4, steps per second:  94, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.500 [3.000, 7.000],  loss: 0.286833, mae: 13.799706, mean_q: 20.059605\n",
            " 135164/150000: episode: 19687, duration: 0.055s, episode steps:   5, steps per second:  92, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.800 [0.000, 8.000],  loss: 0.325792, mae: 14.046679, mean_q: 20.134314\n",
            " 135171/150000: episode: 19688, duration: 0.068s, episode steps:   7, steps per second: 103, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.231583, mae: 14.044150, mean_q: 19.818665\n",
            " 135178/150000: episode: 19689, duration: 0.083s, episode steps:   7, steps per second:  85, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.714 [2.000, 8.000],  loss: 0.337038, mae: 14.296127, mean_q: 19.979189\n",
            " 135186/150000: episode: 19690, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.228954, mae: 13.980375, mean_q: 19.948261\n",
            " 135193/150000: episode: 19691, duration: 0.071s, episode steps:   7, steps per second:  99, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.000 [0.000, 8.000],  loss: 0.382908, mae: 14.078164, mean_q: 19.950573\n",
            " 135198/150000: episode: 19692, duration: 0.055s, episode steps:   5, steps per second:  90, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.600 [1.000, 5.000],  loss: 0.643743, mae: 13.982434, mean_q: 20.065187\n",
            " 135205/150000: episode: 19693, duration: 0.076s, episode steps:   7, steps per second:  92, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.314132, mae: 14.069055, mean_q: 19.868427\n",
            " 135211/150000: episode: 19694, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.300942, mae: 14.050980, mean_q: 20.253098\n",
            " 135217/150000: episode: 19695, duration: 0.061s, episode steps:   6, steps per second:  98, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.327864, mae: 14.181664, mean_q: 19.886044\n",
            " 135224/150000: episode: 19696, duration: 0.082s, episode steps:   7, steps per second:  86, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.320874, mae: 14.022393, mean_q: 20.034971\n",
            " 135229/150000: episode: 19697, duration: 0.049s, episode steps:   5, steps per second: 102, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.000 [2.000, 7.000],  loss: 0.175450, mae: 13.897830, mean_q: 20.080072\n",
            " 135232/150000: episode: 19698, duration: 0.034s, episode steps:   3, steps per second:  89, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.164357, mae: 14.174206, mean_q: 19.802355\n",
            " 135241/150000: episode: 19699, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.270260, mae: 14.045397, mean_q: 19.996279\n",
            " 135248/150000: episode: 19700, duration: 0.102s, episode steps:   7, steps per second:  68, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.857 [0.000, 8.000],  loss: 0.217523, mae: 13.951484, mean_q: 20.099136\n",
            " 135255/150000: episode: 19701, duration: 0.067s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.567737, mae: 14.288575, mean_q: 19.829746\n",
            " 135261/150000: episode: 19702, duration: 0.061s, episode steps:   6, steps per second:  98, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.667 [0.000, 8.000],  loss: 0.396184, mae: 13.992536, mean_q: 19.882566\n",
            " 135266/150000: episode: 19703, duration: 0.050s, episode steps:   5, steps per second:  99, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.272463, mae: 13.906589, mean_q: 20.039818\n",
            " 135272/150000: episode: 19704, duration: 0.073s, episode steps:   6, steps per second:  82, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [1.000, 7.000],  loss: 0.158598, mae: 14.044049, mean_q: 20.067488\n",
            " 135277/150000: episode: 19705, duration: 0.055s, episode steps:   5, steps per second:  91, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.000 [2.000, 7.000],  loss: 0.358564, mae: 14.225143, mean_q: 19.970545\n",
            " 135284/150000: episode: 19706, duration: 0.070s, episode steps:   7, steps per second: 100, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.125163, mae: 14.114848, mean_q: 19.874943\n",
            " 135286/150000: episode: 19707, duration: 0.027s, episode steps:   2, steps per second:  75, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 4.000 [4.000, 4.000],  loss: 0.119477, mae: 14.271847, mean_q: 20.166931\n",
            " 135291/150000: episode: 19708, duration: 0.063s, episode steps:   5, steps per second:  80, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.800 [2.000, 7.000],  loss: 0.436989, mae: 14.064537, mean_q: 19.907049\n",
            " 135299/150000: episode: 19709, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.301995, mae: 13.573137, mean_q: 19.866230\n",
            " 135306/150000: episode: 19710, duration: 0.068s, episode steps:   7, steps per second: 104, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.545399, mae: 13.860364, mean_q: 19.916689\n",
            " 135312/150000: episode: 19711, duration: 0.071s, episode steps:   6, steps per second:  85, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.000 [1.000, 8.000],  loss: 0.240274, mae: 14.205765, mean_q: 19.764967\n",
            " 135318/150000: episode: 19712, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.305095, mae: 13.952682, mean_q: 20.210840\n",
            " 135324/150000: episode: 19713, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.482818, mae: 14.015174, mean_q: 20.020926\n",
            " 135329/150000: episode: 19714, duration: 0.056s, episode steps:   5, steps per second:  89, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.400 [2.000, 8.000],  loss: 0.281732, mae: 13.816160, mean_q: 19.894487\n",
            " 135335/150000: episode: 19715, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.170258, mae: 13.953869, mean_q: 20.078836\n",
            " 135341/150000: episode: 19716, duration: 0.066s, episode steps:   6, steps per second:  91, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.167 [2.000, 7.000],  loss: 0.281516, mae: 13.681427, mean_q: 20.154413\n",
            " 135347/150000: episode: 19717, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 7.000],  loss: 0.307650, mae: 13.993591, mean_q: 19.989985\n",
            " 135356/150000: episode: 19718, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.380973, mae: 13.810843, mean_q: 20.004250\n",
            " 135364/150000: episode: 19719, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.508654, mae: 13.627549, mean_q: 19.928486\n",
            " 135373/150000: episode: 19720, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.444504, mae: 13.647817, mean_q: 19.900023\n",
            " 135382/150000: episode: 19721, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.454205, mae: 14.079445, mean_q: 19.997637\n",
            " 135390/150000: episode: 19722, duration: 0.074s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.201956, mae: 14.031992, mean_q: 19.978924\n",
            " 135398/150000: episode: 19723, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.306764, mae: 14.020124, mean_q: 20.139980\n",
            " 135404/150000: episode: 19724, duration: 0.070s, episode steps:   6, steps per second:  86, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [0.000, 7.000],  loss: 0.450769, mae: 13.873706, mean_q: 20.044901\n",
            " 135411/150000: episode: 19725, duration: 0.072s, episode steps:   7, steps per second:  97, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.336727, mae: 13.957618, mean_q: 20.114151\n",
            " 135418/150000: episode: 19726, duration: 0.076s, episode steps:   7, steps per second:  92, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.243868, mae: 13.959546, mean_q: 19.964781\n",
            " 135424/150000: episode: 19727, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.333 [2.000, 8.000],  loss: 0.358430, mae: 13.619105, mean_q: 19.988895\n",
            " 135433/150000: episode: 19728, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.439755, mae: 14.023115, mean_q: 19.914431\n",
            " 135439/150000: episode: 19729, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.249108, mae: 13.964996, mean_q: 19.950556\n",
            " 135445/150000: episode: 19730, duration: 0.070s, episode steps:   6, steps per second:  86, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.333 [2.000, 8.000],  loss: 0.384221, mae: 13.829103, mean_q: 20.177507\n",
            " 135454/150000: episode: 19731, duration: 0.088s, episode steps:   9, steps per second: 103, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.481742, mae: 13.712068, mean_q: 19.867573\n",
            " 135459/150000: episode: 19732, duration: 0.051s, episode steps:   5, steps per second:  98, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [1.000, 8.000],  loss: 0.246371, mae: 13.663401, mean_q: 20.233669\n",
            " 135466/150000: episode: 19733, duration: 0.070s, episode steps:   7, steps per second: 100, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.567437, mae: 13.778259, mean_q: 19.719330\n",
            " 135474/150000: episode: 19734, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 5.000 [1.000, 8.000],  loss: 0.774541, mae: 13.615318, mean_q: 20.121197\n",
            " 135482/150000: episode: 19735, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.269540, mae: 13.897631, mean_q: 19.974674\n",
            " 135490/150000: episode: 19736, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.875 [1.000, 8.000],  loss: 0.215167, mae: 13.791105, mean_q: 20.158348\n",
            " 135496/150000: episode: 19737, duration: 0.062s, episode steps:   6, steps per second:  97, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.333 [2.000, 8.000],  loss: 0.743205, mae: 13.593823, mean_q: 19.872728\n",
            " 135501/150000: episode: 19738, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.200 [0.000, 8.000],  loss: 0.562859, mae: 14.163429, mean_q: 19.991219\n",
            " 135506/150000: episode: 19739, duration: 0.049s, episode steps:   5, steps per second: 102, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.200 [0.000, 7.000],  loss: 0.410688, mae: 14.126864, mean_q: 19.990683\n",
            " 135514/150000: episode: 19740, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.264855, mae: 14.163702, mean_q: 20.064871\n",
            " 135522/150000: episode: 19741, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.652632, mae: 14.107260, mean_q: 19.961555\n",
            " 135530/150000: episode: 19742, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.388948, mae: 13.899467, mean_q: 19.919716\n",
            " 135539/150000: episode: 19743, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.416601, mae: 14.041111, mean_q: 20.068077\n",
            " 135547/150000: episode: 19744, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.267892, mae: 14.059517, mean_q: 20.064396\n",
            " 135555/150000: episode: 19745, duration: 0.088s, episode steps:   8, steps per second:  90, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.479805, mae: 14.283109, mean_q: 20.072927\n",
            " 135562/150000: episode: 19746, duration: 0.084s, episode steps:   7, steps per second:  83, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.308842, mae: 13.964912, mean_q: 20.150705\n",
            " 135569/150000: episode: 19747, duration: 0.072s, episode steps:   7, steps per second:  97, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.714 [1.000, 8.000],  loss: 0.217726, mae: 13.728496, mean_q: 20.168653\n",
            " 135575/150000: episode: 19748, duration: 0.059s, episode steps:   6, steps per second: 101, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.000 [0.000, 8.000],  loss: 0.746364, mae: 13.757663, mean_q: 20.043215\n",
            " 135583/150000: episode: 19749, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.480257, mae: 13.736341, mean_q: 19.934380\n",
            " 135590/150000: episode: 19750, duration: 0.079s, episode steps:   7, steps per second:  88, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.259701, mae: 14.190982, mean_q: 20.236380\n",
            " 135593/150000: episode: 19751, duration: 0.050s, episode steps:   3, steps per second:  60, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 1.333 [1.000, 2.000],  loss: 0.908361, mae: 13.876637, mean_q: 20.115181\n",
            " 135597/150000: episode: 19752, duration: 0.067s, episode steps:   4, steps per second:  60, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.250 [4.000, 7.000],  loss: 0.487352, mae: 13.714634, mean_q: 20.037460\n",
            " 135601/150000: episode: 19753, duration: 0.075s, episode steps:   4, steps per second:  54, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.250 [0.000, 5.000],  loss: 0.329966, mae: 13.904197, mean_q: 20.219193\n",
            " 135610/150000: episode: 19754, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.774499, mae: 14.052975, mean_q: 20.106462\n",
            " 135615/150000: episode: 19755, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.800 [1.000, 7.000],  loss: 0.295340, mae: 13.796880, mean_q: 19.953470\n",
            " 135624/150000: episode: 19756, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.222 [0.000, 8.000],  loss: 0.462779, mae: 13.440091, mean_q: 20.087229\n",
            " 135632/150000: episode: 19757, duration: 0.117s, episode steps:   8, steps per second:  68, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.269702, mae: 13.725897, mean_q: 19.945808\n",
            " 135637/150000: episode: 19758, duration: 0.074s, episode steps:   5, steps per second:  67, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.000 [2.000, 7.000],  loss: 0.796504, mae: 14.183240, mean_q: 20.229351\n",
            " 135643/150000: episode: 19759, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.000 [0.000, 7.000],  loss: 0.553178, mae: 13.934850, mean_q: 19.855055\n",
            " 135651/150000: episode: 19760, duration: 0.121s, episode steps:   8, steps per second:  66, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.260357, mae: 14.193855, mean_q: 20.110031\n",
            " 135657/150000: episode: 19761, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.521496, mae: 14.155526, mean_q: 20.008673\n",
            " 135662/150000: episode: 19762, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.800 [1.000, 7.000],  loss: 0.280765, mae: 14.179781, mean_q: 20.144222\n",
            " 135667/150000: episode: 19763, duration: 0.071s, episode steps:   5, steps per second:  71, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.282643, mae: 13.662700, mean_q: 19.946411\n",
            " 135673/150000: episode: 19764, duration: 0.083s, episode steps:   6, steps per second:  73, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [0.000, 7.000],  loss: 0.969174, mae: 14.064256, mean_q: 19.962891\n",
            " 135681/150000: episode: 19765, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.294958, mae: 14.030877, mean_q: 19.993675\n",
            " 135688/150000: episode: 19766, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.294221, mae: 14.013370, mean_q: 20.125355\n",
            " 135696/150000: episode: 19767, duration: 0.115s, episode steps:   8, steps per second:  70, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.439595, mae: 14.186823, mean_q: 20.016026\n",
            " 135701/150000: episode: 19768, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 6.000 [3.000, 8.000],  loss: 0.311865, mae: 13.999677, mean_q: 20.096567\n",
            " 135709/150000: episode: 19769, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.573477, mae: 13.815209, mean_q: 20.014977\n",
            " 135717/150000: episode: 19770, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.286538, mae: 14.112985, mean_q: 20.004707\n",
            " 135726/150000: episode: 19771, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.424183, mae: 14.025149, mean_q: 19.835369\n",
            " 135733/150000: episode: 19772, duration: 0.099s, episode steps:   7, steps per second:  71, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.256044, mae: 14.060020, mean_q: 20.132250\n",
            " 135737/150000: episode: 19773, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.750 [3.000, 8.000],  loss: 0.398674, mae: 13.614958, mean_q: 19.841175\n",
            " 135746/150000: episode: 19774, duration: 0.144s, episode steps:   9, steps per second:  62, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.262406, mae: 13.747295, mean_q: 20.019430\n",
            " 135754/150000: episode: 19775, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.751398, mae: 14.048737, mean_q: 20.020287\n",
            " 135761/150000: episode: 19776, duration: 0.107s, episode steps:   7, steps per second:  65, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.328650, mae: 13.922437, mean_q: 20.069601\n",
            " 135767/150000: episode: 19777, duration: 0.121s, episode steps:   6, steps per second:  50, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.577219, mae: 13.811450, mean_q: 20.231552\n",
            " 135774/150000: episode: 19778, duration: 0.128s, episode steps:   7, steps per second:  55, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.414793, mae: 13.916629, mean_q: 20.054834\n",
            " 135778/150000: episode: 19779, duration: 0.073s, episode steps:   4, steps per second:  55, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.000 [2.000, 7.000],  loss: 0.362507, mae: 13.967667, mean_q: 20.159483\n",
            " 135786/150000: episode: 19780, duration: 0.125s, episode steps:   8, steps per second:  64, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.204102, mae: 14.274266, mean_q: 20.321470\n",
            " 135793/150000: episode: 19781, duration: 0.123s, episode steps:   7, steps per second:  57, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.370449, mae: 13.862513, mean_q: 19.910337\n",
            " 135799/150000: episode: 19782, duration: 0.084s, episode steps:   6, steps per second:  72, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [1.000, 7.000],  loss: 0.754592, mae: 13.587092, mean_q: 20.216047\n",
            " 135807/150000: episode: 19783, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.433985, mae: 13.830287, mean_q: 19.842472\n",
            " 135816/150000: episode: 19784, duration: 0.154s, episode steps:   9, steps per second:  59, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.435626, mae: 14.145885, mean_q: 20.483425\n",
            " 135821/150000: episode: 19785, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.000 [2.000, 7.000],  loss: 0.657459, mae: 14.094919, mean_q: 19.770733\n",
            " 135826/150000: episode: 19786, duration: 0.071s, episode steps:   5, steps per second:  71, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.600 [1.000, 7.000],  loss: 0.250820, mae: 14.062137, mean_q: 19.911243\n",
            " 135833/150000: episode: 19787, duration: 0.128s, episode steps:   7, steps per second:  55, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.857 [0.000, 8.000],  loss: 0.400614, mae: 14.066378, mean_q: 20.421055\n",
            " 135842/150000: episode: 19788, duration: 0.132s, episode steps:   9, steps per second:  68, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.491568, mae: 13.801777, mean_q: 19.979692\n",
            " 135851/150000: episode: 19789, duration: 0.148s, episode steps:   9, steps per second:  61, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.412871, mae: 13.935942, mean_q: 20.142672\n",
            " 135859/150000: episode: 19790, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.250 [0.000, 8.000],  loss: 0.612054, mae: 13.981755, mean_q: 20.084873\n",
            " 135867/150000: episode: 19791, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.205038, mae: 13.963287, mean_q: 20.155912\n",
            " 135876/150000: episode: 19792, duration: 0.094s, episode steps:   9, steps per second:  95, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.281023, mae: 13.663973, mean_q: 19.987967\n",
            " 135885/150000: episode: 19793, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.357418, mae: 13.787007, mean_q: 20.015419\n",
            " 135894/150000: episode: 19794, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.665008, mae: 13.763521, mean_q: 19.880493\n",
            " 135900/150000: episode: 19795, duration: 0.071s, episode steps:   6, steps per second:  84, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.000 [0.000, 8.000],  loss: 0.363235, mae: 13.926833, mean_q: 19.737692\n",
            " 135906/150000: episode: 19796, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.167 [2.000, 7.000],  loss: 0.570125, mae: 13.827128, mean_q: 20.055069\n",
            " 135914/150000: episode: 19797, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.875 [0.000, 8.000],  loss: 0.241708, mae: 13.931889, mean_q: 19.946192\n",
            " 135919/150000: episode: 19798, duration: 0.054s, episode steps:   5, steps per second:  92, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.600 [0.000, 8.000],  loss: 0.255625, mae: 14.176134, mean_q: 20.099503\n",
            " 135923/150000: episode: 19799, duration: 0.061s, episode steps:   4, steps per second:  65, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.500 [0.000, 8.000],  loss: 0.212697, mae: 13.937032, mean_q: 19.860439\n",
            " 135926/150000: episode: 19800, duration: 0.036s, episode steps:   3, steps per second:  84, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 5.333 [2.000, 7.000],  loss: 0.381232, mae: 13.911887, mean_q: 20.066841\n",
            " 135931/150000: episode: 19801, duration: 0.049s, episode steps:   5, steps per second: 102, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.000 [2.000, 7.000],  loss: 0.423451, mae: 13.703799, mean_q: 19.989811\n",
            " 135936/150000: episode: 19802, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [2.000, 6.000],  loss: 0.301465, mae: 13.863592, mean_q: 19.893103\n",
            " 135938/150000: episode: 19803, duration: 0.026s, episode steps:   2, steps per second:  78, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 5.000 [5.000, 5.000],  loss: 0.339398, mae: 13.804567, mean_q: 20.094242\n",
            " 135944/150000: episode: 19804, duration: 0.069s, episode steps:   6, steps per second:  87, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 8.000],  loss: 0.266865, mae: 13.778839, mean_q: 20.100779\n",
            " 135953/150000: episode: 19805, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.194367, mae: 14.107554, mean_q: 19.995735\n",
            " 135961/150000: episode: 19806, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.533131, mae: 13.816804, mean_q: 20.071554\n",
            " 135965/150000: episode: 19807, duration: 0.048s, episode steps:   4, steps per second:  83, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.250 [0.000, 5.000],  loss: 0.224496, mae: 13.832571, mean_q: 20.070879\n",
            " 135973/150000: episode: 19808, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.250 [0.000, 8.000],  loss: 0.442583, mae: 13.712845, mean_q: 19.971502\n",
            " 135980/150000: episode: 19809, duration: 0.071s, episode steps:   7, steps per second:  99, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.000 [0.000, 8.000],  loss: 0.208943, mae: 13.972980, mean_q: 20.118586\n",
            " 135985/150000: episode: 19810, duration: 0.054s, episode steps:   5, steps per second:  93, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.328736, mae: 14.034528, mean_q: 19.970692\n",
            " 135994/150000: episode: 19811, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.378143, mae: 13.772640, mean_q: 19.966091\n",
            " 136003/150000: episode: 19812, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.556 [0.000, 8.000],  loss: 0.950087, mae: 13.879986, mean_q: 20.061741\n",
            " 136008/150000: episode: 19813, duration: 0.062s, episode steps:   5, steps per second:  81, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.363364, mae: 14.010653, mean_q: 20.117569\n",
            " 136017/150000: episode: 19814, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.301749, mae: 14.176394, mean_q: 20.083050\n",
            " 136022/150000: episode: 19815, duration: 0.060s, episode steps:   5, steps per second:  83, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [1.000, 8.000],  loss: 0.181894, mae: 14.192961, mean_q: 20.111019\n",
            " 136029/150000: episode: 19816, duration: 0.073s, episode steps:   7, steps per second:  96, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.429 [0.000, 7.000],  loss: 0.203289, mae: 13.699979, mean_q: 20.064068\n",
            " 136037/150000: episode: 19817, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.458101, mae: 13.847553, mean_q: 20.000538\n",
            " 136039/150000: episode: 19818, duration: 0.027s, episode steps:   2, steps per second:  75, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.473124, mae: 13.906522, mean_q: 20.034946\n",
            " 136045/150000: episode: 19819, duration: 0.060s, episode steps:   6, steps per second: 101, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.167 [2.000, 8.000],  loss: 0.469485, mae: 14.280133, mean_q: 20.110687\n",
            " 136050/150000: episode: 19820, duration: 0.064s, episode steps:   5, steps per second:  78, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [0.000, 8.000],  loss: 0.211043, mae: 13.725883, mean_q: 20.309864\n",
            " 136057/150000: episode: 19821, duration: 0.067s, episode steps:   7, steps per second: 105, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.571 [0.000, 8.000],  loss: 0.269703, mae: 13.613893, mean_q: 20.141541\n",
            " 136063/150000: episode: 19822, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [2.000, 7.000],  loss: 0.281471, mae: 13.854542, mean_q: 19.998478\n",
            " 136068/150000: episode: 19823, duration: 0.052s, episode steps:   5, steps per second:  97, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.358716, mae: 13.748624, mean_q: 20.076017\n",
            " 136076/150000: episode: 19824, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.268296, mae: 14.226439, mean_q: 20.304049\n",
            " 136083/150000: episode: 19825, duration: 0.067s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.362992, mae: 13.817180, mean_q: 20.008955\n",
            " 136089/150000: episode: 19826, duration: 0.059s, episode steps:   6, steps per second: 101, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.221957, mae: 13.804715, mean_q: 20.066097\n",
            " 136097/150000: episode: 19827, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.265443, mae: 13.725916, mean_q: 19.990406\n",
            " 136104/150000: episode: 19828, duration: 0.076s, episode steps:   7, steps per second:  92, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 7.000],  loss: 0.363586, mae: 13.972302, mean_q: 19.974035\n",
            " 136113/150000: episode: 19829, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.297764, mae: 13.972214, mean_q: 20.056887\n",
            " 136121/150000: episode: 19830, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.493097, mae: 13.925158, mean_q: 19.895397\n",
            " 136126/150000: episode: 19831, duration: 0.054s, episode steps:   5, steps per second:  93, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.800 [1.000, 7.000],  loss: 0.227951, mae: 14.188901, mean_q: 20.059000\n",
            " 136132/150000: episode: 19832, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.349219, mae: 13.703629, mean_q: 20.154734\n",
            " 136140/150000: episode: 19833, duration: 0.077s, episode steps:   8, steps per second: 103, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.289966, mae: 13.906057, mean_q: 20.036417\n",
            " 136146/150000: episode: 19834, duration: 0.073s, episode steps:   6, steps per second:  82, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [1.000, 7.000],  loss: 0.401778, mae: 13.730672, mean_q: 19.921728\n",
            " 136151/150000: episode: 19835, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.200 [1.000, 7.000],  loss: 0.865901, mae: 13.777310, mean_q: 19.853638\n",
            " 136160/150000: episode: 19836, duration: 0.082s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.498593, mae: 13.924229, mean_q: 20.397362\n",
            " 136169/150000: episode: 19837, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.420438, mae: 13.863415, mean_q: 20.021124\n",
            " 136171/150000: episode: 19838, duration: 0.026s, episode steps:   2, steps per second:  75, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 7.000 [7.000, 7.000],  loss: 0.318489, mae: 13.933168, mean_q: 20.179897\n",
            " 136179/150000: episode: 19839, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.288917, mae: 14.056240, mean_q: 20.065529\n",
            " 136188/150000: episode: 19840, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.271610, mae: 13.897678, mean_q: 20.116476\n",
            " 136196/150000: episode: 19841, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.264567, mae: 13.758980, mean_q: 20.118279\n",
            " 136203/150000: episode: 19842, duration: 0.068s, episode steps:   7, steps per second: 103, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.285925, mae: 14.258005, mean_q: 20.004145\n",
            " 136211/150000: episode: 19843, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.571684, mae: 14.089877, mean_q: 19.938276\n",
            " 136220/150000: episode: 19844, duration: 0.104s, episode steps:   9, steps per second:  86, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.347382, mae: 14.028633, mean_q: 20.193285\n",
            " 136225/150000: episode: 19845, duration: 0.054s, episode steps:   5, steps per second:  92, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.000 [1.000, 7.000],  loss: 0.331194, mae: 13.755903, mean_q: 19.816227\n",
            " 136233/150000: episode: 19846, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.561602, mae: 13.768260, mean_q: 19.872078\n",
            " 136241/150000: episode: 19847, duration: 0.080s, episode steps:   8, steps per second: 101, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.375 [0.000, 8.000],  loss: 0.515755, mae: 13.661255, mean_q: 20.084236\n",
            " 136248/150000: episode: 19848, duration: 0.077s, episode steps:   7, steps per second:  91, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.414417, mae: 13.960885, mean_q: 19.951443\n",
            " 136257/150000: episode: 19849, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.438945, mae: 13.671336, mean_q: 20.053242\n",
            " 136264/150000: episode: 19850, duration: 0.084s, episode steps:   7, steps per second:  83, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.241354, mae: 14.012857, mean_q: 19.919939\n",
            " 136271/150000: episode: 19851, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.388837, mae: 13.949195, mean_q: 20.130280\n",
            " 136279/150000: episode: 19852, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.875 [0.000, 7.000],  loss: 0.252107, mae: 13.888062, mean_q: 20.043627\n",
            " 136288/150000: episode: 19853, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.319848, mae: 13.755844, mean_q: 20.043039\n",
            " 136297/150000: episode: 19854, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.496482, mae: 13.811813, mean_q: 19.894890\n",
            " 136303/150000: episode: 19855, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.333 [2.000, 8.000],  loss: 0.442404, mae: 13.940349, mean_q: 19.809349\n",
            " 136312/150000: episode: 19856, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.229461, mae: 13.936658, mean_q: 20.138462\n",
            " 136318/150000: episode: 19857, duration: 0.057s, episode steps:   6, steps per second: 106, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.500 [0.000, 7.000],  loss: 0.339838, mae: 13.598577, mean_q: 19.787184\n",
            " 136324/150000: episode: 19858, duration: 0.070s, episode steps:   6, steps per second:  86, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [2.000, 8.000],  loss: 0.213479, mae: 13.883229, mean_q: 20.134733\n",
            " 136333/150000: episode: 19859, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.474983, mae: 13.674389, mean_q: 19.955986\n",
            " 136341/150000: episode: 19860, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.875 [0.000, 8.000],  loss: 0.275125, mae: 13.834880, mean_q: 20.026817\n",
            " 136350/150000: episode: 19861, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.212779, mae: 13.867989, mean_q: 20.146492\n",
            " 136356/150000: episode: 19862, duration: 0.070s, episode steps:   6, steps per second:  86, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 6.000],  loss: 0.207105, mae: 13.959927, mean_q: 20.049238\n",
            " 136362/150000: episode: 19863, duration: 0.064s, episode steps:   6, steps per second:  93, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.833 [0.000, 8.000],  loss: 0.575996, mae: 13.680930, mean_q: 20.065613\n",
            " 136368/150000: episode: 19864, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2.667 [0.000, 6.000],  loss: 0.468636, mae: 13.711090, mean_q: 19.799744\n",
            " 136376/150000: episode: 19865, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.250 [1.000, 8.000],  loss: 0.274631, mae: 14.106416, mean_q: 20.066917\n",
            " 136384/150000: episode: 19866, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.166659, mae: 13.806800, mean_q: 19.971458\n",
            " 136391/150000: episode: 19867, duration: 0.067s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.285912, mae: 13.806140, mean_q: 20.047329\n",
            " 136396/150000: episode: 19868, duration: 0.051s, episode steps:   5, steps per second:  98, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.000 [1.000, 8.000],  loss: 0.271608, mae: 13.694140, mean_q: 19.916325\n",
            " 136402/150000: episode: 19869, duration: 0.062s, episode steps:   6, steps per second:  97, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.667 [1.000, 7.000],  loss: 0.289437, mae: 14.002441, mean_q: 19.903376\n",
            " 136410/150000: episode: 19870, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.625 [0.000, 8.000],  loss: 0.657832, mae: 14.000021, mean_q: 20.016134\n",
            " 136415/150000: episode: 19871, duration: 0.049s, episode steps:   5, steps per second: 102, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.800 [1.000, 7.000],  loss: 0.477780, mae: 14.103468, mean_q: 19.928427\n",
            " 136421/150000: episode: 19872, duration: 0.068s, episode steps:   6, steps per second:  88, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.833 [0.000, 8.000],  loss: 0.656225, mae: 14.056081, mean_q: 20.094667\n",
            " 136428/150000: episode: 19873, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.371064, mae: 13.659197, mean_q: 19.928423\n",
            " 136434/150000: episode: 19874, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 8.000],  loss: 0.329328, mae: 13.748662, mean_q: 20.045458\n",
            " 136441/150000: episode: 19875, duration: 0.066s, episode steps:   7, steps per second: 107, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.000 [1.000, 8.000],  loss: 0.503145, mae: 13.760756, mean_q: 20.041494\n",
            " 136447/150000: episode: 19876, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [1.000, 8.000],  loss: 0.563872, mae: 13.823730, mean_q: 19.990286\n",
            " 136452/150000: episode: 19877, duration: 0.051s, episode steps:   5, steps per second:  99, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.322220, mae: 13.866110, mean_q: 20.078520\n",
            " 136457/150000: episode: 19878, duration: 0.071s, episode steps:   5, steps per second:  71, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.800 [4.000, 7.000],  loss: 0.987806, mae: 13.811523, mean_q: 19.720989\n",
            " 136466/150000: episode: 19879, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.484558, mae: 13.962616, mean_q: 20.010838\n",
            " 136472/150000: episode: 19880, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.506214, mae: 13.812973, mean_q: 20.119171\n",
            " 136480/150000: episode: 19881, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.494562, mae: 14.145843, mean_q: 19.975893\n",
            " 136487/150000: episode: 19882, duration: 0.070s, episode steps:   7, steps per second: 100, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.313003, mae: 13.966542, mean_q: 20.202765\n",
            " 136496/150000: episode: 19883, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.447952, mae: 14.019740, mean_q: 19.969685\n",
            " 136503/150000: episode: 19884, duration: 0.077s, episode steps:   7, steps per second:  90, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.298845, mae: 13.955115, mean_q: 20.204411\n",
            " 136511/150000: episode: 19885, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.247830, mae: 13.959924, mean_q: 20.138283\n",
            " 136516/150000: episode: 19886, duration: 0.051s, episode steps:   5, steps per second:  99, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.400 [0.000, 5.000],  loss: 0.801445, mae: 13.954906, mean_q: 19.931110\n",
            " 136521/150000: episode: 19887, duration: 0.067s, episode steps:   5, steps per second:  75, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 6.400 [3.000, 8.000],  loss: 0.811862, mae: 13.804985, mean_q: 19.788921\n",
            " 136523/150000: episode: 19888, duration: 0.038s, episode steps:   2, steps per second:  52, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 3.000 [3.000, 3.000],  loss: 0.182364, mae: 14.003123, mean_q: 20.017700\n",
            " 136526/150000: episode: 19889, duration: 0.034s, episode steps:   3, steps per second:  89, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 3.000 [2.000, 5.000],  loss: 0.265925, mae: 13.794743, mean_q: 20.225332\n",
            " 136535/150000: episode: 19890, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.222 [0.000, 8.000],  loss: 0.376239, mae: 13.642627, mean_q: 20.155462\n",
            " 136542/150000: episode: 19891, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.262672, mae: 13.792551, mean_q: 20.017668\n",
            " 136550/150000: episode: 19892, duration: 0.102s, episode steps:   8, steps per second:  79, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.206764, mae: 13.808929, mean_q: 20.136457\n",
            " 136556/150000: episode: 19893, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [1.000, 7.000],  loss: 0.321698, mae: 13.660048, mean_q: 20.041746\n",
            " 136563/150000: episode: 19894, duration: 0.063s, episode steps:   7, steps per second: 111, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [0.000, 8.000],  loss: 0.201893, mae: 14.331903, mean_q: 20.204874\n",
            " 136568/150000: episode: 19895, duration: 0.049s, episode steps:   5, steps per second: 103, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.200 [1.000, 8.000],  loss: 0.611374, mae: 13.861165, mean_q: 20.050365\n",
            " 136575/150000: episode: 19896, duration: 0.078s, episode steps:   7, steps per second:  90, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 2.857 [0.000, 7.000],  loss: 0.358911, mae: 14.083300, mean_q: 20.034271\n",
            " 136582/150000: episode: 19897, duration: 0.065s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.220566, mae: 14.056149, mean_q: 20.269958\n",
            " 136590/150000: episode: 19898, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.266762, mae: 14.033286, mean_q: 19.978722\n",
            " 136596/150000: episode: 19899, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.364773, mae: 13.979760, mean_q: 20.023159\n",
            " 136603/150000: episode: 19900, duration: 0.079s, episode steps:   7, steps per second:  88, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.294623, mae: 13.959664, mean_q: 20.010054\n",
            " 136609/150000: episode: 19901, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2.833 [0.000, 6.000],  loss: 0.174063, mae: 14.057763, mean_q: 20.102167\n",
            " 136616/150000: episode: 19902, duration: 0.072s, episode steps:   7, steps per second:  98, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.386752, mae: 13.630435, mean_q: 19.896986\n",
            " 136624/150000: episode: 19903, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.254379, mae: 14.050165, mean_q: 20.170444\n",
            " 136629/150000: episode: 19904, duration: 0.051s, episode steps:   5, steps per second:  99, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 8.000],  loss: 0.526126, mae: 13.538218, mean_q: 20.023518\n",
            " 136634/150000: episode: 19905, duration: 0.054s, episode steps:   5, steps per second:  93, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.800 [0.000, 5.000],  loss: 0.267059, mae: 14.040464, mean_q: 19.941418\n",
            " 136641/150000: episode: 19906, duration: 0.068s, episode steps:   7, steps per second: 103, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.575475, mae: 14.268315, mean_q: 20.230276\n",
            " 136649/150000: episode: 19907, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.519596, mae: 13.817817, mean_q: 19.830263\n",
            " 136658/150000: episode: 19908, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.451573, mae: 13.888326, mean_q: 20.155588\n",
            " 136666/150000: episode: 19909, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.250 [0.000, 8.000],  loss: 0.428398, mae: 13.781668, mean_q: 19.929169\n",
            " 136675/150000: episode: 19910, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.359467, mae: 14.034074, mean_q: 20.146444\n",
            " 136682/150000: episode: 19911, duration: 0.067s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.458649, mae: 13.903773, mean_q: 20.054064\n",
            " 136691/150000: episode: 19912, duration: 0.088s, episode steps:   9, steps per second: 103, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.283817, mae: 13.982990, mean_q: 20.001043\n",
            " 136699/150000: episode: 19913, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.291608, mae: 13.546741, mean_q: 20.241158\n",
            " 136708/150000: episode: 19914, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.666724, mae: 14.052528, mean_q: 19.876028\n",
            " 136715/150000: episode: 19915, duration: 0.069s, episode steps:   7, steps per second: 101, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [0.000, 8.000],  loss: 0.354811, mae: 13.869637, mean_q: 20.104155\n",
            " 136723/150000: episode: 19916, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.205946, mae: 14.063822, mean_q: 20.081932\n",
            " 136732/150000: episode: 19917, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.437389, mae: 13.866802, mean_q: 19.953449\n",
            " 136740/150000: episode: 19918, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.375 [1.000, 8.000],  loss: 0.328106, mae: 14.094434, mean_q: 20.065659\n",
            " 136744/150000: episode: 19919, duration: 0.046s, episode steps:   4, steps per second:  86, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.250 [0.000, 5.000],  loss: 0.561829, mae: 13.793421, mean_q: 19.951397\n",
            " 136751/150000: episode: 19920, duration: 0.083s, episode steps:   7, steps per second:  85, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.143 [0.000, 8.000],  loss: 0.599047, mae: 13.694705, mean_q: 20.203497\n",
            " 136758/150000: episode: 19921, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [1.000, 8.000],  loss: 0.414028, mae: 13.880691, mean_q: 19.954834\n",
            " 136765/150000: episode: 19922, duration: 0.068s, episode steps:   7, steps per second: 104, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.857 [2.000, 8.000],  loss: 0.869089, mae: 13.866068, mean_q: 20.014004\n",
            " 136773/150000: episode: 19923, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.309033, mae: 13.896169, mean_q: 20.037766\n",
            " 136780/150000: episode: 19924, duration: 0.076s, episode steps:   7, steps per second:  93, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.714 [0.000, 7.000],  loss: 0.242278, mae: 14.016212, mean_q: 20.182718\n",
            " 136786/150000: episode: 19925, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [1.000, 8.000],  loss: 0.613886, mae: 14.203812, mean_q: 19.925432\n",
            " 136791/150000: episode: 19926, duration: 0.052s, episode steps:   5, steps per second:  96, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [1.000, 8.000],  loss: 0.238426, mae: 13.704226, mean_q: 20.082050\n",
            " 136798/150000: episode: 19927, duration: 0.078s, episode steps:   7, steps per second:  90, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.416624, mae: 13.814839, mean_q: 20.151838\n",
            " 136804/150000: episode: 19928, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [0.000, 7.000],  loss: 0.269775, mae: 13.703698, mean_q: 19.977488\n",
            " 136811/150000: episode: 19929, duration: 0.074s, episode steps:   7, steps per second:  95, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 7.000],  loss: 0.474439, mae: 13.745171, mean_q: 19.978537\n",
            " 136819/150000: episode: 19930, duration: 0.116s, episode steps:   8, steps per second:  69, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.616625, mae: 13.990726, mean_q: 20.130316\n",
            " 136824/150000: episode: 19931, duration: 0.073s, episode steps:   5, steps per second:  68, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.200 [0.000, 7.000],  loss: 0.461375, mae: 13.493246, mean_q: 19.976593\n",
            " 136830/150000: episode: 19932, duration: 0.107s, episode steps:   6, steps per second:  56, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.209672, mae: 14.341922, mean_q: 20.216551\n",
            " 136833/150000: episode: 19933, duration: 0.060s, episode steps:   3, steps per second:  50, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 2.333 [1.000, 5.000],  loss: 0.245218, mae: 13.487521, mean_q: 20.172232\n",
            " 136838/150000: episode: 19934, duration: 0.076s, episode steps:   5, steps per second:  66, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [1.000, 8.000],  loss: 0.362802, mae: 13.891197, mean_q: 20.051027\n",
            " 136845/150000: episode: 19935, duration: 0.101s, episode steps:   7, steps per second:  69, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.303246, mae: 13.868295, mean_q: 19.956120\n",
            " 136853/150000: episode: 19936, duration: 0.116s, episode steps:   8, steps per second:  69, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.189508, mae: 14.046182, mean_q: 20.148382\n",
            " 136862/150000: episode: 19937, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.239480, mae: 13.860361, mean_q: 20.073006\n",
            " 136865/150000: episode: 19938, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.623786, mae: 13.404109, mean_q: 20.003857\n",
            " 136867/150000: episode: 19939, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.170914, mae: 14.032928, mean_q: 20.025297\n",
            " 136872/150000: episode: 19940, duration: 0.098s, episode steps:   5, steps per second:  51, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.278245, mae: 13.619863, mean_q: 19.877451\n",
            " 136881/150000: episode: 19941, duration: 0.124s, episode steps:   9, steps per second:  73, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.111 [0.000, 8.000],  loss: 0.172747, mae: 13.956297, mean_q: 19.980904\n",
            " 136889/150000: episode: 19942, duration: 0.136s, episode steps:   8, steps per second:  59, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.250 [0.000, 8.000],  loss: 0.496697, mae: 13.867962, mean_q: 20.058258\n",
            " 136897/150000: episode: 19943, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.591850, mae: 13.759517, mean_q: 19.801556\n",
            " 136902/150000: episode: 19944, duration: 0.067s, episode steps:   5, steps per second:  74, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.474287, mae: 13.666372, mean_q: 20.104206\n",
            " 136908/150000: episode: 19945, duration: 0.104s, episode steps:   6, steps per second:  58, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.500 [0.000, 6.000],  loss: 0.310368, mae: 14.083554, mean_q: 19.985098\n",
            " 136915/150000: episode: 19946, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.714 [0.000, 8.000],  loss: 0.339576, mae: 13.980153, mean_q: 19.986238\n",
            " 136922/150000: episode: 19947, duration: 0.094s, episode steps:   7, steps per second:  75, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [0.000, 8.000],  loss: 0.205629, mae: 14.156061, mean_q: 19.898579\n",
            " 136929/150000: episode: 19948, duration: 0.105s, episode steps:   7, steps per second:  67, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.226688, mae: 13.665414, mean_q: 20.177288\n",
            " 136937/150000: episode: 19949, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.387477, mae: 14.022247, mean_q: 19.899874\n",
            " 136944/150000: episode: 19950, duration: 0.090s, episode steps:   7, steps per second:  77, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.186424, mae: 13.715510, mean_q: 20.064358\n",
            " 136949/150000: episode: 19951, duration: 0.083s, episode steps:   5, steps per second:  60, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.200 [0.000, 7.000],  loss: 0.353561, mae: 14.095790, mean_q: 20.096455\n",
            " 136955/150000: episode: 19952, duration: 0.099s, episode steps:   6, steps per second:  60, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.389744, mae: 13.591189, mean_q: 19.866644\n",
            " 136960/150000: episode: 19953, duration: 0.074s, episode steps:   5, steps per second:  68, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.800 [0.000, 5.000],  loss: 0.527850, mae: 13.904551, mean_q: 19.881241\n",
            " 136967/150000: episode: 19954, duration: 0.112s, episode steps:   7, steps per second:  63, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 2.429 [0.000, 6.000],  loss: 0.634647, mae: 14.138598, mean_q: 19.998016\n",
            " 136975/150000: episode: 19955, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.290267, mae: 14.087897, mean_q: 19.920343\n",
            " 136978/150000: episode: 19956, duration: 0.047s, episode steps:   3, steps per second:  63, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 2.333 [2.000, 3.000],  loss: 0.185017, mae: 14.131642, mean_q: 20.120150\n",
            " 136984/150000: episode: 19957, duration: 0.085s, episode steps:   6, steps per second:  71, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 2.333 [0.000, 5.000],  loss: 0.505257, mae: 14.163105, mean_q: 20.182356\n",
            " 136991/150000: episode: 19958, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.923784, mae: 13.893564, mean_q: 20.051832\n",
            " 136998/150000: episode: 19959, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.603364, mae: 13.896846, mean_q: 19.971796\n",
            " 137007/150000: episode: 19960, duration: 0.124s, episode steps:   9, steps per second:  73, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.773681, mae: 13.988987, mean_q: 20.276329\n",
            " 137015/150000: episode: 19961, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.125 [0.000, 8.000],  loss: 1.077585, mae: 13.981389, mean_q: 19.902414\n",
            " 137024/150000: episode: 19962, duration: 0.130s, episode steps:   9, steps per second:  69, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.298366, mae: 14.177094, mean_q: 20.359232\n",
            " 137030/150000: episode: 19963, duration: 0.105s, episode steps:   6, steps per second:  57, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [0.000, 7.000],  loss: 0.290162, mae: 13.804588, mean_q: 20.019875\n",
            " 137037/150000: episode: 19964, duration: 0.098s, episode steps:   7, steps per second:  72, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 7.000],  loss: 0.432156, mae: 13.955586, mean_q: 20.049129\n",
            " 137046/150000: episode: 19965, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.249817, mae: 14.036940, mean_q: 20.146126\n",
            " 137053/150000: episode: 19966, duration: 0.107s, episode steps:   7, steps per second:  66, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 5.286 [1.000, 8.000],  loss: 0.353580, mae: 13.891251, mean_q: 20.192566\n",
            " 137058/150000: episode: 19967, duration: 0.084s, episode steps:   5, steps per second:  60, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.200 [2.000, 8.000],  loss: 0.298593, mae: 13.636647, mean_q: 19.808666\n",
            " 137065/150000: episode: 19968, duration: 0.067s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.338748, mae: 13.950036, mean_q: 20.252890\n",
            " 137072/150000: episode: 19969, duration: 0.075s, episode steps:   7, steps per second:  93, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.281484, mae: 13.813309, mean_q: 20.005198\n",
            " 137079/150000: episode: 19970, duration: 0.073s, episode steps:   7, steps per second:  97, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.269947, mae: 13.986682, mean_q: 20.103058\n",
            " 137086/150000: episode: 19971, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.194846, mae: 14.144134, mean_q: 20.155352\n",
            " 137091/150000: episode: 19972, duration: 0.052s, episode steps:   5, steps per second:  96, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.600 [0.000, 6.000],  loss: 0.163753, mae: 14.010010, mean_q: 19.944820\n",
            " 137095/150000: episode: 19973, duration: 0.051s, episode steps:   4, steps per second:  79, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 2.000 [0.000, 4.000],  loss: 0.215778, mae: 13.601473, mean_q: 20.100986\n",
            " 137101/150000: episode: 19974, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [1.000, 8.000],  loss: 0.220355, mae: 13.824025, mean_q: 20.117567\n",
            " 137109/150000: episode: 19975, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.255760, mae: 13.931120, mean_q: 20.051258\n",
            " 137116/150000: episode: 19976, duration: 0.101s, episode steps:   7, steps per second:  69, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.459281, mae: 13.808847, mean_q: 20.012775\n",
            " 137123/150000: episode: 19977, duration: 0.075s, episode steps:   7, steps per second:  93, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.665463, mae: 13.955956, mean_q: 19.840633\n",
            " 137128/150000: episode: 19978, duration: 0.051s, episode steps:   5, steps per second:  99, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.800 [0.000, 8.000],  loss: 0.449806, mae: 13.993975, mean_q: 19.919672\n",
            " 137135/150000: episode: 19979, duration: 0.066s, episode steps:   7, steps per second: 105, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.000 [0.000, 7.000],  loss: 0.293657, mae: 13.949715, mean_q: 20.302612\n",
            " 137141/150000: episode: 19980, duration: 0.071s, episode steps:   6, steps per second:  85, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [1.000, 7.000],  loss: 0.263966, mae: 13.818607, mean_q: 19.904158\n",
            " 137150/150000: episode: 19981, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.329838, mae: 14.025648, mean_q: 20.167336\n",
            " 137157/150000: episode: 19982, duration: 0.065s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.396724, mae: 13.919563, mean_q: 19.920115\n",
            " 137166/150000: episode: 19983, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.327604, mae: 14.052023, mean_q: 20.076910\n",
            " 137169/150000: episode: 19984, duration: 0.032s, episode steps:   3, steps per second:  92, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 4.000 [2.000, 8.000],  loss: 0.249240, mae: 14.040502, mean_q: 20.169455\n",
            " 137176/150000: episode: 19985, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.670689, mae: 13.720759, mean_q: 20.048079\n",
            " 137182/150000: episode: 19986, duration: 0.062s, episode steps:   6, steps per second:  97, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [2.000, 7.000],  loss: 0.479418, mae: 13.861983, mean_q: 20.003036\n",
            " 137191/150000: episode: 19987, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.330536, mae: 13.781154, mean_q: 20.063446\n",
            " 137196/150000: episode: 19988, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.200 [2.000, 7.000],  loss: 0.673577, mae: 14.299417, mean_q: 20.192150\n",
            " 137202/150000: episode: 19989, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.253218, mae: 14.083347, mean_q: 20.221865\n",
            " 137209/150000: episode: 19990, duration: 0.066s, episode steps:   7, steps per second: 107, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.429 [1.000, 8.000],  loss: 0.294328, mae: 14.054046, mean_q: 20.090246\n",
            " 137218/150000: episode: 19991, duration: 0.124s, episode steps:   9, steps per second:  73, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.447925, mae: 13.753262, mean_q: 20.113926\n",
            " 137226/150000: episode: 19992, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.331286, mae: 13.977783, mean_q: 19.851217\n",
            " 137232/150000: episode: 19993, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.667 [2.000, 7.000],  loss: 0.203907, mae: 14.209759, mean_q: 20.131599\n",
            " 137239/150000: episode: 19994, duration: 0.067s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.143 [0.000, 7.000],  loss: 0.248175, mae: 14.496394, mean_q: 20.049421\n",
            " 137247/150000: episode: 19995, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.294377, mae: 13.912937, mean_q: 20.096268\n",
            " 137254/150000: episode: 19996, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.199662, mae: 13.771731, mean_q: 19.921677\n",
            " 137261/150000: episode: 19997, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.204387, mae: 13.736414, mean_q: 20.065281\n",
            " 137269/150000: episode: 19998, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.231112, mae: 13.956692, mean_q: 20.014017\n",
            " 137277/150000: episode: 19999, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.000 [0.000, 7.000],  loss: 0.179113, mae: 13.960154, mean_q: 20.012060\n",
            " 137283/150000: episode: 20000, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [2.000, 8.000],  loss: 0.224488, mae: 13.776412, mean_q: 20.040186\n",
            " 137289/150000: episode: 20001, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [1.000, 8.000],  loss: 0.184848, mae: 13.921203, mean_q: 20.044914\n",
            " 137298/150000: episode: 20002, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.295719, mae: 13.604420, mean_q: 20.089266\n",
            " 137306/150000: episode: 20003, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.180936, mae: 14.166847, mean_q: 20.004211\n",
            " 137312/150000: episode: 20004, duration: 0.066s, episode steps:   6, steps per second:  91, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.167 [2.000, 8.000],  loss: 0.199660, mae: 14.129285, mean_q: 20.070147\n",
            " 137321/150000: episode: 20005, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.172593, mae: 14.007221, mean_q: 20.178129\n",
            " 137330/150000: episode: 20006, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.667 [0.000, 8.000],  loss: 0.185688, mae: 13.690907, mean_q: 20.041237\n",
            " 137338/150000: episode: 20007, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.156009, mae: 14.305097, mean_q: 20.171158\n",
            " 137344/150000: episode: 20008, duration: 0.064s, episode steps:   6, steps per second:  93, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.539385, mae: 13.929555, mean_q: 19.945654\n",
            " 137349/150000: episode: 20009, duration: 0.055s, episode steps:   5, steps per second:  90, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.432607, mae: 13.987704, mean_q: 19.966135\n",
            " 137357/150000: episode: 20010, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.340719, mae: 13.929392, mean_q: 20.144657\n",
            " 137363/150000: episode: 20011, duration: 0.084s, episode steps:   6, steps per second:  71, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [2.000, 7.000],  loss: 0.438967, mae: 13.985614, mean_q: 20.008224\n",
            " 137370/150000: episode: 20012, duration: 0.073s, episode steps:   7, steps per second:  96, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.308081, mae: 14.004339, mean_q: 19.890686\n",
            " 137377/150000: episode: 20013, duration: 0.065s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.377588, mae: 13.624639, mean_q: 20.105162\n",
            " 137383/150000: episode: 20014, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.667 [0.000, 8.000],  loss: 0.285787, mae: 14.046765, mean_q: 19.910872\n",
            " 137392/150000: episode: 20015, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.283298, mae: 13.984943, mean_q: 20.253733\n",
            " 137397/150000: episode: 20016, duration: 0.049s, episode steps:   5, steps per second: 102, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 8.000],  loss: 0.219027, mae: 14.029001, mean_q: 20.056347\n",
            " 137402/150000: episode: 20017, duration: 0.047s, episode steps:   5, steps per second: 106, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.312591, mae: 13.985843, mean_q: 20.128408\n",
            " 137408/150000: episode: 20018, duration: 0.063s, episode steps:   6, steps per second:  96, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [1.000, 8.000],  loss: 0.195128, mae: 14.088740, mean_q: 20.002953\n",
            " 137415/150000: episode: 20019, duration: 0.081s, episode steps:   7, steps per second:  86, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.318142, mae: 14.176751, mean_q: 19.914989\n",
            " 137420/150000: episode: 20020, duration: 0.059s, episode steps:   5, steps per second:  84, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.400 [1.000, 7.000],  loss: 0.323667, mae: 14.169116, mean_q: 20.130976\n",
            " 137427/150000: episode: 20021, duration: 0.072s, episode steps:   7, steps per second:  98, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.857 [0.000, 8.000],  loss: 0.243406, mae: 13.920650, mean_q: 19.923641\n",
            " 137436/150000: episode: 20022, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.746687, mae: 14.013470, mean_q: 20.035440\n",
            " 137443/150000: episode: 20023, duration: 0.073s, episode steps:   7, steps per second:  96, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.369190, mae: 13.540064, mean_q: 19.902292\n",
            " 137451/150000: episode: 20024, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 1.082286, mae: 13.606905, mean_q: 20.074915\n",
            " 137457/150000: episode: 20025, duration: 0.071s, episode steps:   6, steps per second:  85, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 6.000],  loss: 0.326998, mae: 13.957648, mean_q: 20.198210\n",
            " 137464/150000: episode: 20026, duration: 0.075s, episode steps:   7, steps per second:  93, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.481157, mae: 13.850188, mean_q: 20.281878\n",
            " 137468/150000: episode: 20027, duration: 0.043s, episode steps:   4, steps per second:  93, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.000 [2.000, 8.000],  loss: 0.484352, mae: 13.864899, mean_q: 20.084255\n",
            " 137472/150000: episode: 20028, duration: 0.046s, episode steps:   4, steps per second:  87, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.000 [0.000, 8.000],  loss: 0.594907, mae: 13.691134, mean_q: 19.993652\n",
            " 137480/150000: episode: 20029, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.250 [0.000, 8.000],  loss: 0.333740, mae: 13.843143, mean_q: 20.699604\n",
            " 137486/150000: episode: 20030, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 8.000],  loss: 0.401918, mae: 13.812291, mean_q: 19.926451\n",
            " 137494/150000: episode: 20031, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.462613, mae: 13.871188, mean_q: 20.172894\n",
            " 137499/150000: episode: 20032, duration: 0.056s, episode steps:   5, steps per second:  90, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [1.000, 8.000],  loss: 0.367677, mae: 13.885724, mean_q: 20.259686\n",
            " 137504/150000: episode: 20033, duration: 0.061s, episode steps:   5, steps per second:  82, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.800 [1.000, 5.000],  loss: 0.258083, mae: 13.971796, mean_q: 20.169292\n",
            " 137511/150000: episode: 20034, duration: 0.067s, episode steps:   7, steps per second: 104, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.281636, mae: 13.869733, mean_q: 20.150366\n",
            " 137517/150000: episode: 20035, duration: 0.060s, episode steps:   6, steps per second: 101, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [2.000, 7.000],  loss: 0.174351, mae: 13.635167, mean_q: 19.973925\n",
            " 137524/150000: episode: 20036, duration: 0.062s, episode steps:   7, steps per second: 113, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.000 [0.000, 7.000],  loss: 0.177211, mae: 13.826872, mean_q: 20.029341\n",
            " 137530/150000: episode: 20037, duration: 0.070s, episode steps:   6, steps per second:  86, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.203075, mae: 13.956677, mean_q: 20.077868\n",
            " 137537/150000: episode: 20038, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.374187, mae: 13.696157, mean_q: 19.790112\n",
            " 137544/150000: episode: 20039, duration: 0.069s, episode steps:   7, steps per second: 102, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.300402, mae: 14.065460, mean_q: 20.264784\n",
            " 137552/150000: episode: 20040, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.451891, mae: 13.913612, mean_q: 19.950403\n",
            " 137560/150000: episode: 20041, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.345512, mae: 14.089665, mean_q: 20.198580\n",
            " 137566/150000: episode: 20042, duration: 0.059s, episode steps:   6, steps per second: 101, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.307275, mae: 13.816598, mean_q: 19.817133\n",
            " 137573/150000: episode: 20043, duration: 0.077s, episode steps:   7, steps per second:  91, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.275292, mae: 13.518762, mean_q: 19.964190\n",
            " 137578/150000: episode: 20044, duration: 0.054s, episode steps:   5, steps per second:  93, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.000 [0.000, 5.000],  loss: 0.380715, mae: 13.751259, mean_q: 19.963516\n",
            " 137586/150000: episode: 20045, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.000 [0.000, 8.000],  loss: 0.268819, mae: 13.796722, mean_q: 19.916506\n",
            " 137593/150000: episode: 20046, duration: 0.069s, episode steps:   7, steps per second: 102, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.429 [0.000, 8.000],  loss: 0.275097, mae: 14.033783, mean_q: 20.240688\n",
            " 137602/150000: episode: 20047, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.387705, mae: 13.728273, mean_q: 19.945164\n",
            " 137610/150000: episode: 20048, duration: 0.070s, episode steps:   8, steps per second: 114, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.125 [1.000, 8.000],  loss: 0.323848, mae: 13.931990, mean_q: 19.987328\n",
            " 137618/150000: episode: 20049, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.306756, mae: 14.103563, mean_q: 19.980076\n",
            " 137623/150000: episode: 20050, duration: 0.047s, episode steps:   5, steps per second: 107, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 8.000],  loss: 0.139213, mae: 13.675508, mean_q: 20.184713\n",
            " 137627/150000: episode: 20051, duration: 0.055s, episode steps:   4, steps per second:  72, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.000 [0.000, 8.000],  loss: 0.143082, mae: 14.068764, mean_q: 20.202374\n",
            " 137632/150000: episode: 20052, duration: 0.049s, episode steps:   5, steps per second: 101, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.284320, mae: 14.082850, mean_q: 19.898365\n",
            " 137639/150000: episode: 20053, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.429 [1.000, 8.000],  loss: 0.320780, mae: 13.894571, mean_q: 20.157480\n",
            " 137646/150000: episode: 20054, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.400902, mae: 14.147561, mean_q: 20.050159\n",
            " 137654/150000: episode: 20055, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.455445, mae: 13.938478, mean_q: 19.942158\n",
            " 137661/150000: episode: 20056, duration: 0.071s, episode steps:   7, steps per second:  99, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.247955, mae: 13.878255, mean_q: 20.370218\n",
            " 137667/150000: episode: 20057, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.299333, mae: 14.047948, mean_q: 20.213486\n",
            " 137675/150000: episode: 20058, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.264199, mae: 13.812128, mean_q: 20.033684\n",
            " 137682/150000: episode: 20059, duration: 0.071s, episode steps:   7, steps per second:  99, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.307417, mae: 13.737825, mean_q: 20.101057\n",
            " 137688/150000: episode: 20060, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 8.000],  loss: 0.343528, mae: 14.076644, mean_q: 20.165152\n",
            " 137694/150000: episode: 20061, duration: 0.063s, episode steps:   6, steps per second:  95, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.333 [1.000, 7.000],  loss: 0.385583, mae: 14.068711, mean_q: 19.932802\n",
            " 137698/150000: episode: 20062, duration: 0.052s, episode steps:   4, steps per second:  76, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.500 [3.000, 7.000],  loss: 0.265092, mae: 14.139179, mean_q: 20.091545\n",
            " 137704/150000: episode: 20063, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [1.000, 8.000],  loss: 0.377877, mae: 13.934326, mean_q: 19.999228\n",
            " 137713/150000: episode: 20064, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.255593, mae: 14.101856, mean_q: 20.053717\n",
            " 137718/150000: episode: 20065, duration: 0.053s, episode steps:   5, steps per second:  95, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.400 [0.000, 7.000],  loss: 0.364217, mae: 13.750769, mean_q: 20.165493\n",
            " 137725/150000: episode: 20066, duration: 0.078s, episode steps:   7, steps per second:  90, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.430302, mae: 13.814513, mean_q: 19.970621\n",
            " 137730/150000: episode: 20067, duration: 0.052s, episode steps:   5, steps per second:  96, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.400 [0.000, 6.000],  loss: 0.547954, mae: 14.147943, mean_q: 20.100595\n",
            " 137735/150000: episode: 20068, duration: 0.049s, episode steps:   5, steps per second: 102, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.400 [1.000, 8.000],  loss: 0.316220, mae: 14.082300, mean_q: 20.203129\n",
            " 137744/150000: episode: 20069, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.545930, mae: 13.714109, mean_q: 19.882666\n",
            " 137750/150000: episode: 20070, duration: 0.067s, episode steps:   6, steps per second:  89, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.315427, mae: 14.157891, mean_q: 20.080549\n",
            " 137755/150000: episode: 20071, duration: 0.049s, episode steps:   5, steps per second: 102, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.245352, mae: 14.042127, mean_q: 20.186619\n",
            " 137764/150000: episode: 20072, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.667 [0.000, 8.000],  loss: 0.778625, mae: 14.000381, mean_q: 20.054361\n",
            " 137771/150000: episode: 20073, duration: 0.085s, episode steps:   7, steps per second:  83, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.390516, mae: 14.109041, mean_q: 19.994045\n",
            " 137779/150000: episode: 20074, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.256865, mae: 14.224800, mean_q: 20.130056\n",
            " 137784/150000: episode: 20075, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [1.000, 8.000],  loss: 0.287956, mae: 13.586472, mean_q: 20.035727\n",
            " 137792/150000: episode: 20076, duration: 0.097s, episode steps:   8, steps per second:  83, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.257481, mae: 14.124485, mean_q: 19.980675\n",
            " 137801/150000: episode: 20077, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.306741, mae: 13.817816, mean_q: 20.115057\n",
            " 137809/150000: episode: 20078, duration: 0.074s, episode steps:   8, steps per second: 107, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.750 [0.000, 8.000],  loss: 0.244040, mae: 13.664099, mean_q: 20.031368\n",
            " 137817/150000: episode: 20079, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.000 [0.000, 8.000],  loss: 0.326274, mae: 13.737454, mean_q: 20.094173\n",
            " 137825/150000: episode: 20080, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.346384, mae: 13.783993, mean_q: 20.044941\n",
            " 137833/150000: episode: 20081, duration: 0.069s, episode steps:   8, steps per second: 115, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.237969, mae: 13.959764, mean_q: 19.970345\n",
            " 137840/150000: episode: 20082, duration: 0.073s, episode steps:   7, steps per second:  96, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.347896, mae: 13.876731, mean_q: 20.195238\n",
            " 137846/150000: episode: 20083, duration: 0.061s, episode steps:   6, steps per second:  98, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.692862, mae: 13.854098, mean_q: 19.889631\n",
            " 137854/150000: episode: 20084, duration: 0.077s, episode steps:   8, steps per second: 103, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.250546, mae: 13.852859, mean_q: 20.148804\n",
            " 137862/150000: episode: 20085, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.356844, mae: 14.051604, mean_q: 20.089706\n",
            " 137868/150000: episode: 20086, duration: 0.062s, episode steps:   6, steps per second:  96, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [0.000, 7.000],  loss: 0.236576, mae: 14.170616, mean_q: 19.833731\n",
            " 137874/150000: episode: 20087, duration: 0.056s, episode steps:   6, steps per second: 108, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [1.000, 8.000],  loss: 0.198849, mae: 13.752289, mean_q: 20.122896\n",
            " 137883/150000: episode: 20088, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.236158, mae: 13.691544, mean_q: 19.988686\n",
            " 137892/150000: episode: 20089, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.464344, mae: 13.672121, mean_q: 20.002831\n",
            " 137898/150000: episode: 20090, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [1.000, 8.000],  loss: 0.592610, mae: 14.142148, mean_q: 20.083250\n",
            " 137906/150000: episode: 20091, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.323243, mae: 13.778167, mean_q: 20.169212\n",
            " 137915/150000: episode: 20092, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.247956, mae: 13.685981, mean_q: 20.105795\n",
            " 137924/150000: episode: 20093, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.333 [0.000, 8.000],  loss: 0.234979, mae: 14.062201, mean_q: 20.066841\n",
            " 137932/150000: episode: 20094, duration: 0.088s, episode steps:   8, steps per second:  90, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.750 [0.000, 8.000],  loss: 0.325728, mae: 13.792341, mean_q: 19.875179\n",
            " 137937/150000: episode: 20095, duration: 0.049s, episode steps:   5, steps per second: 102, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.800 [0.000, 8.000],  loss: 0.475735, mae: 13.757045, mean_q: 20.155445\n",
            " 137942/150000: episode: 20096, duration: 0.049s, episode steps:   5, steps per second: 102, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.800 [0.000, 8.000],  loss: 0.252028, mae: 13.861961, mean_q: 20.256243\n",
            " 137948/150000: episode: 20097, duration: 0.061s, episode steps:   6, steps per second:  98, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [1.000, 7.000],  loss: 0.192030, mae: 13.742595, mean_q: 19.940056\n",
            " 137955/150000: episode: 20098, duration: 0.080s, episode steps:   7, steps per second:  87, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.571 [0.000, 8.000],  loss: 0.245843, mae: 13.772601, mean_q: 20.148706\n",
            " 137963/150000: episode: 20099, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.340887, mae: 13.964518, mean_q: 20.001347\n",
            " 137969/150000: episode: 20100, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [1.000, 8.000],  loss: 0.180977, mae: 13.773664, mean_q: 20.016897\n",
            " 137975/150000: episode: 20101, duration: 0.063s, episode steps:   6, steps per second:  96, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.333 [1.000, 8.000],  loss: 0.223833, mae: 13.905292, mean_q: 20.043713\n",
            " 137983/150000: episode: 20102, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.250 [0.000, 8.000],  loss: 0.171896, mae: 13.791995, mean_q: 20.065845\n",
            " 137985/150000: episode: 20103, duration: 0.025s, episode steps:   2, steps per second:  81, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 3.000 [3.000, 3.000],  loss: 0.183588, mae: 13.431697, mean_q: 20.083496\n",
            " 137989/150000: episode: 20104, duration: 0.042s, episode steps:   4, steps per second:  95, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.750 [2.000, 8.000],  loss: 0.183571, mae: 13.810168, mean_q: 19.939581\n",
            " 137994/150000: episode: 20105, duration: 0.049s, episode steps:   5, steps per second: 102, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.295933, mae: 14.014338, mean_q: 20.098186\n",
            " 138002/150000: episode: 20106, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.557520, mae: 14.182100, mean_q: 20.148989\n",
            " 138008/150000: episode: 20107, duration: 0.071s, episode steps:   6, steps per second:  85, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.337716, mae: 14.040412, mean_q: 20.139481\n",
            " 138016/150000: episode: 20108, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.273282, mae: 13.737744, mean_q: 20.094505\n",
            " 138022/150000: episode: 20109, duration: 0.073s, episode steps:   6, steps per second:  82, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [1.000, 7.000],  loss: 0.525183, mae: 13.857265, mean_q: 20.139669\n",
            " 138030/150000: episode: 20110, duration: 0.121s, episode steps:   8, steps per second:  66, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.359704, mae: 14.102883, mean_q: 20.063105\n",
            " 138039/150000: episode: 20111, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.441174, mae: 14.039204, mean_q: 20.192112\n",
            " 138045/150000: episode: 20112, duration: 0.091s, episode steps:   6, steps per second:  66, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [2.000, 8.000],  loss: 0.186913, mae: 13.966618, mean_q: 20.042633\n",
            " 138053/150000: episode: 20113, duration: 0.112s, episode steps:   8, steps per second:  71, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.750 [0.000, 8.000],  loss: 0.277446, mae: 13.741613, mean_q: 20.144588\n",
            " 138059/150000: episode: 20114, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [1.000, 7.000],  loss: 0.333587, mae: 13.853516, mean_q: 19.989553\n",
            " 138066/150000: episode: 20115, duration: 0.126s, episode steps:   7, steps per second:  56, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.406924, mae: 13.946632, mean_q: 20.170834\n",
            " 138075/150000: episode: 20116, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.333 [0.000, 8.000],  loss: 0.298417, mae: 14.322803, mean_q: 20.008547\n",
            " 138082/150000: episode: 20117, duration: 0.096s, episode steps:   7, steps per second:  73, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [0.000, 8.000],  loss: 0.358604, mae: 13.917167, mean_q: 20.170673\n",
            " 138089/150000: episode: 20118, duration: 0.111s, episode steps:   7, steps per second:  63, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.380007, mae: 13.702822, mean_q: 19.878353\n",
            " 138097/150000: episode: 20119, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.250 [0.000, 8.000],  loss: 0.283922, mae: 14.095036, mean_q: 20.192112\n",
            " 138104/150000: episode: 20120, duration: 0.090s, episode steps:   7, steps per second:  78, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.380053, mae: 14.224957, mean_q: 20.160940\n",
            " 138110/150000: episode: 20121, duration: 0.085s, episode steps:   6, steps per second:  70, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.288332, mae: 13.875463, mean_q: 20.040987\n",
            " 138119/150000: episode: 20122, duration: 0.124s, episode steps:   9, steps per second:  72, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.228450, mae: 13.952139, mean_q: 20.082985\n",
            " 138126/150000: episode: 20123, duration: 0.086s, episode steps:   7, steps per second:  81, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.494150, mae: 14.065470, mean_q: 20.014750\n",
            " 138131/150000: episode: 20124, duration: 0.072s, episode steps:   5, steps per second:  70, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.000 [0.000, 7.000],  loss: 0.630432, mae: 14.002386, mean_q: 19.836567\n",
            " 138140/150000: episode: 20125, duration: 0.155s, episode steps:   9, steps per second:  58, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.321338, mae: 13.793788, mean_q: 20.109385\n",
            " 138147/150000: episode: 20126, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.234412, mae: 13.620123, mean_q: 20.016125\n",
            " 138152/150000: episode: 20127, duration: 0.065s, episode steps:   5, steps per second:  77, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.200 [0.000, 7.000],  loss: 0.327514, mae: 13.888382, mean_q: 20.168755\n",
            " 138159/150000: episode: 20128, duration: 0.104s, episode steps:   7, steps per second:  67, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.225543, mae: 13.989110, mean_q: 20.054340\n",
            " 138164/150000: episode: 20129, duration: 0.085s, episode steps:   5, steps per second:  59, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.274655, mae: 14.222409, mean_q: 20.175303\n",
            " 138171/150000: episode: 20130, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.379823, mae: 13.841809, mean_q: 20.075399\n",
            " 138178/150000: episode: 20131, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.421484, mae: 13.811585, mean_q: 19.741436\n",
            " 138187/150000: episode: 20132, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.282964, mae: 13.780863, mean_q: 19.998516\n",
            " 138195/150000: episode: 20133, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.125 [0.000, 7.000],  loss: 0.441698, mae: 13.942754, mean_q: 19.918739\n",
            " 138203/150000: episode: 20134, duration: 0.121s, episode steps:   8, steps per second:  66, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.287094, mae: 13.835478, mean_q: 20.262650\n",
            " 138210/150000: episode: 20135, duration: 0.107s, episode steps:   7, steps per second:  65, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.214768, mae: 13.765222, mean_q: 19.965101\n",
            " 138217/150000: episode: 20136, duration: 0.104s, episode steps:   7, steps per second:  67, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.652076, mae: 13.768372, mean_q: 19.869707\n",
            " 138223/150000: episode: 20137, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.913080, mae: 13.650950, mean_q: 20.102859\n",
            " 138229/150000: episode: 20138, duration: 0.113s, episode steps:   6, steps per second:  53, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [1.000, 7.000],  loss: 0.316350, mae: 13.712251, mean_q: 19.816607\n",
            " 138235/150000: episode: 20139, duration: 0.087s, episode steps:   6, steps per second:  69, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [0.000, 8.000],  loss: 0.390050, mae: 13.928582, mean_q: 20.202938\n",
            " 138244/150000: episode: 20140, duration: 0.128s, episode steps:   9, steps per second:  70, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.503635, mae: 13.986922, mean_q: 19.898155\n",
            " 138248/150000: episode: 20141, duration: 0.069s, episode steps:   4, steps per second:  58, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 6.000 [2.000, 8.000],  loss: 0.506056, mae: 14.364009, mean_q: 20.190008\n",
            " 138256/150000: episode: 20142, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.250 [0.000, 8.000],  loss: 0.276836, mae: 13.956232, mean_q: 20.348591\n",
            " 138263/150000: episode: 20143, duration: 0.081s, episode steps:   7, steps per second:  86, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.146851, mae: 14.171188, mean_q: 20.065378\n",
            " 138268/150000: episode: 20144, duration: 0.047s, episode steps:   5, steps per second: 106, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.363731, mae: 13.857679, mean_q: 19.944267\n",
            " 138276/150000: episode: 20145, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.216365, mae: 13.889048, mean_q: 20.231560\n",
            " 138282/150000: episode: 20146, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2.667 [0.000, 6.000],  loss: 0.249720, mae: 13.579361, mean_q: 19.921293\n",
            " 138287/150000: episode: 20147, duration: 0.064s, episode steps:   5, steps per second:  78, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.800 [1.000, 8.000],  loss: 0.181602, mae: 14.172046, mean_q: 20.040974\n",
            " 138294/150000: episode: 20148, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.243329, mae: 14.334884, mean_q: 20.026291\n",
            " 138301/150000: episode: 20149, duration: 0.068s, episode steps:   7, steps per second: 103, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.286 [0.000, 8.000],  loss: 0.341179, mae: 13.675069, mean_q: 19.857759\n",
            " 138309/150000: episode: 20150, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.435452, mae: 13.863861, mean_q: 20.241945\n",
            " 138314/150000: episode: 20151, duration: 0.052s, episode steps:   5, steps per second:  97, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.290604, mae: 14.144552, mean_q: 19.966740\n",
            " 138320/150000: episode: 20152, duration: 0.059s, episode steps:   6, steps per second: 101, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [0.000, 7.000],  loss: 0.794040, mae: 13.995113, mean_q: 20.006201\n",
            " 138329/150000: episode: 20153, duration: 0.090s, episode steps:   9, steps per second:  99, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.251084, mae: 14.126031, mean_q: 19.990232\n",
            " 138334/150000: episode: 20154, duration: 0.057s, episode steps:   5, steps per second:  87, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.600 [0.000, 6.000],  loss: 0.495841, mae: 13.796112, mean_q: 19.624149\n",
            " 138342/150000: episode: 20155, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.443678, mae: 14.062813, mean_q: 19.949947\n",
            " 138351/150000: episode: 20156, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.885099, mae: 13.905696, mean_q: 19.960722\n",
            " 138360/150000: episode: 20157, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.315770, mae: 14.069343, mean_q: 20.148447\n",
            " 138369/150000: episode: 20158, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.293000, mae: 13.603668, mean_q: 19.941523\n",
            " 138378/150000: episode: 20159, duration: 0.103s, episode steps:   9, steps per second:  87, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.889 [0.000, 8.000],  loss: 0.329712, mae: 13.631027, mean_q: 20.017693\n",
            " 138385/150000: episode: 20160, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.512901, mae: 13.913367, mean_q: 20.091658\n",
            " 138391/150000: episode: 20161, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [2.000, 8.000],  loss: 0.329046, mae: 14.073623, mean_q: 19.998571\n",
            " 138400/150000: episode: 20162, duration: 0.082s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.336824, mae: 13.713965, mean_q: 20.021294\n",
            " 138407/150000: episode: 20163, duration: 0.079s, episode steps:   7, steps per second:  89, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.280371, mae: 13.516306, mean_q: 20.081287\n",
            " 138414/150000: episode: 20164, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [1.000, 8.000],  loss: 0.470700, mae: 13.938772, mean_q: 20.285234\n",
            " 138419/150000: episode: 20165, duration: 0.048s, episode steps:   5, steps per second: 104, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.800 [2.000, 7.000],  loss: 0.200647, mae: 13.866712, mean_q: 20.018564\n",
            " 138428/150000: episode: 20166, duration: 0.102s, episode steps:   9, steps per second:  89, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.210828, mae: 13.933905, mean_q: 20.101345\n",
            " 138435/150000: episode: 20167, duration: 0.067s, episode steps:   7, steps per second: 104, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.287236, mae: 13.864065, mean_q: 20.073465\n",
            " 138444/150000: episode: 20168, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.556 [0.000, 8.000],  loss: 0.289613, mae: 13.580891, mean_q: 19.989868\n",
            " 138448/150000: episode: 20169, duration: 0.040s, episode steps:   4, steps per second:  99, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.250 [0.000, 7.000],  loss: 0.200592, mae: 13.595705, mean_q: 20.134228\n",
            " 138455/150000: episode: 20170, duration: 0.084s, episode steps:   7, steps per second:  84, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.191089, mae: 14.165059, mean_q: 20.156599\n",
            " 138462/150000: episode: 20171, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.714 [1.000, 8.000],  loss: 0.465358, mae: 13.833672, mean_q: 19.919434\n",
            " 138465/150000: episode: 20172, duration: 0.031s, episode steps:   3, steps per second:  98, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 4.667 [4.000, 6.000],  loss: 0.609811, mae: 13.957896, mean_q: 20.118219\n",
            " 138473/150000: episode: 20173, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.351580, mae: 14.168098, mean_q: 20.133165\n",
            " 138480/150000: episode: 20174, duration: 0.078s, episode steps:   7, steps per second:  90, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.278118, mae: 13.831065, mean_q: 19.956942\n",
            " 138487/150000: episode: 20175, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.000 [0.000, 8.000],  loss: 0.291613, mae: 14.380107, mean_q: 20.008133\n",
            " 138492/150000: episode: 20176, duration: 0.050s, episode steps:   5, steps per second:  99, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.200 [0.000, 8.000],  loss: 0.169811, mae: 14.078644, mean_q: 20.249659\n",
            " 138501/150000: episode: 20177, duration: 0.089s, episode steps:   9, steps per second: 102, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.254984, mae: 14.117290, mean_q: 20.067497\n",
            " 138507/150000: episode: 20178, duration: 0.068s, episode steps:   6, steps per second:  88, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [2.000, 7.000],  loss: 0.275791, mae: 13.806489, mean_q: 20.170855\n",
            " 138512/150000: episode: 20179, duration: 0.049s, episode steps:   5, steps per second: 102, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.000 [2.000, 7.000],  loss: 0.996005, mae: 13.907043, mean_q: 20.097332\n",
            " 138515/150000: episode: 20180, duration: 0.033s, episode steps:   3, steps per second:  90, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 5.000 [4.000, 7.000],  loss: 0.302237, mae: 13.883222, mean_q: 19.918316\n",
            " 138521/150000: episode: 20181, duration: 0.060s, episode steps:   6, steps per second: 101, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [2.000, 7.000],  loss: 0.314350, mae: 13.949794, mean_q: 20.097033\n",
            " 138526/150000: episode: 20182, duration: 0.080s, episode steps:   5, steps per second:  63, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 6.000],  loss: 0.321005, mae: 14.008405, mean_q: 20.196737\n",
            " 138535/150000: episode: 20183, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.237960, mae: 13.888680, mean_q: 20.074911\n",
            " 138543/150000: episode: 20184, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.500 [0.000, 8.000],  loss: 0.438899, mae: 13.904360, mean_q: 20.064463\n",
            " 138549/150000: episode: 20185, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.682347, mae: 13.875134, mean_q: 20.045061\n",
            " 138556/150000: episode: 20186, duration: 0.069s, episode steps:   7, steps per second: 102, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.322817, mae: 13.969043, mean_q: 19.856470\n",
            " 138563/150000: episode: 20187, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.143 [0.000, 7.000],  loss: 0.247334, mae: 14.129496, mean_q: 20.085865\n",
            " 138571/150000: episode: 20188, duration: 0.097s, episode steps:   8, steps per second:  83, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.221267, mae: 14.086479, mean_q: 20.013445\n",
            " 138578/150000: episode: 20189, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.571 [0.000, 8.000],  loss: 0.392548, mae: 13.870615, mean_q: 20.106440\n",
            " 138585/150000: episode: 20190, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.344765, mae: 14.204400, mean_q: 20.039623\n",
            " 138592/150000: episode: 20191, duration: 0.068s, episode steps:   7, steps per second: 103, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.269884, mae: 14.106374, mean_q: 20.155779\n",
            " 138597/150000: episode: 20192, duration: 0.063s, episode steps:   5, steps per second:  79, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.800 [0.000, 8.000],  loss: 0.477651, mae: 13.981692, mean_q: 19.945610\n",
            " 138605/150000: episode: 20193, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.230510, mae: 13.785961, mean_q: 20.052786\n",
            " 138612/150000: episode: 20194, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.255429, mae: 13.832664, mean_q: 20.055653\n",
            " 138621/150000: episode: 20195, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.238212, mae: 14.022975, mean_q: 19.976797\n",
            " 138627/150000: episode: 20196, duration: 0.071s, episode steps:   6, steps per second:  84, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [2.000, 7.000],  loss: 0.218182, mae: 13.910446, mean_q: 19.991385\n",
            " 138630/150000: episode: 20197, duration: 0.034s, episode steps:   3, steps per second:  88, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 5.333 [0.000, 8.000],  loss: 0.582691, mae: 14.074502, mean_q: 19.896008\n",
            " 138634/150000: episode: 20198, duration: 0.042s, episode steps:   4, steps per second:  95, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 2.500 [0.000, 8.000],  loss: 0.292463, mae: 13.992915, mean_q: 20.163214\n",
            " 138641/150000: episode: 20199, duration: 0.074s, episode steps:   7, steps per second:  94, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.461222, mae: 14.242669, mean_q: 20.151186\n",
            " 138648/150000: episode: 20200, duration: 0.076s, episode steps:   7, steps per second:  92, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.429 [0.000, 8.000],  loss: 0.489823, mae: 13.675215, mean_q: 19.975452\n",
            " 138655/150000: episode: 20201, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.212580, mae: 13.792337, mean_q: 20.179514\n",
            " 138659/150000: episode: 20202, duration: 0.040s, episode steps:   4, steps per second: 100, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.250 [2.000, 7.000],  loss: 0.463552, mae: 14.155046, mean_q: 20.025465\n",
            " 138667/150000: episode: 20203, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.625 [0.000, 8.000],  loss: 0.354254, mae: 13.809036, mean_q: 20.156981\n",
            " 138673/150000: episode: 20204, duration: 0.056s, episode steps:   6, steps per second: 108, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [1.000, 8.000],  loss: 0.233199, mae: 14.310516, mean_q: 20.104361\n",
            " 138679/150000: episode: 20205, duration: 0.055s, episode steps:   6, steps per second: 110, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.571492, mae: 14.238412, mean_q: 20.195965\n",
            " 138688/150000: episode: 20206, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.257501, mae: 14.167374, mean_q: 20.021049\n",
            " 138695/150000: episode: 20207, duration: 0.080s, episode steps:   7, steps per second:  88, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.254979, mae: 14.219333, mean_q: 20.077837\n",
            " 138703/150000: episode: 20208, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.625 [1.000, 8.000],  loss: 0.311879, mae: 14.095786, mean_q: 20.123749\n",
            " 138710/150000: episode: 20209, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.400433, mae: 14.175819, mean_q: 19.991690\n",
            " 138717/150000: episode: 20210, duration: 0.063s, episode steps:   7, steps per second: 111, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.984930, mae: 14.325514, mean_q: 20.121099\n",
            " 138719/150000: episode: 20211, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 6.000 [6.000, 6.000],  loss: 0.289277, mae: 13.699684, mean_q: 20.079952\n",
            " 138727/150000: episode: 20212, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.355581, mae: 13.765049, mean_q: 20.205795\n",
            " 138734/150000: episode: 20213, duration: 0.063s, episode steps:   7, steps per second: 112, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.286 [2.000, 7.000],  loss: 0.620513, mae: 13.691396, mean_q: 20.055752\n",
            " 138741/150000: episode: 20214, duration: 0.082s, episode steps:   7, steps per second:  85, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.429 [0.000, 8.000],  loss: 0.565342, mae: 13.809575, mean_q: 20.235489\n",
            " 138748/150000: episode: 20215, duration: 0.067s, episode steps:   7, steps per second: 104, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.196268, mae: 13.766774, mean_q: 20.115183\n",
            " 138756/150000: episode: 20216, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.373711, mae: 14.238420, mean_q: 19.999701\n",
            " 138763/150000: episode: 20217, duration: 0.075s, episode steps:   7, steps per second:  93, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.546397, mae: 13.747376, mean_q: 19.954227\n",
            " 138769/150000: episode: 20218, duration: 0.066s, episode steps:   6, steps per second:  91, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.333 [2.000, 7.000],  loss: 0.364964, mae: 13.727692, mean_q: 20.013475\n",
            " 138777/150000: episode: 20219, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.623228, mae: 14.069440, mean_q: 20.195898\n",
            " 138786/150000: episode: 20220, duration: 0.094s, episode steps:   9, steps per second:  95, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.306747, mae: 14.166660, mean_q: 20.090393\n",
            " 138792/150000: episode: 20221, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 7.000],  loss: 0.319917, mae: 13.903386, mean_q: 20.040056\n",
            " 138798/150000: episode: 20222, duration: 0.062s, episode steps:   6, steps per second:  97, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2.667 [0.000, 6.000],  loss: 0.217826, mae: 13.764171, mean_q: 20.025087\n",
            " 138807/150000: episode: 20223, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.315085, mae: 13.612247, mean_q: 20.071396\n",
            " 138815/150000: episode: 20224, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.375 [0.000, 7.000],  loss: 0.257561, mae: 14.077351, mean_q: 20.057800\n",
            " 138820/150000: episode: 20225, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.000 [0.000, 5.000],  loss: 0.248684, mae: 14.038592, mean_q: 20.010427\n",
            " 138827/150000: episode: 20226, duration: 0.084s, episode steps:   7, steps per second:  84, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.323041, mae: 13.687053, mean_q: 20.042204\n",
            " 138834/150000: episode: 20227, duration: 0.078s, episode steps:   7, steps per second:  90, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.857 [0.000, 7.000],  loss: 0.221377, mae: 14.103702, mean_q: 20.019592\n",
            " 138840/150000: episode: 20228, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [1.000, 7.000],  loss: 0.245343, mae: 13.807747, mean_q: 20.003668\n",
            " 138846/150000: episode: 20229, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.821198, mae: 13.831788, mean_q: 20.074629\n",
            " 138855/150000: episode: 20230, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.407732, mae: 13.792727, mean_q: 19.969604\n",
            " 138862/150000: episode: 20231, duration: 0.072s, episode steps:   7, steps per second:  97, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.522186, mae: 13.586858, mean_q: 19.947388\n",
            " 138871/150000: episode: 20232, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.349891, mae: 13.894030, mean_q: 20.083199\n",
            " 138874/150000: episode: 20233, duration: 0.031s, episode steps:   3, steps per second:  96, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 3.000 [2.000, 5.000],  loss: 0.307833, mae: 13.994694, mean_q: 20.268194\n",
            " 138880/150000: episode: 20234, duration: 0.073s, episode steps:   6, steps per second:  82, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [1.000, 8.000],  loss: 0.985343, mae: 13.493983, mean_q: 19.811510\n",
            " 138886/150000: episode: 20235, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [0.000, 8.000],  loss: 0.557319, mae: 13.742153, mean_q: 19.740452\n",
            " 138894/150000: episode: 20236, duration: 0.074s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.282033, mae: 14.123795, mean_q: 20.276703\n",
            " 138901/150000: episode: 20237, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.251437, mae: 13.995185, mean_q: 20.390110\n",
            " 138908/150000: episode: 20238, duration: 0.079s, episode steps:   7, steps per second:  89, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.857 [0.000, 7.000],  loss: 0.309847, mae: 13.944473, mean_q: 20.136831\n",
            " 138916/150000: episode: 20239, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.486115, mae: 13.771206, mean_q: 20.075764\n",
            " 138922/150000: episode: 20240, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.164171, mae: 13.710320, mean_q: 20.118752\n",
            " 138928/150000: episode: 20241, duration: 0.065s, episode steps:   6, steps per second:  92, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.637277, mae: 13.882327, mean_q: 19.964579\n",
            " 138937/150000: episode: 20242, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.567627, mae: 13.954241, mean_q: 19.992393\n",
            " 138943/150000: episode: 20243, duration: 0.066s, episode steps:   6, steps per second:  91, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.000 [0.000, 8.000],  loss: 0.517923, mae: 13.923924, mean_q: 20.337404\n",
            " 138948/150000: episode: 20244, duration: 0.050s, episode steps:   5, steps per second: 101, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.290244, mae: 13.913673, mean_q: 20.065449\n",
            " 138954/150000: episode: 20245, duration: 0.064s, episode steps:   6, steps per second:  94, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [0.000, 7.000],  loss: 0.314194, mae: 14.014852, mean_q: 20.007446\n",
            " 138962/150000: episode: 20246, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.499924, mae: 13.763649, mean_q: 20.174782\n",
            " 138971/150000: episode: 20247, duration: 0.085s, episode steps:   9, steps per second: 105, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.380819, mae: 14.031517, mean_q: 20.071503\n",
            " 138977/150000: episode: 20248, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.279923, mae: 13.891303, mean_q: 20.177271\n",
            " 138984/150000: episode: 20249, duration: 0.072s, episode steps:   7, steps per second:  97, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.208871, mae: 13.928918, mean_q: 20.342560\n",
            " 138993/150000: episode: 20250, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.182529, mae: 14.095954, mean_q: 20.217075\n",
            " 138998/150000: episode: 20251, duration: 0.052s, episode steps:   5, steps per second:  96, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.400 [2.000, 8.000],  loss: 0.245324, mae: 13.920270, mean_q: 20.012585\n",
            " 139004/150000: episode: 20252, duration: 0.059s, episode steps:   6, steps per second: 101, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 6.000],  loss: 0.349884, mae: 14.235354, mean_q: 20.143755\n",
            " 139010/150000: episode: 20253, duration: 0.065s, episode steps:   6, steps per second:  92, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.167 [2.000, 8.000],  loss: 0.220429, mae: 14.268720, mean_q: 20.060732\n",
            " 139016/150000: episode: 20254, duration: 0.062s, episode steps:   6, steps per second:  97, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.193138, mae: 13.994872, mean_q: 20.115812\n",
            " 139022/150000: episode: 20255, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.336751, mae: 13.605651, mean_q: 19.880548\n",
            " 139027/150000: episode: 20256, duration: 0.080s, episode steps:   5, steps per second:  63, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.334185, mae: 13.866305, mean_q: 19.826605\n",
            " 139034/150000: episode: 20257, duration: 0.070s, episode steps:   7, steps per second: 100, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.329174, mae: 13.762251, mean_q: 20.132246\n",
            " 139043/150000: episode: 20258, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.365211, mae: 13.768590, mean_q: 19.908731\n",
            " 139050/150000: episode: 20259, duration: 0.081s, episode steps:   7, steps per second:  86, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.571 [0.000, 8.000],  loss: 0.482415, mae: 14.016222, mean_q: 20.244913\n",
            " 139059/150000: episode: 20260, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.384969, mae: 14.032406, mean_q: 20.059397\n",
            " 139064/150000: episode: 20261, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.600 [0.000, 8.000],  loss: 0.664480, mae: 13.609372, mean_q: 19.989351\n",
            " 139069/150000: episode: 20262, duration: 0.049s, episode steps:   5, steps per second: 103, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.800 [0.000, 8.000],  loss: 0.353316, mae: 13.980245, mean_q: 20.293747\n",
            " 139077/150000: episode: 20263, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.297669, mae: 13.794962, mean_q: 20.076952\n",
            " 139084/150000: episode: 20264, duration: 0.071s, episode steps:   7, steps per second:  99, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.213538, mae: 13.894120, mean_q: 20.091982\n",
            " 139091/150000: episode: 20265, duration: 0.068s, episode steps:   7, steps per second: 103, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 2.571 [0.000, 6.000],  loss: 0.295906, mae: 13.844186, mean_q: 20.008593\n",
            " 139097/150000: episode: 20266, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.265326, mae: 14.006020, mean_q: 20.071951\n",
            " 139104/150000: episode: 20267, duration: 0.082s, episode steps:   7, steps per second:  85, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.448487, mae: 14.081645, mean_q: 19.902418\n",
            " 139113/150000: episode: 20268, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.310851, mae: 13.803595, mean_q: 19.901670\n",
            " 139120/150000: episode: 20269, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.428665, mae: 13.947349, mean_q: 19.827488\n",
            " 139129/150000: episode: 20270, duration: 0.109s, episode steps:   9, steps per second:  82, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.229927, mae: 13.932616, mean_q: 20.236504\n",
            " 139138/150000: episode: 20271, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.653225, mae: 13.867651, mean_q: 19.872244\n",
            " 139145/150000: episode: 20272, duration: 0.071s, episode steps:   7, steps per second:  98, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.286 [0.000, 8.000],  loss: 0.249832, mae: 14.370015, mean_q: 20.243814\n",
            " 139150/150000: episode: 20273, duration: 0.063s, episode steps:   5, steps per second:  79, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.337554, mae: 14.041327, mean_q: 20.032803\n",
            " 139158/150000: episode: 20274, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.125 [0.000, 8.000],  loss: 0.666665, mae: 13.981210, mean_q: 20.175480\n",
            " 139167/150000: episode: 20275, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.396448, mae: 13.950532, mean_q: 20.001434\n",
            " 139174/150000: episode: 20276, duration: 0.077s, episode steps:   7, steps per second:  91, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 7.000],  loss: 0.390203, mae: 13.830894, mean_q: 20.320044\n",
            " 139183/150000: episode: 20277, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.553489, mae: 13.774532, mean_q: 20.189606\n",
            " 139192/150000: episode: 20278, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.321418, mae: 13.871602, mean_q: 20.308672\n",
            " 139197/150000: episode: 20279, duration: 0.064s, episode steps:   5, steps per second:  78, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.429574, mae: 13.808049, mean_q: 20.070436\n",
            " 139206/150000: episode: 20280, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.402206, mae: 13.954405, mean_q: 20.096066\n",
            " 139213/150000: episode: 20281, duration: 0.070s, episode steps:   7, steps per second: 100, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.249117, mae: 13.961804, mean_q: 20.036352\n",
            " 139222/150000: episode: 20282, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.562039, mae: 14.141643, mean_q: 20.172548\n",
            " 139229/150000: episode: 20283, duration: 0.105s, episode steps:   7, steps per second:  66, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.667493, mae: 13.878316, mean_q: 20.131941\n",
            " 139236/150000: episode: 20284, duration: 0.108s, episode steps:   7, steps per second:  65, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.439953, mae: 13.855886, mean_q: 20.156237\n",
            " 139241/150000: episode: 20285, duration: 0.078s, episode steps:   5, steps per second:  64, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.355834, mae: 14.121542, mean_q: 20.080000\n",
            " 139250/150000: episode: 20286, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.685105, mae: 14.310135, mean_q: 20.152021\n",
            " 139258/150000: episode: 20287, duration: 0.115s, episode steps:   8, steps per second:  69, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.875 [0.000, 7.000],  loss: 0.327409, mae: 13.935552, mean_q: 20.073883\n",
            " 139265/150000: episode: 20288, duration: 0.095s, episode steps:   7, steps per second:  73, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.270432, mae: 13.967765, mean_q: 20.053217\n",
            " 139271/150000: episode: 20289, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.199458, mae: 14.500695, mean_q: 20.162783\n",
            " 139276/150000: episode: 20290, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.800 [0.000, 7.000],  loss: 0.258579, mae: 14.212690, mean_q: 20.127350\n",
            " 139284/150000: episode: 20291, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.375 [0.000, 8.000],  loss: 0.250499, mae: 13.808027, mean_q: 20.106520\n",
            " 139289/150000: episode: 20292, duration: 0.067s, episode steps:   5, steps per second:  75, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.200 [0.000, 6.000],  loss: 0.219363, mae: 13.738373, mean_q: 20.195757\n",
            " 139295/150000: episode: 20293, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [2.000, 8.000],  loss: 0.223228, mae: 13.451686, mean_q: 19.954145\n",
            " 139303/150000: episode: 20294, duration: 0.128s, episode steps:   8, steps per second:  63, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.286058, mae: 13.737621, mean_q: 20.023714\n",
            " 139312/150000: episode: 20295, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.437297, mae: 14.047884, mean_q: 20.109846\n",
            " 139318/150000: episode: 20296, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 2.667 [0.000, 6.000],  loss: 0.204268, mae: 13.365354, mean_q: 20.142023\n",
            " 139326/150000: episode: 20297, duration: 0.098s, episode steps:   8, steps per second:  81, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.500 [0.000, 8.000],  loss: 0.236378, mae: 13.864078, mean_q: 20.067825\n",
            " 139333/150000: episode: 20298, duration: 0.088s, episode steps:   7, steps per second:  80, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.189247, mae: 13.842116, mean_q: 20.217142\n",
            " 139339/150000: episode: 20299, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 2.833 [0.000, 6.000],  loss: 0.461671, mae: 13.772717, mean_q: 20.053606\n",
            " 139346/150000: episode: 20300, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.322640, mae: 13.902423, mean_q: 20.010233\n",
            " 139351/150000: episode: 20301, duration: 0.075s, episode steps:   5, steps per second:  67, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.800 [0.000, 8.000],  loss: 0.232916, mae: 13.840996, mean_q: 20.155674\n",
            " 139360/150000: episode: 20302, duration: 0.120s, episode steps:   9, steps per second:  75, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.387056, mae: 13.847930, mean_q: 20.049967\n",
            " 139365/150000: episode: 20303, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.206221, mae: 13.693108, mean_q: 20.052813\n",
            " 139371/150000: episode: 20304, duration: 0.099s, episode steps:   6, steps per second:  61, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 7.000],  loss: 0.338583, mae: 13.996409, mean_q: 20.055416\n",
            " 139378/150000: episode: 20305, duration: 0.125s, episode steps:   7, steps per second:  56, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.174609, mae: 13.762358, mean_q: 20.159000\n",
            " 139383/150000: episode: 20306, duration: 0.077s, episode steps:   5, steps per second:  65, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.000 [1.000, 7.000],  loss: 0.281883, mae: 14.026255, mean_q: 20.030346\n",
            " 139389/150000: episode: 20307, duration: 0.091s, episode steps:   6, steps per second:  66, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.212311, mae: 13.951870, mean_q: 20.124319\n",
            " 139395/150000: episode: 20308, duration: 0.086s, episode steps:   6, steps per second:  70, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.333 [1.000, 7.000],  loss: 0.205194, mae: 14.131600, mean_q: 20.069338\n",
            " 139404/150000: episode: 20309, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.462529, mae: 13.844625, mean_q: 19.915321\n",
            " 139409/150000: episode: 20310, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.293141, mae: 14.116922, mean_q: 20.211294\n",
            " 139414/150000: episode: 20311, duration: 0.073s, episode steps:   5, steps per second:  68, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.200 [1.000, 5.000],  loss: 0.256417, mae: 13.938784, mean_q: 20.060894\n",
            " 139422/150000: episode: 20312, duration: 0.102s, episode steps:   8, steps per second:  79, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.125 [0.000, 8.000],  loss: 0.621317, mae: 14.089510, mean_q: 20.018887\n",
            " 139430/150000: episode: 20313, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.527181, mae: 13.980256, mean_q: 19.997211\n",
            " 139437/150000: episode: 20314, duration: 0.090s, episode steps:   7, steps per second:  78, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.160907, mae: 13.880678, mean_q: 20.183397\n",
            " 139445/150000: episode: 20315, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.288699, mae: 13.744860, mean_q: 20.142673\n",
            " 139452/150000: episode: 20316, duration: 0.115s, episode steps:   7, steps per second:  61, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.350539, mae: 13.933293, mean_q: 19.797726\n",
            " 139458/150000: episode: 20317, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.229828, mae: 13.600169, mean_q: 20.164156\n",
            " 139464/150000: episode: 20318, duration: 0.084s, episode steps:   6, steps per second:  71, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [0.000, 8.000],  loss: 0.352774, mae: 13.656890, mean_q: 20.071581\n",
            " 139469/150000: episode: 20319, duration: 0.087s, episode steps:   5, steps per second:  57, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.323997, mae: 13.999631, mean_q: 19.775089\n",
            " 139478/150000: episode: 20320, duration: 0.114s, episode steps:   9, steps per second:  79, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.275125, mae: 14.138354, mean_q: 20.131941\n",
            " 139481/150000: episode: 20321, duration: 0.035s, episode steps:   3, steps per second:  85, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 5.667 [5.000, 6.000],  loss: 0.160309, mae: 14.491229, mean_q: 20.206427\n",
            " 139490/150000: episode: 20322, duration: 0.088s, episode steps:   9, steps per second: 103, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.778 [0.000, 8.000],  loss: 0.303069, mae: 13.903514, mean_q: 20.204172\n",
            " 139496/150000: episode: 20323, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.833 [1.000, 8.000],  loss: 0.260905, mae: 13.875257, mean_q: 19.995411\n",
            " 139502/150000: episode: 20324, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.219102, mae: 13.886813, mean_q: 20.078350\n",
            " 139510/150000: episode: 20325, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.414133, mae: 14.102024, mean_q: 20.020550\n",
            " 139518/150000: episode: 20326, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.402483, mae: 13.807302, mean_q: 20.063648\n",
            " 139525/150000: episode: 20327, duration: 0.066s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.199931, mae: 14.060353, mean_q: 20.101641\n",
            " 139534/150000: episode: 20328, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.378989, mae: 14.016960, mean_q: 20.042057\n",
            " 139541/150000: episode: 20329, duration: 0.081s, episode steps:   7, steps per second:  87, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.714 [1.000, 8.000],  loss: 0.289757, mae: 13.688400, mean_q: 20.139143\n",
            " 139548/150000: episode: 20330, duration: 0.076s, episode steps:   7, steps per second:  93, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.143 [0.000, 8.000],  loss: 0.341334, mae: 13.726908, mean_q: 20.080324\n",
            " 139557/150000: episode: 20331, duration: 0.078s, episode steps:   9, steps per second: 116, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.401877, mae: 14.197124, mean_q: 19.951548\n",
            " 139563/150000: episode: 20332, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.478416, mae: 14.167663, mean_q: 19.896019\n",
            " 139571/150000: episode: 20333, duration: 0.080s, episode steps:   8, steps per second:  99, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.419879, mae: 13.691837, mean_q: 20.156322\n",
            " 139577/150000: episode: 20334, duration: 0.055s, episode steps:   6, steps per second: 108, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 2.667 [0.000, 5.000],  loss: 0.482445, mae: 13.799329, mean_q: 20.054665\n",
            " 139583/150000: episode: 20335, duration: 0.057s, episode steps:   6, steps per second: 106, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [1.000, 8.000],  loss: 0.256557, mae: 13.944394, mean_q: 20.148691\n",
            " 139591/150000: episode: 20336, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.250021, mae: 13.714864, mean_q: 20.107555\n",
            " 139596/150000: episode: 20337, duration: 0.047s, episode steps:   5, steps per second: 107, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.474637, mae: 13.647723, mean_q: 20.015537\n",
            " 139602/150000: episode: 20338, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 7.000],  loss: 0.508761, mae: 14.027497, mean_q: 20.034098\n",
            " 139608/150000: episode: 20339, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.672276, mae: 13.817757, mean_q: 20.152805\n",
            " 139616/150000: episode: 20340, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.290065, mae: 13.948591, mean_q: 20.157131\n",
            " 139624/150000: episode: 20341, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.288615, mae: 14.112423, mean_q: 20.135120\n",
            " 139629/150000: episode: 20342, duration: 0.051s, episode steps:   5, steps per second:  99, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [2.000, 6.000],  loss: 0.214072, mae: 14.028193, mean_q: 19.807011\n",
            " 139637/150000: episode: 20343, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.308350, mae: 13.864002, mean_q: 20.176964\n",
            " 139643/150000: episode: 20344, duration: 0.063s, episode steps:   6, steps per second:  95, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 8.000],  loss: 0.395067, mae: 13.982434, mean_q: 19.951639\n",
            " 139647/150000: episode: 20345, duration: 0.050s, episode steps:   4, steps per second:  81, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.750 [3.000, 8.000],  loss: 0.309211, mae: 13.761407, mean_q: 20.160576\n",
            " 139652/150000: episode: 20346, duration: 0.049s, episode steps:   5, steps per second: 101, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.800 [0.000, 8.000],  loss: 0.162201, mae: 14.081693, mean_q: 20.274036\n",
            " 139660/150000: episode: 20347, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.320129, mae: 14.068581, mean_q: 19.825098\n",
            " 139665/150000: episode: 20348, duration: 0.049s, episode steps:   5, steps per second: 102, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.800 [1.000, 4.000],  loss: 0.379592, mae: 14.048864, mean_q: 19.954046\n",
            " 139672/150000: episode: 20349, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.857 [1.000, 7.000],  loss: 0.812762, mae: 13.991500, mean_q: 19.853519\n",
            " 139675/150000: episode: 20350, duration: 0.033s, episode steps:   3, steps per second:  91, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 5.667 [5.000, 7.000],  loss: 0.602042, mae: 13.646634, mean_q: 19.891539\n",
            " 139681/150000: episode: 20351, duration: 0.062s, episode steps:   6, steps per second:  97, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 2.167 [0.000, 5.000],  loss: 0.343918, mae: 14.133342, mean_q: 20.408657\n",
            " 139690/150000: episode: 20352, duration: 0.096s, episode steps:   9, steps per second:  93, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.412411, mae: 13.671953, mean_q: 19.983416\n",
            " 139698/150000: episode: 20353, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.403982, mae: 13.798148, mean_q: 20.085636\n",
            " 139705/150000: episode: 20354, duration: 0.078s, episode steps:   7, steps per second:  90, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.289918, mae: 13.948713, mean_q: 19.984304\n",
            " 139712/150000: episode: 20355, duration: 0.066s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.290160, mae: 13.978331, mean_q: 20.086702\n",
            " 139720/150000: episode: 20356, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.263079, mae: 14.274244, mean_q: 20.071730\n",
            " 139728/150000: episode: 20357, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.258609, mae: 14.044678, mean_q: 20.006586\n",
            " 139734/150000: episode: 20358, duration: 0.064s, episode steps:   6, steps per second:  93, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 7.000],  loss: 0.438278, mae: 13.966647, mean_q: 19.964087\n",
            " 139739/150000: episode: 20359, duration: 0.049s, episode steps:   5, steps per second: 101, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.267362, mae: 13.964006, mean_q: 20.093090\n",
            " 139745/150000: episode: 20360, duration: 0.069s, episode steps:   6, steps per second:  87, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [0.000, 8.000],  loss: 0.317989, mae: 13.903802, mean_q: 20.070251\n",
            " 139753/150000: episode: 20361, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.229218, mae: 13.902792, mean_q: 20.021191\n",
            " 139762/150000: episode: 20362, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.439287, mae: 13.580807, mean_q: 20.089603\n",
            " 139768/150000: episode: 20363, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.718255, mae: 13.677644, mean_q: 19.996565\n",
            " 139775/150000: episode: 20364, duration: 0.078s, episode steps:   7, steps per second:  90, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.341290, mae: 13.968573, mean_q: 20.155567\n",
            " 139781/150000: episode: 20365, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.667 [0.000, 8.000],  loss: 0.183138, mae: 13.966808, mean_q: 20.216993\n",
            " 139786/150000: episode: 20366, duration: 0.049s, episode steps:   5, steps per second: 102, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.600 [2.000, 8.000],  loss: 0.220737, mae: 13.938021, mean_q: 20.036476\n",
            " 139794/150000: episode: 20367, duration: 0.074s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.197121, mae: 13.852589, mean_q: 20.135227\n",
            " 139801/150000: episode: 20368, duration: 0.077s, episode steps:   7, steps per second:  91, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.221973, mae: 13.891091, mean_q: 20.263964\n",
            " 139809/150000: episode: 20369, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.460779, mae: 14.089180, mean_q: 20.145483\n",
            " 139817/150000: episode: 20370, duration: 0.070s, episode steps:   8, steps per second: 114, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.264166, mae: 13.850330, mean_q: 20.011942\n",
            " 139825/150000: episode: 20371, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.225422, mae: 13.872539, mean_q: 20.199791\n",
            " 139833/150000: episode: 20372, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.286360, mae: 13.809956, mean_q: 19.905552\n",
            " 139841/150000: episode: 20373, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.541425, mae: 13.584225, mean_q: 20.097889\n",
            " 139846/150000: episode: 20374, duration: 0.064s, episode steps:   5, steps per second:  79, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.600 [1.000, 7.000],  loss: 0.460332, mae: 13.601193, mean_q: 19.873909\n",
            " 139854/150000: episode: 20375, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.397181, mae: 14.053202, mean_q: 20.232853\n",
            " 139861/150000: episode: 20376, duration: 0.068s, episode steps:   7, steps per second: 103, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.450996, mae: 13.877655, mean_q: 20.056473\n",
            " 139870/150000: episode: 20377, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.427339, mae: 13.681772, mean_q: 19.925117\n",
            " 139875/150000: episode: 20378, duration: 0.049s, episode steps:   5, steps per second: 103, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.145452, mae: 13.868761, mean_q: 20.184296\n",
            " 139884/150000: episode: 20379, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.274493, mae: 13.789079, mean_q: 20.135021\n",
            " 139889/150000: episode: 20380, duration: 0.048s, episode steps:   5, steps per second: 103, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 6.200 [3.000, 8.000],  loss: 0.311368, mae: 13.665152, mean_q: 19.997377\n",
            " 139897/150000: episode: 20381, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.473493, mae: 14.019861, mean_q: 20.141264\n",
            " 139906/150000: episode: 20382, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.568318, mae: 14.184376, mean_q: 20.131042\n",
            " 139914/150000: episode: 20383, duration: 0.074s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.239005, mae: 13.997396, mean_q: 20.110361\n",
            " 139923/150000: episode: 20384, duration: 0.096s, episode steps:   9, steps per second:  93, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.476693, mae: 13.908091, mean_q: 19.935448\n",
            " 139931/150000: episode: 20385, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.259080, mae: 13.943768, mean_q: 20.253233\n",
            " 139937/150000: episode: 20386, duration: 0.060s, episode steps:   6, steps per second: 101, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.242010, mae: 13.933505, mean_q: 20.022110\n",
            " 139942/150000: episode: 20387, duration: 0.049s, episode steps:   5, steps per second: 102, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.400 [2.000, 7.000],  loss: 0.198412, mae: 14.281425, mean_q: 19.944605\n",
            " 139951/150000: episode: 20388, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.406308, mae: 14.058971, mean_q: 19.902706\n",
            " 139960/150000: episode: 20389, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.213488, mae: 13.811571, mean_q: 20.232426\n",
            " 139966/150000: episode: 20390, duration: 0.061s, episode steps:   6, steps per second:  98, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.409482, mae: 13.927552, mean_q: 19.885380\n",
            " 139973/150000: episode: 20391, duration: 0.073s, episode steps:   7, steps per second:  96, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.351916, mae: 13.835648, mean_q: 20.037195\n",
            " 139982/150000: episode: 20392, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.488135, mae: 13.935725, mean_q: 19.947905\n",
            " 139990/150000: episode: 20393, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.284388, mae: 14.300441, mean_q: 20.125484\n",
            " 139999/150000: episode: 20394, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.216141, mae: 14.233653, mean_q: 20.207428\n",
            " 140006/150000: episode: 20395, duration: 0.069s, episode steps:   7, steps per second: 102, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.429 [0.000, 8.000],  loss: 0.392947, mae: 14.388868, mean_q: 19.945118\n",
            " 140012/150000: episode: 20396, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [0.000, 7.000],  loss: 0.216635, mae: 14.121202, mean_q: 20.030355\n",
            " 140020/150000: episode: 20397, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.271835, mae: 13.888591, mean_q: 19.976021\n",
            " 140027/150000: episode: 20398, duration: 0.087s, episode steps:   7, steps per second:  80, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.295994, mae: 14.035867, mean_q: 19.783039\n",
            " 140033/150000: episode: 20399, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [0.000, 8.000],  loss: 0.235425, mae: 14.008129, mean_q: 20.116159\n",
            " 140040/150000: episode: 20400, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.369495, mae: 14.036319, mean_q: 20.190338\n",
            " 140047/150000: episode: 20401, duration: 0.067s, episode steps:   7, steps per second: 104, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.328568, mae: 13.647337, mean_q: 19.852156\n",
            " 140056/150000: episode: 20402, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.187850, mae: 14.253601, mean_q: 20.068922\n",
            " 140065/150000: episode: 20403, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.335407, mae: 14.139553, mean_q: 20.080292\n",
            " 140067/150000: episode: 20404, duration: 0.028s, episode steps:   2, steps per second:  72, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.163933, mae: 13.178577, mean_q: 20.024349\n",
            " 140073/150000: episode: 20405, duration: 0.061s, episode steps:   6, steps per second:  99, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.283975, mae: 13.428707, mean_q: 20.042068\n",
            " 140081/150000: episode: 20406, duration: 0.112s, episode steps:   8, steps per second:  72, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.699199, mae: 13.748107, mean_q: 20.138920\n",
            " 140087/150000: episode: 20407, duration: 0.061s, episode steps:   6, steps per second:  99, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.833 [0.000, 8.000],  loss: 0.587914, mae: 14.027215, mean_q: 20.188728\n",
            " 140095/150000: episode: 20408, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.451031, mae: 13.967123, mean_q: 20.155048\n",
            " 140102/150000: episode: 20409, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.363192, mae: 14.014400, mean_q: 20.028748\n",
            " 140107/150000: episode: 20410, duration: 0.061s, episode steps:   5, steps per second:  82, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.000 [0.000, 8.000],  loss: 0.523853, mae: 13.945514, mean_q: 19.915928\n",
            " 140114/150000: episode: 20411, duration: 0.069s, episode steps:   7, steps per second: 101, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.329518, mae: 14.051758, mean_q: 20.108768\n",
            " 140123/150000: episode: 20412, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.254555, mae: 13.491841, mean_q: 19.961109\n",
            " 140129/150000: episode: 20413, duration: 0.063s, episode steps:   6, steps per second:  96, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [0.000, 7.000],  loss: 0.415878, mae: 14.066864, mean_q: 19.895960\n",
            " 140136/150000: episode: 20414, duration: 0.068s, episode steps:   7, steps per second: 102, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.714 [0.000, 7.000],  loss: 0.264930, mae: 13.536627, mean_q: 19.917145\n",
            " 140143/150000: episode: 20415, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.555208, mae: 13.636697, mean_q: 20.071310\n",
            " 140150/150000: episode: 20416, duration: 0.088s, episode steps:   7, steps per second:  79, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [0.000, 8.000],  loss: 0.308852, mae: 13.412713, mean_q: 20.011078\n",
            " 140156/150000: episode: 20417, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.667 [2.000, 7.000],  loss: 0.258789, mae: 14.118342, mean_q: 20.222237\n",
            " 140164/150000: episode: 20418, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.125 [0.000, 8.000],  loss: 0.344239, mae: 13.953200, mean_q: 19.944298\n",
            " 140172/150000: episode: 20419, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.125 [1.000, 8.000],  loss: 0.247111, mae: 13.983263, mean_q: 20.211071\n",
            " 140178/150000: episode: 20420, duration: 0.069s, episode steps:   6, steps per second:  87, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 7.000],  loss: 0.220389, mae: 13.618773, mean_q: 20.117867\n",
            " 140186/150000: episode: 20421, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.526823, mae: 13.725512, mean_q: 20.143528\n",
            " 140194/150000: episode: 20422, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.420858, mae: 13.616758, mean_q: 19.758530\n",
            " 140200/150000: episode: 20423, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.205491, mae: 14.052733, mean_q: 20.032888\n",
            " 140206/150000: episode: 20424, duration: 0.062s, episode steps:   6, steps per second:  97, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [1.000, 8.000],  loss: 0.546185, mae: 14.117313, mean_q: 20.074358\n",
            " 140212/150000: episode: 20425, duration: 0.060s, episode steps:   6, steps per second:  99, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [1.000, 8.000],  loss: 0.517313, mae: 14.359260, mean_q: 19.937271\n",
            " 140219/150000: episode: 20426, duration: 0.083s, episode steps:   7, steps per second:  84, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.395136, mae: 14.000440, mean_q: 19.946224\n",
            " 140224/150000: episode: 20427, duration: 0.048s, episode steps:   5, steps per second: 103, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.200 [2.000, 8.000],  loss: 0.314244, mae: 13.770673, mean_q: 19.961100\n",
            " 140233/150000: episode: 20428, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.272520, mae: 14.204051, mean_q: 20.357075\n",
            " 140240/150000: episode: 20429, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.641054, mae: 13.863847, mean_q: 19.815769\n",
            " 140248/150000: episode: 20430, duration: 0.094s, episode steps:   8, steps per second:  86, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.350763, mae: 13.886924, mean_q: 20.082556\n",
            " 140256/150000: episode: 20431, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.250 [0.000, 8.000],  loss: 0.603153, mae: 13.952584, mean_q: 19.904160\n",
            " 140261/150000: episode: 20432, duration: 0.049s, episode steps:   5, steps per second: 101, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.200 [1.000, 7.000],  loss: 0.757981, mae: 13.992302, mean_q: 20.228760\n",
            " 140267/150000: episode: 20433, duration: 0.063s, episode steps:   6, steps per second:  95, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.312812, mae: 13.830916, mean_q: 19.963102\n",
            " 140275/150000: episode: 20434, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.748802, mae: 14.083544, mean_q: 20.078941\n",
            " 140284/150000: episode: 20435, duration: 0.080s, episode steps:   9, steps per second: 112, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.778 [0.000, 8.000],  loss: 0.302616, mae: 14.186590, mean_q: 20.132839\n",
            " 140287/150000: episode: 20436, duration: 0.033s, episode steps:   3, steps per second:  92, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 5.000 [3.000, 6.000],  loss: 0.261132, mae: 13.783011, mean_q: 19.938316\n",
            " 140292/150000: episode: 20437, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.000 [0.000, 8.000],  loss: 0.385234, mae: 13.772316, mean_q: 19.772892\n",
            " 140297/150000: episode: 20438, duration: 0.061s, episode steps:   5, steps per second:  82, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.313756, mae: 14.041710, mean_q: 20.092180\n",
            " 140305/150000: episode: 20439, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.250 [0.000, 8.000],  loss: 0.180805, mae: 14.023342, mean_q: 20.002491\n",
            " 140314/150000: episode: 20440, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.717057, mae: 13.989125, mean_q: 20.054050\n",
            " 140322/150000: episode: 20441, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.513230, mae: 13.795657, mean_q: 19.977217\n",
            " 140329/150000: episode: 20442, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.365202, mae: 13.906812, mean_q: 20.083658\n",
            " 140337/150000: episode: 20443, duration: 0.071s, episode steps:   8, steps per second: 112, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.314279, mae: 13.793488, mean_q: 20.088947\n",
            " 140346/150000: episode: 20444, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.277067, mae: 14.185847, mean_q: 20.140036\n",
            " 140353/150000: episode: 20445, duration: 0.074s, episode steps:   7, steps per second:  94, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.289152, mae: 14.097020, mean_q: 20.006710\n",
            " 140358/150000: episode: 20446, duration: 0.051s, episode steps:   5, steps per second:  97, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [2.000, 8.000],  loss: 0.730759, mae: 14.270903, mean_q: 19.928337\n",
            " 140364/150000: episode: 20447, duration: 0.057s, episode steps:   6, steps per second: 106, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.333 [2.000, 8.000],  loss: 0.319842, mae: 13.906079, mean_q: 19.707949\n",
            " 140371/150000: episode: 20448, duration: 0.076s, episode steps:   7, steps per second:  92, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 5.143 [0.000, 8.000],  loss: 0.387679, mae: 13.865515, mean_q: 19.896982\n",
            " 140376/150000: episode: 20449, duration: 0.051s, episode steps:   5, steps per second:  99, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.652409, mae: 14.094991, mean_q: 20.007370\n",
            " 140381/150000: episode: 20450, duration: 0.049s, episode steps:   5, steps per second: 101, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.253128, mae: 13.741094, mean_q: 19.997250\n",
            " 140390/150000: episode: 20451, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.610853, mae: 13.644638, mean_q: 19.754452\n",
            " 140398/150000: episode: 20452, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.264390, mae: 13.962354, mean_q: 20.097454\n",
            " 140405/150000: episode: 20453, duration: 0.070s, episode steps:   7, steps per second: 100, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.771124, mae: 13.744426, mean_q: 19.798201\n",
            " 140414/150000: episode: 20454, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.223304, mae: 13.928061, mean_q: 19.911795\n",
            " 140422/150000: episode: 20455, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 5.000 [1.000, 8.000],  loss: 0.321116, mae: 14.048071, mean_q: 20.018734\n",
            " 140428/150000: episode: 20456, duration: 0.061s, episode steps:   6, steps per second:  98, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 7.000],  loss: 0.417479, mae: 13.850788, mean_q: 20.088465\n",
            " 140434/150000: episode: 20457, duration: 0.056s, episode steps:   6, steps per second: 108, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.500 [1.000, 7.000],  loss: 0.339963, mae: 13.629420, mean_q: 19.856115\n",
            " 140437/150000: episode: 20458, duration: 0.033s, episode steps:   3, steps per second:  90, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 6.333 [5.000, 7.000],  loss: 0.628745, mae: 14.020027, mean_q: 19.862932\n",
            " 140446/150000: episode: 20459, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.308905, mae: 13.910089, mean_q: 19.963860\n",
            " 140454/150000: episode: 20460, duration: 0.112s, episode steps:   8, steps per second:  72, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.490460, mae: 13.889465, mean_q: 19.899553\n",
            " 140462/150000: episode: 20461, duration: 0.119s, episode steps:   8, steps per second:  67, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.246502, mae: 13.668793, mean_q: 20.141230\n",
            " 140467/150000: episode: 20462, duration: 0.075s, episode steps:   5, steps per second:  67, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 8.000],  loss: 0.185885, mae: 13.816153, mean_q: 19.881466\n",
            " 140473/150000: episode: 20463, duration: 0.088s, episode steps:   6, steps per second:  69, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 7.000],  loss: 0.356408, mae: 13.751985, mean_q: 19.938719\n",
            " 140478/150000: episode: 20464, duration: 0.090s, episode steps:   5, steps per second:  56, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.200 [2.000, 7.000],  loss: 0.328812, mae: 13.944928, mean_q: 20.027594\n",
            " 140486/150000: episode: 20465, duration: 0.117s, episode steps:   8, steps per second:  69, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 5.000 [1.000, 8.000],  loss: 0.294563, mae: 13.958285, mean_q: 19.658501\n",
            " 140495/150000: episode: 20466, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.298211, mae: 13.448177, mean_q: 19.972683\n",
            " 140503/150000: episode: 20467, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.348667, mae: 13.980654, mean_q: 19.915550\n",
            " 140509/150000: episode: 20468, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.000 [0.000, 7.000],  loss: 0.224012, mae: 14.088947, mean_q: 19.917862\n",
            " 140518/150000: episode: 20469, duration: 0.126s, episode steps:   9, steps per second:  71, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.343127, mae: 13.860024, mean_q: 20.085522\n",
            " 140525/150000: episode: 20470, duration: 0.100s, episode steps:   7, steps per second:  70, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.429 [1.000, 8.000],  loss: 0.239333, mae: 13.731843, mean_q: 19.885227\n",
            " 140531/150000: episode: 20471, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.221977, mae: 13.909379, mean_q: 19.967289\n",
            " 140538/150000: episode: 20472, duration: 0.094s, episode steps:   7, steps per second:  75, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.335901, mae: 13.825377, mean_q: 19.901861\n",
            " 140547/150000: episode: 20473, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.351112, mae: 14.166436, mean_q: 20.083576\n",
            " 140555/150000: episode: 20474, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.498171, mae: 14.055500, mean_q: 19.868193\n",
            " 140564/150000: episode: 20475, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.712596, mae: 13.805265, mean_q: 19.778599\n",
            " 140573/150000: episode: 20476, duration: 0.124s, episode steps:   9, steps per second:  73, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.286072, mae: 14.075788, mean_q: 20.162188\n",
            " 140582/150000: episode: 20477, duration: 0.137s, episode steps:   9, steps per second:  66, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.363519, mae: 13.920475, mean_q: 19.789476\n",
            " 140588/150000: episode: 20478, duration: 0.096s, episode steps:   6, steps per second:  62, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [0.000, 7.000],  loss: 0.221086, mae: 14.267486, mean_q: 20.156967\n",
            " 140596/150000: episode: 20479, duration: 0.134s, episode steps:   8, steps per second:  60, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.221784, mae: 13.772713, mean_q: 19.806404\n",
            " 140605/150000: episode: 20480, duration: 0.148s, episode steps:   9, steps per second:  61, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.556 [1.000, 8.000],  loss: 0.375590, mae: 14.137911, mean_q: 20.033424\n",
            " 140612/150000: episode: 20481, duration: 0.101s, episode steps:   7, steps per second:  69, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.571 [0.000, 8.000],  loss: 0.242701, mae: 13.587916, mean_q: 19.746042\n",
            " 140617/150000: episode: 20482, duration: 0.079s, episode steps:   5, steps per second:  63, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.800 [0.000, 8.000],  loss: 0.396274, mae: 13.567724, mean_q: 19.780062\n",
            " 140622/150000: episode: 20483, duration: 0.080s, episode steps:   5, steps per second:  63, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.800 [0.000, 8.000],  loss: 0.250783, mae: 14.385661, mean_q: 20.098898\n",
            " 140629/150000: episode: 20484, duration: 0.112s, episode steps:   7, steps per second:  63, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.555284, mae: 14.020320, mean_q: 19.779123\n",
            " 140633/150000: episode: 20485, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 1.500 [0.000, 4.000],  loss: 0.187246, mae: 14.265219, mean_q: 20.004772\n",
            " 140641/150000: episode: 20486, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.875 [1.000, 7.000],  loss: 0.492311, mae: 14.076658, mean_q: 19.876240\n",
            " 140647/150000: episode: 20487, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 7.000],  loss: 0.477994, mae: 13.790866, mean_q: 20.052652\n",
            " 140655/150000: episode: 20488, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.646097, mae: 13.537080, mean_q: 19.984379\n",
            " 140661/150000: episode: 20489, duration: 0.097s, episode steps:   6, steps per second:  62, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 6.000],  loss: 0.289299, mae: 13.867326, mean_q: 19.863001\n",
            " 140668/150000: episode: 20490, duration: 0.104s, episode steps:   7, steps per second:  67, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.302872, mae: 13.623363, mean_q: 19.827986\n",
            " 140675/150000: episode: 20491, duration: 0.102s, episode steps:   7, steps per second:  68, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.714 [1.000, 8.000],  loss: 0.285947, mae: 13.708940, mean_q: 19.698711\n",
            " 140681/150000: episode: 20492, duration: 0.105s, episode steps:   6, steps per second:  57, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.500 [2.000, 8.000],  loss: 0.255634, mae: 14.019811, mean_q: 20.002636\n",
            " 140685/150000: episode: 20493, duration: 0.042s, episode steps:   4, steps per second:  96, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 2.250 [0.000, 7.000],  loss: 0.255371, mae: 14.394012, mean_q: 19.831491\n",
            " 140688/150000: episode: 20494, duration: 0.034s, episode steps:   3, steps per second:  88, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 6.667 [6.000, 7.000],  loss: 0.230808, mae: 13.706184, mean_q: 19.648581\n",
            " 140695/150000: episode: 20495, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 5.143 [0.000, 8.000],  loss: 0.224657, mae: 14.028304, mean_q: 19.979885\n",
            " 140702/150000: episode: 20496, duration: 0.066s, episode steps:   7, steps per second: 107, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.714 [0.000, 7.000],  loss: 0.156257, mae: 13.764363, mean_q: 19.988892\n",
            " 140706/150000: episode: 20497, duration: 0.040s, episode steps:   4, steps per second:  99, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 2.000 [0.000, 3.000],  loss: 0.272384, mae: 13.846175, mean_q: 19.837654\n",
            " 140711/150000: episode: 20498, duration: 0.048s, episode steps:   5, steps per second: 105, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.000 [0.000, 5.000],  loss: 0.216346, mae: 13.634320, mean_q: 19.850405\n",
            " 140717/150000: episode: 20499, duration: 0.064s, episode steps:   6, steps per second:  94, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 8.000],  loss: 0.170539, mae: 14.024826, mean_q: 20.000742\n",
            " 140723/150000: episode: 20500, duration: 0.068s, episode steps:   6, steps per second:  88, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [1.000, 8.000],  loss: 0.162819, mae: 14.278736, mean_q: 19.941858\n",
            " 140729/150000: episode: 20501, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [0.000, 7.000],  loss: 0.186409, mae: 14.027507, mean_q: 19.891819\n",
            " 140734/150000: episode: 20502, duration: 0.048s, episode steps:   5, steps per second: 104, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.800 [0.000, 8.000],  loss: 0.496231, mae: 13.933943, mean_q: 20.041668\n",
            " 140742/150000: episode: 20503, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.256683, mae: 14.006807, mean_q: 19.908009\n",
            " 140751/150000: episode: 20504, duration: 0.080s, episode steps:   9, steps per second: 112, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.312331, mae: 14.097843, mean_q: 19.911608\n",
            " 140759/150000: episode: 20505, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.576259, mae: 14.064337, mean_q: 19.871204\n",
            " 140768/150000: episode: 20506, duration: 0.096s, episode steps:   9, steps per second:  93, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.299731, mae: 13.824054, mean_q: 19.977837\n",
            " 140773/150000: episode: 20507, duration: 0.049s, episode steps:   5, steps per second: 101, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.800 [0.000, 8.000],  loss: 0.255539, mae: 14.227028, mean_q: 19.953592\n",
            " 140777/150000: episode: 20508, duration: 0.044s, episode steps:   4, steps per second:  91, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.750 [4.000, 7.000],  loss: 0.481825, mae: 13.303498, mean_q: 20.162207\n",
            " 140785/150000: episode: 20509, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.344054, mae: 13.893665, mean_q: 19.725594\n",
            " 140793/150000: episode: 20510, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.591539, mae: 13.858353, mean_q: 20.099648\n",
            " 140800/150000: episode: 20511, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.681710, mae: 14.118526, mean_q: 19.889471\n",
            " 140806/150000: episode: 20512, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.500 [2.000, 8.000],  loss: 0.369616, mae: 13.976428, mean_q: 19.814631\n",
            " 140813/150000: episode: 20513, duration: 0.071s, episode steps:   7, steps per second:  99, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.475304, mae: 13.772378, mean_q: 19.992308\n",
            " 140817/150000: episode: 20514, duration: 0.059s, episode steps:   4, steps per second:  67, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 2.250 [0.000, 8.000],  loss: 0.479686, mae: 13.950060, mean_q: 20.082117\n",
            " 140824/150000: episode: 20515, duration: 0.074s, episode steps:   7, steps per second:  95, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.429 [0.000, 7.000],  loss: 0.280385, mae: 13.736982, mean_q: 19.919935\n",
            " 140832/150000: episode: 20516, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.528832, mae: 14.017043, mean_q: 20.109421\n",
            " 140841/150000: episode: 20517, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.662507, mae: 14.025469, mean_q: 19.754753\n",
            " 140849/150000: episode: 20518, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.309068, mae: 13.993198, mean_q: 19.923523\n",
            " 140855/150000: episode: 20519, duration: 0.064s, episode steps:   6, steps per second:  94, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [0.000, 7.000],  loss: 0.438211, mae: 13.752758, mean_q: 19.915564\n",
            " 140864/150000: episode: 20520, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.549035, mae: 13.922375, mean_q: 19.965269\n",
            " 140872/150000: episode: 20521, duration: 0.074s, episode steps:   8, steps per second: 109, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.750 [0.000, 8.000],  loss: 0.187326, mae: 13.837936, mean_q: 19.967205\n",
            " 140879/150000: episode: 20522, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.358291, mae: 13.859933, mean_q: 19.822678\n",
            " 140882/150000: episode: 20523, duration: 0.033s, episode steps:   3, steps per second:  91, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.205281, mae: 13.776073, mean_q: 19.677757\n",
            " 140891/150000: episode: 20524, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.259498, mae: 13.904540, mean_q: 20.032648\n",
            " 140899/150000: episode: 20525, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.392283, mae: 13.663834, mean_q: 19.757874\n",
            " 140907/150000: episode: 20526, duration: 0.071s, episode steps:   8, steps per second: 112, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.233153, mae: 13.723766, mean_q: 20.083843\n",
            " 140913/150000: episode: 20527, duration: 0.057s, episode steps:   6, steps per second: 106, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.220112, mae: 13.476529, mean_q: 19.803915\n",
            " 140921/150000: episode: 20528, duration: 0.088s, episode steps:   8, steps per second:  90, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.250 [0.000, 8.000],  loss: 0.499281, mae: 14.047488, mean_q: 20.025642\n",
            " 140928/150000: episode: 20529, duration: 0.067s, episode steps:   7, steps per second: 104, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.328977, mae: 13.683873, mean_q: 19.773428\n",
            " 140937/150000: episode: 20530, duration: 0.094s, episode steps:   9, steps per second:  95, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.324971, mae: 14.261006, mean_q: 20.253693\n",
            " 140942/150000: episode: 20531, duration: 0.048s, episode steps:   5, steps per second: 104, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.600 [1.000, 6.000],  loss: 0.815808, mae: 13.254465, mean_q: 19.787281\n",
            " 140950/150000: episode: 20532, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.576615, mae: 14.239490, mean_q: 20.069183\n",
            " 140955/150000: episode: 20533, duration: 0.057s, episode steps:   5, steps per second:  88, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 1.600 [0.000, 5.000],  loss: 0.392930, mae: 13.976560, mean_q: 19.860455\n",
            " 140964/150000: episode: 20534, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.294816, mae: 14.005927, mean_q: 20.060896\n",
            " 140971/150000: episode: 20535, duration: 0.069s, episode steps:   7, steps per second: 102, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 7.000],  loss: 0.201089, mae: 13.603875, mean_q: 19.994995\n",
            " 140976/150000: episode: 20536, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.200 [0.000, 8.000],  loss: 0.688835, mae: 13.533804, mean_q: 20.084354\n",
            " 140983/150000: episode: 20537, duration: 0.073s, episode steps:   7, steps per second:  95, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.571 [0.000, 8.000],  loss: 0.340012, mae: 14.020567, mean_q: 19.980181\n",
            " 140990/150000: episode: 20538, duration: 0.083s, episode steps:   7, steps per second:  84, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.624405, mae: 13.806266, mean_q: 19.880102\n",
            " 140998/150000: episode: 20539, duration: 0.072s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.355719, mae: 13.759859, mean_q: 19.905472\n",
            " 141006/150000: episode: 20540, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.776023, mae: 13.942379, mean_q: 20.072712\n",
            " 141013/150000: episode: 20541, duration: 0.074s, episode steps:   7, steps per second:  95, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.295474, mae: 13.817649, mean_q: 19.841379\n",
            " 141018/150000: episode: 20542, duration: 0.053s, episode steps:   5, steps per second:  94, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.800 [0.000, 8.000],  loss: 0.287901, mae: 13.888494, mean_q: 20.093832\n",
            " 141026/150000: episode: 20543, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.233216, mae: 13.930010, mean_q: 19.925476\n",
            " 141032/150000: episode: 20544, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.500 [0.000, 8.000],  loss: 0.268378, mae: 13.544079, mean_q: 19.699440\n",
            " 141041/150000: episode: 20545, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.225182, mae: 14.008925, mean_q: 20.075397\n",
            " 141049/150000: episode: 20546, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.125 [0.000, 8.000],  loss: 0.226303, mae: 13.638075, mean_q: 19.798725\n",
            " 141053/150000: episode: 20547, duration: 0.054s, episode steps:   4, steps per second:  74, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.750 [2.000, 7.000],  loss: 0.663290, mae: 14.178905, mean_q: 19.983137\n",
            " 141061/150000: episode: 20548, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.529766, mae: 13.713398, mean_q: 19.841080\n",
            " 141070/150000: episode: 20549, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.222 [0.000, 8.000],  loss: 0.474851, mae: 13.354959, mean_q: 19.968279\n",
            " 141078/150000: episode: 20550, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.542994, mae: 13.730759, mean_q: 19.705175\n",
            " 141087/150000: episode: 20551, duration: 0.110s, episode steps:   9, steps per second:  82, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.604452, mae: 13.896932, mean_q: 20.105598\n",
            " 141096/150000: episode: 20552, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.764632, mae: 13.660870, mean_q: 19.809502\n",
            " 141100/150000: episode: 20553, duration: 0.041s, episode steps:   4, steps per second:  98, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.250 [1.000, 8.000],  loss: 0.257613, mae: 13.504605, mean_q: 20.050987\n",
            " 141106/150000: episode: 20554, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.260557, mae: 13.859692, mean_q: 19.849440\n",
            " 141110/150000: episode: 20555, duration: 0.053s, episode steps:   4, steps per second:  76, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.000 [0.000, 6.000],  loss: 0.556607, mae: 13.578007, mean_q: 19.962917\n",
            " 141116/150000: episode: 20556, duration: 0.060s, episode steps:   6, steps per second:  99, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.275192, mae: 13.587090, mean_q: 19.882925\n",
            " 141122/150000: episode: 20557, duration: 0.061s, episode steps:   6, steps per second:  98, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.258082, mae: 13.944725, mean_q: 19.898804\n",
            " 141130/150000: episode: 20558, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.309534, mae: 13.690670, mean_q: 19.904697\n",
            " 141137/150000: episode: 20559, duration: 0.078s, episode steps:   7, steps per second:  90, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.292561, mae: 14.049498, mean_q: 20.099169\n",
            " 141146/150000: episode: 20560, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.216486, mae: 13.907651, mean_q: 19.954174\n",
            " 141154/150000: episode: 20561, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.307829, mae: 14.255754, mean_q: 19.879604\n",
            " 141162/150000: episode: 20562, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.169977, mae: 14.108712, mean_q: 20.072159\n",
            " 141168/150000: episode: 20563, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.500 [2.000, 8.000],  loss: 0.310888, mae: 13.842435, mean_q: 19.972279\n",
            " 141171/150000: episode: 20564, duration: 0.032s, episode steps:   3, steps per second:  94, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 2.667 [0.000, 8.000],  loss: 0.241005, mae: 13.559444, mean_q: 19.939157\n",
            " 141180/150000: episode: 20565, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.411560, mae: 13.903129, mean_q: 20.039949\n",
            " 141188/150000: episode: 20566, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.344586, mae: 14.056826, mean_q: 20.103336\n",
            " 141195/150000: episode: 20567, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 5.143 [1.000, 8.000],  loss: 0.231058, mae: 13.990324, mean_q: 20.039087\n",
            " 141202/150000: episode: 20568, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 2.571 [0.000, 5.000],  loss: 0.217611, mae: 13.726626, mean_q: 19.951273\n",
            " 141209/150000: episode: 20569, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.143 [0.000, 8.000],  loss: 0.738560, mae: 13.885160, mean_q: 19.877163\n",
            " 141211/150000: episode: 20570, duration: 0.031s, episode steps:   2, steps per second:  64, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 6.000 [6.000, 6.000],  loss: 0.548050, mae: 13.698563, mean_q: 19.651155\n",
            " 141217/150000: episode: 20571, duration: 0.063s, episode steps:   6, steps per second:  95, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 6.000],  loss: 0.263445, mae: 13.897152, mean_q: 19.835966\n",
            " 141224/150000: episode: 20572, duration: 0.062s, episode steps:   7, steps per second: 112, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 7.000],  loss: 0.601050, mae: 14.134940, mean_q: 20.248045\n",
            " 141230/150000: episode: 20573, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.724506, mae: 14.016170, mean_q: 19.744751\n",
            " 141235/150000: episode: 20574, duration: 0.064s, episode steps:   5, steps per second:  78, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.375446, mae: 13.882297, mean_q: 20.014969\n",
            " 141243/150000: episode: 20575, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.321333, mae: 13.790259, mean_q: 20.247803\n",
            " 141251/150000: episode: 20576, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.431536, mae: 13.430203, mean_q: 19.794153\n",
            " 141258/150000: episode: 20577, duration: 0.082s, episode steps:   7, steps per second:  86, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.342425, mae: 13.688064, mean_q: 19.861143\n",
            " 141267/150000: episode: 20578, duration: 0.084s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.433882, mae: 13.742290, mean_q: 19.934549\n",
            " 141274/150000: episode: 20579, duration: 0.071s, episode steps:   7, steps per second:  98, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.488256, mae: 13.751500, mean_q: 20.071043\n",
            " 141280/150000: episode: 20580, duration: 0.073s, episode steps:   6, steps per second:  83, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.696770, mae: 13.742470, mean_q: 19.951834\n",
            " 141287/150000: episode: 20581, duration: 0.071s, episode steps:   7, steps per second:  98, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.429 [0.000, 8.000],  loss: 0.243846, mae: 13.579427, mean_q: 19.948887\n",
            " 141295/150000: episode: 20582, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.358870, mae: 13.975410, mean_q: 19.972187\n",
            " 141302/150000: episode: 20583, duration: 0.076s, episode steps:   7, steps per second:  92, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.213353, mae: 14.066591, mean_q: 20.123737\n",
            " 141310/150000: episode: 20584, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.200819, mae: 13.877024, mean_q: 19.758183\n",
            " 141317/150000: episode: 20585, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 5.286 [2.000, 8.000],  loss: 0.714343, mae: 14.042181, mean_q: 19.986490\n",
            " 141325/150000: episode: 20586, duration: 0.081s, episode steps:   8, steps per second:  98, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.500 [0.000, 8.000],  loss: 0.376488, mae: 14.051735, mean_q: 20.045132\n",
            " 141333/150000: episode: 20587, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.547850, mae: 13.765944, mean_q: 20.127577\n",
            " 141341/150000: episode: 20588, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.522476, mae: 13.910670, mean_q: 19.738922\n",
            " 141349/150000: episode: 20589, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.452407, mae: 13.878461, mean_q: 20.021137\n",
            " 141355/150000: episode: 20590, duration: 0.063s, episode steps:   6, steps per second:  95, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.310284, mae: 13.777634, mean_q: 19.951717\n",
            " 141363/150000: episode: 20591, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.187990, mae: 13.821697, mean_q: 19.961752\n",
            " 141372/150000: episode: 20592, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.332366, mae: 13.658690, mean_q: 19.879091\n",
            " 141381/150000: episode: 20593, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.189140, mae: 13.936661, mean_q: 20.029091\n",
            " 141387/150000: episode: 20594, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.227878, mae: 13.332230, mean_q: 19.848595\n",
            " 141396/150000: episode: 20595, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.255340, mae: 13.819852, mean_q: 19.859909\n",
            " 141402/150000: episode: 20596, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [1.000, 7.000],  loss: 0.208957, mae: 13.660380, mean_q: 19.825882\n",
            " 141411/150000: episode: 20597, duration: 0.080s, episode steps:   9, steps per second: 112, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.215129, mae: 13.784892, mean_q: 19.845715\n",
            " 141416/150000: episode: 20598, duration: 0.051s, episode steps:   5, steps per second:  98, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.400 [3.000, 8.000],  loss: 0.345224, mae: 13.733040, mean_q: 19.957478\n",
            " 141423/150000: episode: 20599, duration: 0.078s, episode steps:   7, steps per second:  90, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.571 [0.000, 8.000],  loss: 0.224513, mae: 13.847106, mean_q: 19.979221\n",
            " 141428/150000: episode: 20600, duration: 0.046s, episode steps:   5, steps per second: 110, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.600 [2.000, 8.000],  loss: 0.242019, mae: 13.935114, mean_q: 20.008678\n",
            " 141434/150000: episode: 20601, duration: 0.054s, episode steps:   6, steps per second: 110, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [2.000, 8.000],  loss: 0.196856, mae: 13.967753, mean_q: 20.078382\n",
            " 141442/150000: episode: 20602, duration: 0.070s, episode steps:   8, steps per second: 115, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.875 [0.000, 8.000],  loss: 0.261206, mae: 13.895556, mean_q: 19.963799\n",
            " 141448/150000: episode: 20603, duration: 0.067s, episode steps:   6, steps per second:  89, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [1.000, 8.000],  loss: 0.165438, mae: 13.955745, mean_q: 20.110224\n",
            " 141457/150000: episode: 20604, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.504168, mae: 13.598858, mean_q: 19.801456\n",
            " 141464/150000: episode: 20605, duration: 0.063s, episode steps:   7, steps per second: 111, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 7.000],  loss: 0.210974, mae: 14.196402, mean_q: 20.001678\n",
            " 141473/150000: episode: 20606, duration: 0.093s, episode steps:   9, steps per second:  96, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.347913, mae: 13.646606, mean_q: 19.864532\n",
            " 141482/150000: episode: 20607, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.667 [0.000, 8.000],  loss: 0.438804, mae: 13.948262, mean_q: 19.974440\n",
            " 141491/150000: episode: 20608, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.390325, mae: 13.853100, mean_q: 20.077284\n",
            " 141495/150000: episode: 20609, duration: 0.046s, episode steps:   4, steps per second:  88, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.000 [3.000, 8.000],  loss: 0.176078, mae: 13.653481, mean_q: 19.873352\n",
            " 141504/150000: episode: 20610, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.225207, mae: 13.982113, mean_q: 19.933123\n",
            " 141511/150000: episode: 20611, duration: 0.072s, episode steps:   7, steps per second:  97, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.399347, mae: 13.788954, mean_q: 19.871078\n",
            " 141519/150000: episode: 20612, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.175383, mae: 14.360374, mean_q: 20.135290\n",
            " 141525/150000: episode: 20613, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.000 [2.000, 8.000],  loss: 0.467093, mae: 13.953209, mean_q: 19.587437\n",
            " 141532/150000: episode: 20614, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.143 [1.000, 8.000],  loss: 0.431665, mae: 13.929262, mean_q: 20.068506\n",
            " 141534/150000: episode: 20615, duration: 0.025s, episode steps:   2, steps per second:  82, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 4.000 [4.000, 4.000],  loss: 0.227741, mae: 13.776608, mean_q: 20.098972\n",
            " 141542/150000: episode: 20616, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.306772, mae: 13.856331, mean_q: 19.989637\n",
            " 141550/150000: episode: 20617, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.723952, mae: 13.795961, mean_q: 19.844603\n",
            " 141559/150000: episode: 20618, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.521383, mae: 14.076145, mean_q: 19.967348\n",
            " 141565/150000: episode: 20619, duration: 0.072s, episode steps:   6, steps per second:  83, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [1.000, 7.000],  loss: 0.488646, mae: 13.881728, mean_q: 19.964483\n",
            " 141573/150000: episode: 20620, duration: 0.084s, episode steps:   8, steps per second:  96, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.494497, mae: 13.701754, mean_q: 19.996933\n",
            " 141581/150000: episode: 20621, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.472622, mae: 13.811741, mean_q: 19.774626\n",
            " 141588/150000: episode: 20622, duration: 0.080s, episode steps:   7, steps per second:  87, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.528588, mae: 14.282914, mean_q: 20.100403\n",
            " 141596/150000: episode: 20623, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.625 [0.000, 8.000],  loss: 0.335221, mae: 14.002991, mean_q: 20.264769\n",
            " 141602/150000: episode: 20624, duration: 0.055s, episode steps:   6, steps per second: 110, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.260092, mae: 14.168667, mean_q: 20.022974\n",
            " 141608/150000: episode: 20625, duration: 0.056s, episode steps:   6, steps per second: 108, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 6.000],  loss: 0.286093, mae: 14.068973, mean_q: 19.775370\n",
            " 141615/150000: episode: 20626, duration: 0.074s, episode steps:   7, steps per second:  95, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.187077, mae: 13.724666, mean_q: 20.023996\n",
            " 141622/150000: episode: 20627, duration: 0.066s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.184564, mae: 13.866659, mean_q: 20.047144\n",
            " 141628/150000: episode: 20628, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.333 [2.000, 8.000],  loss: 0.186990, mae: 13.890792, mean_q: 19.871798\n",
            " 141635/150000: episode: 20629, duration: 0.063s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.435631, mae: 13.915651, mean_q: 20.025156\n",
            " 141640/150000: episode: 20630, duration: 0.060s, episode steps:   5, steps per second:  84, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.800 [0.000, 8.000],  loss: 0.265901, mae: 14.018809, mean_q: 19.926609\n",
            " 141645/150000: episode: 20631, duration: 0.048s, episode steps:   5, steps per second: 103, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.200 [0.000, 7.000],  loss: 0.643113, mae: 13.921188, mean_q: 20.014429\n",
            " 141651/150000: episode: 20632, duration: 0.055s, episode steps:   6, steps per second: 108, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [1.000, 7.000],  loss: 0.550576, mae: 13.883720, mean_q: 19.878416\n",
            " 141660/150000: episode: 20633, duration: 0.116s, episode steps:   9, steps per second:  78, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.490357, mae: 13.794235, mean_q: 19.999699\n",
            " 141667/150000: episode: 20634, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.207233, mae: 13.890437, mean_q: 20.014614\n",
            " 141674/150000: episode: 20635, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.278586, mae: 13.804583, mean_q: 19.913897\n",
            " 141679/150000: episode: 20636, duration: 0.093s, episode steps:   5, steps per second:  54, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.135088, mae: 13.810458, mean_q: 19.964634\n",
            " 141688/150000: episode: 20637, duration: 0.121s, episode steps:   9, steps per second:  74, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.347503, mae: 14.127502, mean_q: 20.152582\n",
            " 141691/150000: episode: 20638, duration: 0.046s, episode steps:   3, steps per second:  65, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 4.333 [4.000, 5.000],  loss: 0.167223, mae: 14.289765, mean_q: 19.971975\n",
            " 141699/150000: episode: 20639, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.233217, mae: 14.301189, mean_q: 19.973078\n",
            " 141707/150000: episode: 20640, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.213978, mae: 14.121265, mean_q: 20.006599\n",
            " 141712/150000: episode: 20641, duration: 0.067s, episode steps:   5, steps per second:  74, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [2.000, 6.000],  loss: 0.276910, mae: 14.140370, mean_q: 19.901520\n",
            " 141721/150000: episode: 20642, duration: 0.121s, episode steps:   9, steps per second:  75, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.503096, mae: 14.217758, mean_q: 19.839684\n",
            " 141729/150000: episode: 20643, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.750 [0.000, 8.000],  loss: 0.584746, mae: 13.756962, mean_q: 20.099617\n",
            " 141735/150000: episode: 20644, duration: 0.087s, episode steps:   6, steps per second:  69, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.333 [0.000, 8.000],  loss: 0.378448, mae: 13.730588, mean_q: 19.668266\n",
            " 141742/150000: episode: 20645, duration: 0.107s, episode steps:   7, steps per second:  66, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.272535, mae: 13.808037, mean_q: 20.083866\n",
            " 141748/150000: episode: 20646, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.428303, mae: 14.000129, mean_q: 19.895863\n",
            " 141754/150000: episode: 20647, duration: 0.084s, episode steps:   6, steps per second:  71, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.250907, mae: 13.437284, mean_q: 19.952421\n",
            " 141761/150000: episode: 20648, duration: 0.111s, episode steps:   7, steps per second:  63, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.273635, mae: 13.777247, mean_q: 19.918562\n",
            " 141769/150000: episode: 20649, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.412865, mae: 13.872235, mean_q: 19.887886\n",
            " 141777/150000: episode: 20650, duration: 0.124s, episode steps:   8, steps per second:  65, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.371467, mae: 13.581298, mean_q: 19.850830\n",
            " 141784/150000: episode: 20651, duration: 0.108s, episode steps:   7, steps per second:  65, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.143 [0.000, 6.000],  loss: 0.211881, mae: 13.908792, mean_q: 19.982351\n",
            " 141791/150000: episode: 20652, duration: 0.104s, episode steps:   7, steps per second:  67, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.187916, mae: 14.088964, mean_q: 19.905657\n",
            " 141797/150000: episode: 20653, duration: 0.099s, episode steps:   6, steps per second:  60, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.667 [1.000, 8.000],  loss: 0.354828, mae: 14.343953, mean_q: 19.830521\n",
            " 141804/150000: episode: 20654, duration: 0.124s, episode steps:   7, steps per second:  57, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 5.429 [0.000, 8.000],  loss: 0.348182, mae: 13.753295, mean_q: 19.931044\n",
            " 141808/150000: episode: 20655, duration: 0.063s, episode steps:   4, steps per second:  64, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 2.500 [0.000, 7.000],  loss: 0.322339, mae: 13.496134, mean_q: 19.972775\n",
            " 141815/150000: episode: 20656, duration: 0.101s, episode steps:   7, steps per second:  70, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.435605, mae: 14.060279, mean_q: 20.033871\n",
            " 141820/150000: episode: 20657, duration: 0.075s, episode steps:   5, steps per second:  67, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.280433, mae: 13.916586, mean_q: 20.060547\n",
            " 141828/150000: episode: 20658, duration: 0.105s, episode steps:   8, steps per second:  76, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.318282, mae: 13.577052, mean_q: 19.975811\n",
            " 141835/150000: episode: 20659, duration: 0.098s, episode steps:   7, steps per second:  72, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.286 [0.000, 8.000],  loss: 0.287566, mae: 13.717699, mean_q: 19.789064\n",
            " 141843/150000: episode: 20660, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.548018, mae: 13.833315, mean_q: 19.951641\n",
            " 141848/150000: episode: 20661, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [1.000, 8.000],  loss: 0.593765, mae: 13.829636, mean_q: 19.701376\n",
            " 141854/150000: episode: 20662, duration: 0.085s, episode steps:   6, steps per second:  71, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [1.000, 8.000],  loss: 0.419528, mae: 13.703518, mean_q: 20.238840\n",
            " 141862/150000: episode: 20663, duration: 0.112s, episode steps:   8, steps per second:  72, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.875 [1.000, 8.000],  loss: 0.307612, mae: 13.700821, mean_q: 19.812391\n",
            " 141871/150000: episode: 20664, duration: 0.124s, episode steps:   9, steps per second:  73, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.368852, mae: 13.846565, mean_q: 20.073139\n",
            " 141876/150000: episode: 20665, duration: 0.084s, episode steps:   5, steps per second:  59, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 8.000],  loss: 0.307018, mae: 13.842473, mean_q: 19.866529\n",
            " 141881/150000: episode: 20666, duration: 0.081s, episode steps:   5, steps per second:  62, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [2.000, 6.000],  loss: 0.594731, mae: 14.027522, mean_q: 19.783796\n",
            " 141888/150000: episode: 20667, duration: 0.121s, episode steps:   7, steps per second:  58, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.571 [1.000, 8.000],  loss: 0.273890, mae: 14.152725, mean_q: 20.002066\n",
            " 141894/150000: episode: 20668, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [0.000, 7.000],  loss: 0.274984, mae: 13.935848, mean_q: 19.903358\n",
            " 141902/150000: episode: 20669, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.383356, mae: 13.937048, mean_q: 19.807770\n",
            " 141911/150000: episode: 20670, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.237324, mae: 13.746210, mean_q: 19.940350\n",
            " 141919/150000: episode: 20671, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.185204, mae: 14.117792, mean_q: 20.034370\n",
            " 141926/150000: episode: 20672, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.184458, mae: 13.671753, mean_q: 19.922077\n",
            " 141933/150000: episode: 20673, duration: 0.071s, episode steps:   7, steps per second:  99, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 7.000],  loss: 0.284511, mae: 13.814659, mean_q: 19.949209\n",
            " 141938/150000: episode: 20674, duration: 0.055s, episode steps:   5, steps per second:  90, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.400 [0.000, 6.000],  loss: 0.217274, mae: 14.087222, mean_q: 19.724730\n",
            " 141947/150000: episode: 20675, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.475638, mae: 13.842950, mean_q: 19.946640\n",
            " 141956/150000: episode: 20676, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.401210, mae: 13.898721, mean_q: 19.844887\n",
            " 141964/150000: episode: 20677, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.415660, mae: 13.925020, mean_q: 20.008539\n",
            " 141971/150000: episode: 20678, duration: 0.075s, episode steps:   7, steps per second:  94, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.278272, mae: 13.759643, mean_q: 20.045856\n",
            " 141980/150000: episode: 20679, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.328758, mae: 13.903514, mean_q: 20.025520\n",
            " 141989/150000: episode: 20680, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.283685, mae: 13.907604, mean_q: 19.858534\n",
            " 141992/150000: episode: 20681, duration: 0.032s, episode steps:   3, steps per second:  92, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 1.667 [1.000, 2.000],  loss: 0.264489, mae: 13.333764, mean_q: 20.012091\n",
            " 142001/150000: episode: 20682, duration: 0.082s, episode steps:   9, steps per second: 109, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.667 [0.000, 8.000],  loss: 0.336939, mae: 13.912062, mean_q: 19.816177\n",
            " 142010/150000: episode: 20683, duration: 0.096s, episode steps:   9, steps per second:  93, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.165333, mae: 14.093469, mean_q: 20.064125\n",
            " 142018/150000: episode: 20684, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.293703, mae: 14.282884, mean_q: 19.970016\n",
            " 142027/150000: episode: 20685, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.430261, mae: 13.663277, mean_q: 19.893749\n",
            " 142035/150000: episode: 20686, duration: 0.090s, episode steps:   8, steps per second:  89, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.875 [0.000, 7.000],  loss: 0.641739, mae: 13.877637, mean_q: 19.878342\n",
            " 142042/150000: episode: 20687, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.274471, mae: 14.002317, mean_q: 19.989862\n",
            " 142049/150000: episode: 20688, duration: 0.069s, episode steps:   7, steps per second: 101, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.290919, mae: 13.848308, mean_q: 20.002087\n",
            " 142056/150000: episode: 20689, duration: 0.071s, episode steps:   7, steps per second:  99, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.499494, mae: 13.849120, mean_q: 19.911947\n",
            " 142064/150000: episode: 20690, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.266703, mae: 13.884291, mean_q: 19.876211\n",
            " 142071/150000: episode: 20691, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.221850, mae: 13.799329, mean_q: 19.998951\n",
            " 142079/150000: episode: 20692, duration: 0.091s, episode steps:   8, steps per second:  87, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.403888, mae: 14.065222, mean_q: 19.974884\n",
            " 142088/150000: episode: 20693, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.186315, mae: 13.869740, mean_q: 19.991045\n",
            " 142097/150000: episode: 20694, duration: 0.082s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.223781, mae: 13.773440, mean_q: 20.073757\n",
            " 142103/150000: episode: 20695, duration: 0.072s, episode steps:   6, steps per second:  84, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.000 [1.000, 8.000],  loss: 0.207570, mae: 14.181099, mean_q: 20.003572\n",
            " 142110/150000: episode: 20696, duration: 0.076s, episode steps:   7, steps per second:  93, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.231015, mae: 13.900390, mean_q: 19.942492\n",
            " 142115/150000: episode: 20697, duration: 0.051s, episode steps:   5, steps per second:  97, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.200 [0.000, 6.000],  loss: 0.188093, mae: 13.745746, mean_q: 19.827888\n",
            " 142123/150000: episode: 20698, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.472640, mae: 14.165916, mean_q: 20.003738\n",
            " 142130/150000: episode: 20699, duration: 0.078s, episode steps:   7, steps per second:  90, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [0.000, 8.000],  loss: 0.208299, mae: 14.031915, mean_q: 19.864868\n",
            " 142137/150000: episode: 20700, duration: 0.068s, episode steps:   7, steps per second: 103, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.427539, mae: 13.901827, mean_q: 19.956684\n",
            " 142145/150000: episode: 20701, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.505434, mae: 13.873390, mean_q: 19.795425\n",
            " 142152/150000: episode: 20702, duration: 0.073s, episode steps:   7, steps per second:  95, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.331751, mae: 13.637452, mean_q: 20.079960\n",
            " 142160/150000: episode: 20703, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.243187, mae: 13.932552, mean_q: 19.968693\n",
            " 142165/150000: episode: 20704, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.330586, mae: 14.011279, mean_q: 20.055681\n",
            " 142170/150000: episode: 20705, duration: 0.074s, episode steps:   5, steps per second:  67, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.200 [0.000, 5.000],  loss: 0.271320, mae: 13.949244, mean_q: 20.075687\n",
            " 142175/150000: episode: 20706, duration: 0.057s, episode steps:   5, steps per second:  88, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.200 [0.000, 8.000],  loss: 0.156889, mae: 14.044935, mean_q: 20.006840\n",
            " 142181/150000: episode: 20707, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.176501, mae: 13.974468, mean_q: 20.078110\n",
            " 142189/150000: episode: 20708, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.319835, mae: 13.928655, mean_q: 19.870537\n",
            " 142194/150000: episode: 20709, duration: 0.064s, episode steps:   5, steps per second:  78, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 6.000],  loss: 0.192552, mae: 13.752274, mean_q: 19.891434\n",
            " 142200/150000: episode: 20710, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 6.000],  loss: 0.702157, mae: 13.628461, mean_q: 19.815294\n",
            " 142208/150000: episode: 20711, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.402590, mae: 13.611816, mean_q: 19.954645\n",
            " 142216/150000: episode: 20712, duration: 0.071s, episode steps:   8, steps per second: 112, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.284900, mae: 13.836109, mean_q: 20.021969\n",
            " 142222/150000: episode: 20713, duration: 0.068s, episode steps:   6, steps per second:  89, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.778611, mae: 14.105335, mean_q: 19.775999\n",
            " 142228/150000: episode: 20714, duration: 0.060s, episode steps:   6, steps per second: 101, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2.667 [0.000, 6.000],  loss: 0.281489, mae: 14.149219, mean_q: 20.142921\n",
            " 142235/150000: episode: 20715, duration: 0.069s, episode steps:   7, steps per second: 102, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.441002, mae: 13.943805, mean_q: 19.805527\n",
            " 142240/150000: episode: 20716, duration: 0.061s, episode steps:   5, steps per second:  82, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.400 [0.000, 5.000],  loss: 0.401751, mae: 13.710966, mean_q: 20.016914\n",
            " 142245/150000: episode: 20717, duration: 0.052s, episode steps:   5, steps per second:  96, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.800 [0.000, 8.000],  loss: 0.571721, mae: 14.013127, mean_q: 20.173452\n",
            " 142250/150000: episode: 20718, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.200 [0.000, 5.000],  loss: 0.614078, mae: 13.836077, mean_q: 19.909126\n",
            " 142256/150000: episode: 20719, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.219462, mae: 14.076069, mean_q: 19.898584\n",
            " 142262/150000: episode: 20720, duration: 0.061s, episode steps:   6, steps per second:  98, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.499418, mae: 13.828534, mean_q: 20.037432\n",
            " 142264/150000: episode: 20721, duration: 0.028s, episode steps:   2, steps per second:  71, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 3.000 [3.000, 3.000],  loss: 0.204670, mae: 13.839432, mean_q: 19.844458\n",
            " 142270/150000: episode: 20722, duration: 0.062s, episode steps:   6, steps per second:  97, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 8.000],  loss: 0.307150, mae: 13.563472, mean_q: 19.947388\n",
            " 142278/150000: episode: 20723, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.331728, mae: 13.657093, mean_q: 19.906391\n",
            " 142287/150000: episode: 20724, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.292644, mae: 14.001726, mean_q: 20.019848\n",
            " 142296/150000: episode: 20725, duration: 0.080s, episode steps:   9, steps per second: 112, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.230256, mae: 13.890563, mean_q: 20.044163\n",
            " 142301/150000: episode: 20726, duration: 0.048s, episode steps:   5, steps per second: 105, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.200 [0.000, 5.000],  loss: 0.211362, mae: 14.276108, mean_q: 19.820623\n",
            " 142309/150000: episode: 20727, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.365086, mae: 13.784012, mean_q: 19.955570\n",
            " 142315/150000: episode: 20728, duration: 0.067s, episode steps:   6, steps per second:  90, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.284958, mae: 13.779969, mean_q: 19.960600\n",
            " 142323/150000: episode: 20729, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.258020, mae: 13.798493, mean_q: 19.913500\n",
            " 142330/150000: episode: 20730, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.229874, mae: 13.848348, mean_q: 19.888187\n",
            " 142339/150000: episode: 20731, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.257241, mae: 14.068843, mean_q: 20.025097\n",
            " 142346/150000: episode: 20732, duration: 0.068s, episode steps:   7, steps per second: 103, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.398970, mae: 13.879354, mean_q: 19.664719\n",
            " 142355/150000: episode: 20733, duration: 0.078s, episode steps:   9, steps per second: 116, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.239459, mae: 14.109028, mean_q: 20.219757\n",
            " 142364/150000: episode: 20734, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.372106, mae: 13.612743, mean_q: 19.733927\n",
            " 142373/150000: episode: 20735, duration: 0.081s, episode steps:   9, steps per second: 112, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.571563, mae: 14.241253, mean_q: 20.007444\n",
            " 142381/150000: episode: 20736, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.433905, mae: 14.075308, mean_q: 20.016851\n",
            " 142388/150000: episode: 20737, duration: 0.076s, episode steps:   7, steps per second:  92, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.237892, mae: 13.967330, mean_q: 20.001190\n",
            " 142397/150000: episode: 20738, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.303399, mae: 13.692311, mean_q: 19.808750\n",
            " 142405/150000: episode: 20739, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.250 [0.000, 8.000],  loss: 0.521890, mae: 13.678546, mean_q: 19.803539\n",
            " 142412/150000: episode: 20740, duration: 0.078s, episode steps:   7, steps per second:  90, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.209321, mae: 13.858989, mean_q: 19.925642\n",
            " 142420/150000: episode: 20741, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.522740, mae: 13.759327, mean_q: 20.012093\n",
            " 142426/150000: episode: 20742, duration: 0.060s, episode steps:   6, steps per second:  99, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.821602, mae: 13.924645, mean_q: 19.853058\n",
            " 142434/150000: episode: 20743, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.407064, mae: 13.886028, mean_q: 19.967831\n",
            " 142441/150000: episode: 20744, duration: 0.072s, episode steps:   7, steps per second:  97, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.278164, mae: 13.916247, mean_q: 20.268595\n",
            " 142447/150000: episode: 20745, duration: 0.064s, episode steps:   6, steps per second:  93, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2.833 [0.000, 6.000],  loss: 0.835393, mae: 14.108424, mean_q: 19.358727\n",
            " 142452/150000: episode: 20746, duration: 0.054s, episode steps:   5, steps per second:  93, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.450879, mae: 13.779554, mean_q: 20.025192\n",
            " 142454/150000: episode: 20747, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.315569, mae: 14.229944, mean_q: 20.309277\n",
            " 142459/150000: episode: 20748, duration: 0.047s, episode steps:   5, steps per second: 107, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.400 [2.000, 8.000],  loss: 0.141297, mae: 13.796720, mean_q: 20.017656\n",
            " 142465/150000: episode: 20749, duration: 0.055s, episode steps:   6, steps per second: 110, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.000 [1.000, 8.000],  loss: 0.182693, mae: 13.968659, mean_q: 19.957808\n",
            " 142469/150000: episode: 20750, duration: 0.038s, episode steps:   4, steps per second: 104, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.250 [0.000, 7.000],  loss: 0.277789, mae: 13.724257, mean_q: 20.044437\n",
            " 142474/150000: episode: 20751, duration: 0.051s, episode steps:   5, steps per second:  99, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.000 [0.000, 5.000],  loss: 0.523008, mae: 13.932096, mean_q: 20.001486\n",
            " 142483/150000: episode: 20752, duration: 0.096s, episode steps:   9, steps per second:  93, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.165198, mae: 13.892132, mean_q: 19.929794\n",
            " 142489/150000: episode: 20753, duration: 0.056s, episode steps:   6, steps per second: 106, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.407085, mae: 13.844131, mean_q: 20.005621\n",
            " 142494/150000: episode: 20754, duration: 0.048s, episode steps:   5, steps per second: 103, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.200 [0.000, 5.000],  loss: 0.198248, mae: 13.498217, mean_q: 19.836325\n",
            " 142502/150000: episode: 20755, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.530924, mae: 14.049335, mean_q: 19.775303\n",
            " 142511/150000: episode: 20756, duration: 0.104s, episode steps:   9, steps per second:  86, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.460280, mae: 13.946681, mean_q: 19.898314\n",
            " 142519/150000: episode: 20757, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.312135, mae: 13.865416, mean_q: 19.978146\n",
            " 142524/150000: episode: 20758, duration: 0.048s, episode steps:   5, steps per second: 104, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.800 [2.000, 7.000],  loss: 0.345323, mae: 13.461728, mean_q: 20.291782\n",
            " 142529/150000: episode: 20759, duration: 0.048s, episode steps:   5, steps per second: 104, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.800 [2.000, 7.000],  loss: 0.238512, mae: 14.265619, mean_q: 20.046635\n",
            " 142538/150000: episode: 20760, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.276182, mae: 13.716285, mean_q: 20.155519\n",
            " 142544/150000: episode: 20761, duration: 0.069s, episode steps:   6, steps per second:  87, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [2.000, 8.000],  loss: 0.222979, mae: 13.515882, mean_q: 19.901842\n",
            " 142550/150000: episode: 20762, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.397367, mae: 13.725983, mean_q: 19.891573\n",
            " 142558/150000: episode: 20763, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.255248, mae: 13.991442, mean_q: 19.962471\n",
            " 142564/150000: episode: 20764, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.833 [0.000, 8.000],  loss: 0.201950, mae: 14.133480, mean_q: 20.257830\n",
            " 142572/150000: episode: 20765, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.197153, mae: 13.815271, mean_q: 19.972694\n",
            " 142581/150000: episode: 20766, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.303703, mae: 13.968949, mean_q: 19.993599\n",
            " 142589/150000: episode: 20767, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.554567, mae: 13.927757, mean_q: 19.838825\n",
            " 142597/150000: episode: 20768, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.285207, mae: 13.679871, mean_q: 19.898451\n",
            " 142604/150000: episode: 20769, duration: 0.067s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.226660, mae: 13.776496, mean_q: 20.085712\n",
            " 142613/150000: episode: 20770, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.195409, mae: 14.006078, mean_q: 19.791546\n",
            " 142621/150000: episode: 20771, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.316754, mae: 14.054735, mean_q: 19.846773\n",
            " 142628/150000: episode: 20772, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.857 [1.000, 8.000],  loss: 0.512452, mae: 14.017920, mean_q: 19.829229\n",
            " 142636/150000: episode: 20773, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.289470, mae: 13.835201, mean_q: 19.952314\n",
            " 142645/150000: episode: 20774, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.281270, mae: 14.112039, mean_q: 20.005676\n",
            " 142650/150000: episode: 20775, duration: 0.048s, episode steps:   5, steps per second: 104, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.400 [0.000, 6.000],  loss: 0.259901, mae: 14.028842, mean_q: 20.052946\n",
            " 142658/150000: episode: 20776, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.483808, mae: 13.741142, mean_q: 20.028732\n",
            " 142667/150000: episode: 20777, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.287804, mae: 14.040752, mean_q: 20.193565\n",
            " 142675/150000: episode: 20778, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.250 [1.000, 8.000],  loss: 0.432564, mae: 13.776422, mean_q: 19.804115\n",
            " 142683/150000: episode: 20779, duration: 0.097s, episode steps:   8, steps per second:  83, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.417116, mae: 14.089933, mean_q: 20.037785\n",
            " 142691/150000: episode: 20780, duration: 0.077s, episode steps:   8, steps per second: 103, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.483955, mae: 14.018381, mean_q: 19.920971\n",
            " 142699/150000: episode: 20781, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.325915, mae: 13.841309, mean_q: 20.013222\n",
            " 142704/150000: episode: 20782, duration: 0.046s, episode steps:   5, steps per second: 108, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [2.000, 8.000],  loss: 0.274605, mae: 13.938286, mean_q: 20.257200\n",
            " 142712/150000: episode: 20783, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.325222, mae: 14.073881, mean_q: 19.949409\n",
            " 142720/150000: episode: 20784, duration: 0.071s, episode steps:   8, steps per second: 112, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.226527, mae: 14.056366, mean_q: 19.973425\n",
            " 142728/150000: episode: 20785, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.500106, mae: 13.862789, mean_q: 19.846172\n",
            " 142737/150000: episode: 20786, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.266321, mae: 14.124543, mean_q: 20.037907\n",
            " 142746/150000: episode: 20787, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.183277, mae: 13.923437, mean_q: 19.993406\n",
            " 142754/150000: episode: 20788, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.625 [0.000, 7.000],  loss: 0.171312, mae: 13.860066, mean_q: 19.980087\n",
            " 142762/150000: episode: 20789, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.194436, mae: 14.168539, mean_q: 19.955845\n",
            " 142769/150000: episode: 20790, duration: 0.062s, episode steps:   7, steps per second: 112, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [1.000, 8.000],  loss: 0.293435, mae: 13.662308, mean_q: 19.847605\n",
            " 142776/150000: episode: 20791, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.210797, mae: 14.055125, mean_q: 19.902538\n",
            " 142784/150000: episode: 20792, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.125 [0.000, 8.000],  loss: 0.298543, mae: 13.910972, mean_q: 19.980301\n",
            " 142792/150000: episode: 20793, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.625 [0.000, 8.000],  loss: 0.262369, mae: 13.868770, mean_q: 19.913082\n",
            " 142799/150000: episode: 20794, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.567155, mae: 14.039721, mean_q: 19.826063\n",
            " 142806/150000: episode: 20795, duration: 0.067s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [0.000, 8.000],  loss: 0.431846, mae: 13.863046, mean_q: 19.969362\n",
            " 142811/150000: episode: 20796, duration: 0.084s, episode steps:   5, steps per second:  59, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 8.000],  loss: 0.929043, mae: 13.820322, mean_q: 20.201672\n",
            " 142819/150000: episode: 20797, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.297933, mae: 14.153722, mean_q: 19.845100\n",
            " 142828/150000: episode: 20798, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.452385, mae: 13.909846, mean_q: 20.205894\n",
            " 142836/150000: episode: 20799, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.357292, mae: 13.911690, mean_q: 20.020847\n",
            " 142845/150000: episode: 20800, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.272247, mae: 13.991460, mean_q: 20.208441\n",
            " 142851/150000: episode: 20801, duration: 0.066s, episode steps:   6, steps per second:  91, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [1.000, 7.000],  loss: 0.140724, mae: 14.039826, mean_q: 19.951960\n",
            " 142857/150000: episode: 20802, duration: 0.061s, episode steps:   6, steps per second:  99, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [1.000, 7.000],  loss: 0.561163, mae: 13.924110, mean_q: 19.978903\n",
            " 142865/150000: episode: 20803, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.250 [0.000, 8.000],  loss: 0.247846, mae: 14.147739, mean_q: 19.952600\n",
            " 142872/150000: episode: 20804, duration: 0.073s, episode steps:   7, steps per second:  96, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.486269, mae: 13.796037, mean_q: 19.801058\n",
            " 142877/150000: episode: 20805, duration: 0.054s, episode steps:   5, steps per second:  93, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.269362, mae: 14.126869, mean_q: 20.145763\n",
            " 142886/150000: episode: 20806, duration: 0.131s, episode steps:   9, steps per second:  69, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.585801, mae: 13.882039, mean_q: 19.859520\n",
            " 142891/150000: episode: 20807, duration: 0.103s, episode steps:   5, steps per second:  49, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.800 [0.000, 5.000],  loss: 0.556160, mae: 14.018570, mean_q: 19.974400\n",
            " 142900/150000: episode: 20808, duration: 0.121s, episode steps:   9, steps per second:  74, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.457731, mae: 13.958179, mean_q: 20.031868\n",
            " 142906/150000: episode: 20809, duration: 0.096s, episode steps:   6, steps per second:  63, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.567374, mae: 13.692261, mean_q: 19.710768\n",
            " 142913/150000: episode: 20810, duration: 0.101s, episode steps:   7, steps per second:  69, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.871837, mae: 13.911546, mean_q: 20.025579\n",
            " 142921/150000: episode: 20811, duration: 0.118s, episode steps:   8, steps per second:  68, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.343841, mae: 13.928074, mean_q: 20.023815\n",
            " 142928/150000: episode: 20812, duration: 0.101s, episode steps:   7, steps per second:  69, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.467292, mae: 13.768687, mean_q: 19.788404\n",
            " 142936/150000: episode: 20813, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.401588, mae: 13.741426, mean_q: 20.004515\n",
            " 142942/150000: episode: 20814, duration: 0.094s, episode steps:   6, steps per second:  64, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.417025, mae: 14.191943, mean_q: 20.077734\n",
            " 142947/150000: episode: 20815, duration: 0.073s, episode steps:   5, steps per second:  69, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 1.600 [0.000, 5.000],  loss: 0.360000, mae: 13.919833, mean_q: 20.054852\n",
            " 142956/150000: episode: 20816, duration: 0.134s, episode steps:   9, steps per second:  67, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.354753, mae: 13.640613, mean_q: 19.982716\n",
            " 142962/150000: episode: 20817, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.799971, mae: 14.222111, mean_q: 19.949463\n",
            " 142970/150000: episode: 20818, duration: 0.102s, episode steps:   8, steps per second:  78, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.394633, mae: 13.953725, mean_q: 19.944492\n",
            " 142976/150000: episode: 20819, duration: 0.100s, episode steps:   6, steps per second:  60, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.833 [2.000, 8.000],  loss: 0.219263, mae: 13.999817, mean_q: 20.196341\n",
            " 142982/150000: episode: 20820, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.000 [0.000, 8.000],  loss: 0.261414, mae: 13.887184, mean_q: 19.913527\n",
            " 142990/150000: episode: 20821, duration: 0.109s, episode steps:   8, steps per second:  74, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.164335, mae: 13.770737, mean_q: 20.009712\n",
            " 142997/150000: episode: 20822, duration: 0.114s, episode steps:   7, steps per second:  62, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.535118, mae: 14.036309, mean_q: 19.947178\n",
            " 143003/150000: episode: 20823, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.167 [2.000, 8.000],  loss: 0.317157, mae: 13.672576, mean_q: 19.910904\n",
            " 143010/150000: episode: 20824, duration: 0.091s, episode steps:   7, steps per second:  77, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.429 [0.000, 8.000],  loss: 0.360619, mae: 13.807549, mean_q: 20.042084\n",
            " 143016/150000: episode: 20825, duration: 0.093s, episode steps:   6, steps per second:  64, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [1.000, 7.000],  loss: 0.365808, mae: 13.558991, mean_q: 19.987829\n",
            " 143023/150000: episode: 20826, duration: 0.106s, episode steps:   7, steps per second:  66, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.444825, mae: 14.131860, mean_q: 19.660288\n",
            " 143027/150000: episode: 20827, duration: 0.057s, episode steps:   4, steps per second:  71, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 1.750 [0.000, 3.000],  loss: 0.744042, mae: 14.056909, mean_q: 19.834784\n",
            " 143034/150000: episode: 20828, duration: 0.131s, episode steps:   7, steps per second:  54, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.857 [0.000, 8.000],  loss: 0.547475, mae: 14.166055, mean_q: 19.799520\n",
            " 143040/150000: episode: 20829, duration: 0.083s, episode steps:   6, steps per second:  72, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.500 [1.000, 8.000],  loss: 0.491663, mae: 14.104156, mean_q: 19.976728\n",
            " 143047/150000: episode: 20830, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.571 [1.000, 8.000],  loss: 0.637711, mae: 13.728528, mean_q: 19.904184\n",
            " 143052/150000: episode: 20831, duration: 0.078s, episode steps:   5, steps per second:  64, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 6.000 [4.000, 8.000],  loss: 0.338008, mae: 13.821213, mean_q: 19.923462\n",
            " 143059/150000: episode: 20832, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.857 [1.000, 8.000],  loss: 0.250576, mae: 14.331713, mean_q: 20.096918\n",
            " 143066/150000: episode: 20833, duration: 0.094s, episode steps:   7, steps per second:  75, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.714 [0.000, 8.000],  loss: 0.320825, mae: 13.708413, mean_q: 20.061123\n",
            " 143075/150000: episode: 20834, duration: 0.133s, episode steps:   9, steps per second:  68, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.454894, mae: 13.821857, mean_q: 19.960711\n",
            " 143082/150000: episode: 20835, duration: 0.098s, episode steps:   7, steps per second:  72, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.285983, mae: 13.878749, mean_q: 20.010464\n",
            " 143088/150000: episode: 20836, duration: 0.084s, episode steps:   6, steps per second:  71, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [1.000, 7.000],  loss: 0.682572, mae: 14.158810, mean_q: 19.847719\n",
            " 143093/150000: episode: 20837, duration: 0.099s, episode steps:   5, steps per second:  50, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.370885, mae: 13.882939, mean_q: 19.684727\n",
            " 143098/150000: episode: 20838, duration: 0.069s, episode steps:   5, steps per second:  72, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.591943, mae: 14.169126, mean_q: 19.753019\n",
            " 143107/150000: episode: 20839, duration: 0.123s, episode steps:   9, steps per second:  73, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.652798, mae: 13.983888, mean_q: 19.855412\n",
            " 143113/150000: episode: 20840, duration: 0.102s, episode steps:   6, steps per second:  59, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [1.000, 7.000],  loss: 0.499811, mae: 14.163601, mean_q: 19.863150\n",
            " 143121/150000: episode: 20841, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.565543, mae: 13.814400, mean_q: 19.986752\n",
            " 143130/150000: episode: 20842, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.296638, mae: 13.759943, mean_q: 20.046526\n",
            " 143137/150000: episode: 20843, duration: 0.075s, episode steps:   7, steps per second:  93, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.314870, mae: 13.932264, mean_q: 19.928562\n",
            " 143146/150000: episode: 20844, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.286916, mae: 14.024171, mean_q: 20.009926\n",
            " 143153/150000: episode: 20845, duration: 0.067s, episode steps:   7, steps per second: 104, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.571 [0.000, 8.000],  loss: 0.275328, mae: 13.862506, mean_q: 19.937231\n",
            " 143162/150000: episode: 20846, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.228051, mae: 13.957730, mean_q: 20.005228\n",
            " 143167/150000: episode: 20847, duration: 0.049s, episode steps:   5, steps per second: 103, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.209918, mae: 14.116011, mean_q: 19.616327\n",
            " 143172/150000: episode: 20848, duration: 0.052s, episode steps:   5, steps per second:  96, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.235957, mae: 14.188911, mean_q: 19.941006\n",
            " 143180/150000: episode: 20849, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.750 [0.000, 8.000],  loss: 0.333810, mae: 14.027498, mean_q: 19.954590\n",
            " 143185/150000: episode: 20850, duration: 0.066s, episode steps:   5, steps per second:  76, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.400 [0.000, 7.000],  loss: 0.322436, mae: 13.998761, mean_q: 20.328074\n",
            " 143193/150000: episode: 20851, duration: 0.072s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.606959, mae: 13.588247, mean_q: 19.945705\n",
            " 143199/150000: episode: 20852, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.371607, mae: 13.864446, mean_q: 20.002195\n",
            " 143206/150000: episode: 20853, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.644135, mae: 13.713709, mean_q: 20.222458\n",
            " 143214/150000: episode: 20854, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.750 [0.000, 8.000],  loss: 0.243299, mae: 13.692656, mean_q: 19.829315\n",
            " 143222/150000: episode: 20855, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.191665, mae: 14.017658, mean_q: 20.220427\n",
            " 143228/150000: episode: 20856, duration: 0.053s, episode steps:   6, steps per second: 113, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [1.000, 8.000],  loss: 0.680478, mae: 13.797745, mean_q: 19.803015\n",
            " 143234/150000: episode: 20857, duration: 0.053s, episode steps:   6, steps per second: 114, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.350310, mae: 13.772961, mean_q: 19.755516\n",
            " 143241/150000: episode: 20858, duration: 0.082s, episode steps:   7, steps per second:  85, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.571 [0.000, 8.000],  loss: 0.257382, mae: 13.858840, mean_q: 20.041883\n",
            " 143247/150000: episode: 20859, duration: 0.055s, episode steps:   6, steps per second: 110, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.500 [0.000, 8.000],  loss: 0.347235, mae: 14.249531, mean_q: 19.868690\n",
            " 143251/150000: episode: 20860, duration: 0.039s, episode steps:   4, steps per second: 102, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 2.750 [0.000, 4.000],  loss: 0.186230, mae: 14.366110, mean_q: 20.119907\n",
            " 143258/150000: episode: 20861, duration: 0.061s, episode steps:   7, steps per second: 116, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.253262, mae: 13.750565, mean_q: 20.049038\n",
            " 143265/150000: episode: 20862, duration: 0.071s, episode steps:   7, steps per second:  98, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.202927, mae: 13.721200, mean_q: 20.001612\n",
            " 143273/150000: episode: 20863, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.244439, mae: 13.771248, mean_q: 19.935303\n",
            " 143280/150000: episode: 20864, duration: 0.067s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.143 [0.000, 8.000],  loss: 0.298456, mae: 14.100807, mean_q: 19.957415\n",
            " 143286/150000: episode: 20865, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.274858, mae: 14.001414, mean_q: 19.843660\n",
            " 143292/150000: episode: 20866, duration: 0.086s, episode steps:   6, steps per second:  70, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.285217, mae: 14.138726, mean_q: 20.141006\n",
            " 143299/150000: episode: 20867, duration: 0.065s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.423437, mae: 13.875521, mean_q: 19.905851\n",
            " 143305/150000: episode: 20868, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.667 [2.000, 7.000],  loss: 0.216842, mae: 13.846751, mean_q: 19.847101\n",
            " 143313/150000: episode: 20869, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.443959, mae: 13.807783, mean_q: 19.953043\n",
            " 143322/150000: episode: 20870, duration: 0.108s, episode steps:   9, steps per second:  83, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.216102, mae: 13.981866, mean_q: 19.984442\n",
            " 143330/150000: episode: 20871, duration: 0.072s, episode steps:   8, steps per second: 112, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.497370, mae: 13.967769, mean_q: 19.956619\n",
            " 143337/150000: episode: 20872, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.253462, mae: 13.974738, mean_q: 19.926420\n",
            " 143341/150000: episode: 20873, duration: 0.039s, episode steps:   4, steps per second: 104, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.000 [2.000, 5.000],  loss: 0.194715, mae: 13.629266, mean_q: 20.022049\n",
            " 143347/150000: episode: 20874, duration: 0.071s, episode steps:   6, steps per second:  85, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [1.000, 7.000],  loss: 0.227038, mae: 14.140172, mean_q: 19.980818\n",
            " 143352/150000: episode: 20875, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [2.000, 8.000],  loss: 0.259313, mae: 14.099833, mean_q: 20.076431\n",
            " 143354/150000: episode: 20876, duration: 0.024s, episode steps:   2, steps per second:  85, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 4.000 [4.000, 4.000],  loss: 0.571967, mae: 14.189632, mean_q: 19.778889\n",
            " 143361/150000: episode: 20877, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.429 [0.000, 7.000],  loss: 0.330671, mae: 13.892255, mean_q: 19.797899\n",
            " 143369/150000: episode: 20878, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.246228, mae: 13.707300, mean_q: 20.116802\n",
            " 143376/150000: episode: 20879, duration: 0.066s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.230509, mae: 13.932347, mean_q: 19.966579\n",
            " 143385/150000: episode: 20880, duration: 0.078s, episode steps:   9, steps per second: 116, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.635753, mae: 14.106497, mean_q: 20.009356\n",
            " 143393/150000: episode: 20881, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.447934, mae: 14.116777, mean_q: 19.870668\n",
            " 143399/150000: episode: 20882, duration: 0.064s, episode steps:   6, steps per second:  94, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.667 [0.000, 8.000],  loss: 0.306731, mae: 14.182364, mean_q: 20.031088\n",
            " 143406/150000: episode: 20883, duration: 0.067s, episode steps:   7, steps per second: 104, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.296089, mae: 13.718797, mean_q: 20.133808\n",
            " 143413/150000: episode: 20884, duration: 0.069s, episode steps:   7, steps per second: 101, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.487177, mae: 13.583674, mean_q: 19.963892\n",
            " 143419/150000: episode: 20885, duration: 0.082s, episode steps:   6, steps per second:  73, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 7.000],  loss: 0.270945, mae: 13.577718, mean_q: 19.825228\n",
            " 143426/150000: episode: 20886, duration: 0.094s, episode steps:   7, steps per second:  74, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.000 [0.000, 8.000],  loss: 0.432520, mae: 13.887993, mean_q: 19.980091\n",
            " 143434/150000: episode: 20887, duration: 0.106s, episode steps:   8, steps per second:  76, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.329052, mae: 13.649960, mean_q: 19.900177\n",
            " 143440/150000: episode: 20888, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [0.000, 8.000],  loss: 0.358101, mae: 14.092609, mean_q: 19.930288\n",
            " 143448/150000: episode: 20889, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.196658, mae: 13.790249, mean_q: 19.871527\n",
            " 143453/150000: episode: 20890, duration: 0.047s, episode steps:   5, steps per second: 106, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.200 [3.000, 7.000],  loss: 0.897749, mae: 13.864471, mean_q: 19.873892\n",
            " 143459/150000: episode: 20891, duration: 0.069s, episode steps:   6, steps per second:  87, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 7.000],  loss: 0.707678, mae: 14.070172, mean_q: 20.040670\n",
            " 143468/150000: episode: 20892, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.500656, mae: 13.710333, mean_q: 19.929556\n",
            " 143473/150000: episode: 20893, duration: 0.046s, episode steps:   5, steps per second: 108, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.512505, mae: 14.279493, mean_q: 20.011135\n",
            " 143481/150000: episode: 20894, duration: 0.069s, episode steps:   8, steps per second: 117, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 5.000 [0.000, 8.000],  loss: 0.307534, mae: 13.908037, mean_q: 19.718353\n",
            " 143484/150000: episode: 20895, duration: 0.044s, episode steps:   3, steps per second:  68, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 4.000 [2.000, 5.000],  loss: 0.321477, mae: 13.440071, mean_q: 20.231068\n",
            " 143493/150000: episode: 20896, duration: 0.089s, episode steps:   9, steps per second: 102, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.897946, mae: 13.668594, mean_q: 19.832308\n",
            " 143498/150000: episode: 20897, duration: 0.045s, episode steps:   5, steps per second: 110, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.496929, mae: 14.131540, mean_q: 19.937061\n",
            " 143506/150000: episode: 20898, duration: 0.071s, episode steps:   8, steps per second: 112, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.372337, mae: 14.156420, mean_q: 20.230579\n",
            " 143515/150000: episode: 20899, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.592646, mae: 13.884521, mean_q: 19.852394\n",
            " 143522/150000: episode: 20900, duration: 0.072s, episode steps:   7, steps per second:  97, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.558202, mae: 13.622629, mean_q: 19.895632\n",
            " 143531/150000: episode: 20901, duration: 0.091s, episode steps:   9, steps per second:  98, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.347667, mae: 13.701118, mean_q: 20.005701\n",
            " 143539/150000: episode: 20902, duration: 0.069s, episode steps:   8, steps per second: 116, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.342551, mae: 13.875269, mean_q: 20.034945\n",
            " 143546/150000: episode: 20903, duration: 0.062s, episode steps:   7, steps per second: 112, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 5.000 [1.000, 8.000],  loss: 0.567614, mae: 14.154977, mean_q: 20.035555\n",
            " 143554/150000: episode: 20904, duration: 0.074s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.526880, mae: 13.931244, mean_q: 19.856714\n",
            " 143561/150000: episode: 20905, duration: 0.080s, episode steps:   7, steps per second:  88, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.333058, mae: 14.140132, mean_q: 20.298405\n",
            " 143569/150000: episode: 20906, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.381296, mae: 13.832850, mean_q: 19.869207\n",
            " 143575/150000: episode: 20907, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.833 [0.000, 8.000],  loss: 0.303757, mae: 13.998512, mean_q: 20.048145\n",
            " 143581/150000: episode: 20908, duration: 0.062s, episode steps:   6, steps per second:  97, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.000 [0.000, 7.000],  loss: 0.337836, mae: 13.693650, mean_q: 19.972321\n",
            " 143588/150000: episode: 20909, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.376929, mae: 13.553574, mean_q: 20.147818\n",
            " 143595/150000: episode: 20910, duration: 0.072s, episode steps:   7, steps per second:  98, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.277716, mae: 13.911080, mean_q: 19.923706\n",
            " 143600/150000: episode: 20911, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [1.000, 8.000],  loss: 0.398351, mae: 13.926712, mean_q: 19.794140\n",
            " 143605/150000: episode: 20912, duration: 0.049s, episode steps:   5, steps per second: 101, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.200 [0.000, 8.000],  loss: 0.246721, mae: 14.157431, mean_q: 20.174881\n",
            " 143610/150000: episode: 20913, duration: 0.050s, episode steps:   5, steps per second:  99, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.200 [0.000, 8.000],  loss: 0.229173, mae: 13.644018, mean_q: 20.089483\n",
            " 143617/150000: episode: 20914, duration: 0.067s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.437557, mae: 14.006774, mean_q: 19.779163\n",
            " 143623/150000: episode: 20915, duration: 0.063s, episode steps:   6, steps per second:  95, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.468455, mae: 13.685054, mean_q: 19.881659\n",
            " 143628/150000: episode: 20916, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 8.000],  loss: 0.389729, mae: 14.029856, mean_q: 19.711731\n",
            " 143634/150000: episode: 20917, duration: 0.055s, episode steps:   6, steps per second: 108, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.358179, mae: 13.685730, mean_q: 20.132284\n",
            " 143639/150000: episode: 20918, duration: 0.049s, episode steps:   5, steps per second: 101, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.400 [0.000, 7.000],  loss: 0.414275, mae: 14.270235, mean_q: 20.022251\n",
            " 143647/150000: episode: 20919, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.203674, mae: 14.162494, mean_q: 19.839622\n",
            " 143650/150000: episode: 20920, duration: 0.032s, episode steps:   3, steps per second:  94, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 6.333 [3.000, 8.000],  loss: 0.172344, mae: 14.070188, mean_q: 20.129120\n",
            " 143655/150000: episode: 20921, duration: 0.046s, episode steps:   5, steps per second: 108, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.400 [0.000, 8.000],  loss: 0.420630, mae: 13.746138, mean_q: 19.986828\n",
            " 143661/150000: episode: 20922, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.343708, mae: 13.884564, mean_q: 19.954885\n",
            " 143667/150000: episode: 20923, duration: 0.054s, episode steps:   6, steps per second: 111, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.301647, mae: 13.920306, mean_q: 20.027910\n",
            " 143672/150000: episode: 20924, duration: 0.061s, episode steps:   5, steps per second:  82, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.800 [2.000, 7.000],  loss: 0.231307, mae: 13.727442, mean_q: 20.075373\n",
            " 143681/150000: episode: 20925, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.533886, mae: 14.040923, mean_q: 20.011925\n",
            " 143688/150000: episode: 20926, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.315932, mae: 13.546271, mean_q: 19.877035\n",
            " 143693/150000: episode: 20927, duration: 0.075s, episode steps:   5, steps per second:  67, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.445800, mae: 13.977977, mean_q: 20.165417\n",
            " 143699/150000: episode: 20928, duration: 0.060s, episode steps:   6, steps per second:  99, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.500 [0.000, 8.000],  loss: 0.165828, mae: 14.079414, mean_q: 19.933016\n",
            " 143707/150000: episode: 20929, duration: 0.069s, episode steps:   8, steps per second: 116, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.125 [0.000, 8.000],  loss: 0.205980, mae: 13.949121, mean_q: 20.082420\n",
            " 143709/150000: episode: 20930, duration: 0.029s, episode steps:   2, steps per second:  70, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 0.000 [0.000, 0.000],  loss: 0.128236, mae: 13.617128, mean_q: 20.159742\n",
            " 143714/150000: episode: 20931, duration: 0.046s, episode steps:   5, steps per second: 108, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.184786, mae: 14.261533, mean_q: 20.011024\n",
            " 143721/150000: episode: 20932, duration: 0.077s, episode steps:   7, steps per second:  91, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.143 [0.000, 7.000],  loss: 0.241365, mae: 14.188982, mean_q: 20.017565\n",
            " 143729/150000: episode: 20933, duration: 0.069s, episode steps:   8, steps per second: 116, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.193859, mae: 13.654926, mean_q: 20.075706\n",
            " 143738/150000: episode: 20934, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.249904, mae: 14.232690, mean_q: 19.970310\n",
            " 143747/150000: episode: 20935, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.254992, mae: 13.927225, mean_q: 19.924763\n",
            " 143754/150000: episode: 20936, duration: 0.062s, episode steps:   7, steps per second: 113, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.234337, mae: 13.841387, mean_q: 19.957586\n",
            " 143761/150000: episode: 20937, duration: 0.075s, episode steps:   7, steps per second:  93, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.242152, mae: 13.901443, mean_q: 20.002884\n",
            " 143767/150000: episode: 20938, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.249123, mae: 14.025319, mean_q: 20.106081\n",
            " 143773/150000: episode: 20939, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.286032, mae: 13.544459, mean_q: 19.865969\n",
            " 143780/150000: episode: 20940, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 7.000],  loss: 0.233867, mae: 14.208319, mean_q: 19.938446\n",
            " 143789/150000: episode: 20941, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.411266, mae: 13.792700, mean_q: 19.987879\n",
            " 143794/150000: episode: 20942, duration: 0.051s, episode steps:   5, steps per second:  98, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.843007, mae: 13.582789, mean_q: 19.463806\n",
            " 143801/150000: episode: 20943, duration: 0.068s, episode steps:   7, steps per second: 103, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.714 [0.000, 8.000],  loss: 0.736730, mae: 14.146887, mean_q: 20.325197\n",
            " 143806/150000: episode: 20944, duration: 0.046s, episode steps:   5, steps per second: 108, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.800 [2.000, 7.000],  loss: 0.336700, mae: 13.992926, mean_q: 19.708412\n",
            " 143814/150000: episode: 20945, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.373878, mae: 14.255768, mean_q: 20.156761\n",
            " 143823/150000: episode: 20946, duration: 0.078s, episode steps:   9, steps per second: 115, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.295991, mae: 13.918729, mean_q: 19.994934\n",
            " 143830/150000: episode: 20947, duration: 0.068s, episode steps:   7, steps per second: 102, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.400029, mae: 13.874036, mean_q: 20.139559\n",
            " 143836/150000: episode: 20948, duration: 0.054s, episode steps:   6, steps per second: 112, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.597533, mae: 13.790836, mean_q: 20.080690\n",
            " 143842/150000: episode: 20949, duration: 0.068s, episode steps:   6, steps per second:  88, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [2.000, 8.000],  loss: 0.281670, mae: 14.169835, mean_q: 19.933146\n",
            " 143850/150000: episode: 20950, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.571631, mae: 13.941744, mean_q: 20.117668\n",
            " 143859/150000: episode: 20951, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.424951, mae: 13.876171, mean_q: 19.830368\n",
            " 143866/150000: episode: 20952, duration: 0.069s, episode steps:   7, steps per second: 101, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.375141, mae: 14.290544, mean_q: 20.065596\n",
            " 143875/150000: episode: 20953, duration: 0.081s, episode steps:   9, steps per second: 110, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.302994, mae: 14.013477, mean_q: 19.975464\n",
            " 143883/150000: episode: 20954, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.458365, mae: 13.970201, mean_q: 19.976280\n",
            " 143890/150000: episode: 20955, duration: 0.082s, episode steps:   7, steps per second:  85, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.740331, mae: 13.526746, mean_q: 20.102186\n",
            " 143897/150000: episode: 20956, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.383449, mae: 13.921028, mean_q: 19.928576\n",
            " 143903/150000: episode: 20957, duration: 0.067s, episode steps:   6, steps per second:  89, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.000 [1.000, 7.000],  loss: 0.309034, mae: 13.828137, mean_q: 20.102753\n",
            " 143910/150000: episode: 20958, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.275161, mae: 13.806560, mean_q: 19.924059\n",
            " 143916/150000: episode: 20959, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 7.000],  loss: 0.199379, mae: 14.126487, mean_q: 20.131119\n",
            " 143921/150000: episode: 20960, duration: 0.047s, episode steps:   5, steps per second: 107, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.400 [1.000, 7.000],  loss: 0.277903, mae: 13.764872, mean_q: 19.932520\n",
            " 143928/150000: episode: 20961, duration: 0.062s, episode steps:   7, steps per second: 112, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.477222, mae: 14.073503, mean_q: 19.970755\n",
            " 143936/150000: episode: 20962, duration: 0.071s, episode steps:   8, steps per second: 112, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.297355, mae: 14.010427, mean_q: 19.811167\n",
            " 143945/150000: episode: 20963, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.268658, mae: 13.801903, mean_q: 19.862572\n",
            " 143952/150000: episode: 20964, duration: 0.070s, episode steps:   7, steps per second: 100, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.441884, mae: 14.034124, mean_q: 20.014309\n",
            " 143958/150000: episode: 20965, duration: 0.055s, episode steps:   6, steps per second: 108, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.201141, mae: 13.680394, mean_q: 20.099976\n",
            " 143965/150000: episode: 20966, duration: 0.068s, episode steps:   7, steps per second: 104, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.465226, mae: 14.160197, mean_q: 20.043919\n",
            " 143972/150000: episode: 20967, duration: 0.078s, episode steps:   7, steps per second:  90, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.283395, mae: 14.167821, mean_q: 20.074884\n",
            " 143978/150000: episode: 20968, duration: 0.061s, episode steps:   6, steps per second:  98, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.000 [1.000, 6.000],  loss: 0.325223, mae: 13.665214, mean_q: 19.917320\n",
            " 143986/150000: episode: 20969, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.468483, mae: 14.156266, mean_q: 19.984283\n",
            " 143992/150000: episode: 20970, duration: 0.057s, episode steps:   6, steps per second: 106, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.667 [1.000, 8.000],  loss: 0.308947, mae: 13.691071, mean_q: 19.900669\n",
            " 143995/150000: episode: 20971, duration: 0.045s, episode steps:   3, steps per second:  66, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 5.333 [5.000, 6.000],  loss: 0.259195, mae: 13.816638, mean_q: 19.874174\n",
            " 144004/150000: episode: 20972, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.261148, mae: 13.831921, mean_q: 19.938824\n",
            " 144011/150000: episode: 20973, duration: 0.072s, episode steps:   7, steps per second:  98, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.207791, mae: 13.955165, mean_q: 20.044928\n",
            " 144017/150000: episode: 20974, duration: 0.069s, episode steps:   6, steps per second:  87, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.261947, mae: 14.036144, mean_q: 19.965170\n",
            " 144023/150000: episode: 20975, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2.833 [0.000, 6.000],  loss: 0.194437, mae: 13.982545, mean_q: 19.735121\n",
            " 144032/150000: episode: 20976, duration: 0.080s, episode steps:   9, steps per second: 112, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.522428, mae: 13.958460, mean_q: 19.858852\n",
            " 144039/150000: episode: 20977, duration: 0.068s, episode steps:   7, steps per second: 103, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.238592, mae: 13.867881, mean_q: 19.983612\n",
            " 144048/150000: episode: 20978, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.630831, mae: 13.940761, mean_q: 19.905375\n",
            " 144056/150000: episode: 20979, duration: 0.074s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.628733, mae: 13.833241, mean_q: 19.744808\n",
            " 144061/150000: episode: 20980, duration: 0.062s, episode steps:   5, steps per second:  80, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.277974, mae: 14.101065, mean_q: 20.169851\n",
            " 144070/150000: episode: 20981, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.416751, mae: 14.120699, mean_q: 20.156969\n",
            " 144077/150000: episode: 20982, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.257699, mae: 13.446150, mean_q: 19.999960\n",
            " 144086/150000: episode: 20983, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.291581, mae: 14.035465, mean_q: 19.876844\n",
            " 144092/150000: episode: 20984, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [2.000, 8.000],  loss: 0.613372, mae: 13.574135, mean_q: 19.927925\n",
            " 144100/150000: episode: 20985, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.401746, mae: 13.965068, mean_q: 20.003605\n",
            " 144105/150000: episode: 20986, duration: 0.054s, episode steps:   5, steps per second:  93, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.800 [0.000, 5.000],  loss: 0.214063, mae: 13.708972, mean_q: 19.971487\n",
            " 144114/150000: episode: 20987, duration: 0.131s, episode steps:   9, steps per second:  69, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.270297, mae: 13.833247, mean_q: 20.045769\n",
            " 144121/150000: episode: 20988, duration: 0.093s, episode steps:   7, steps per second:  75, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.418684, mae: 13.802366, mean_q: 19.976866\n",
            " 144127/150000: episode: 20989, duration: 0.086s, episode steps:   6, steps per second:  69, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 6.000],  loss: 0.433627, mae: 14.038037, mean_q: 20.219315\n",
            " 144135/150000: episode: 20990, duration: 0.128s, episode steps:   8, steps per second:  63, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.372920, mae: 14.191235, mean_q: 19.990314\n",
            " 144141/150000: episode: 20991, duration: 0.085s, episode steps:   6, steps per second:  70, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.227825, mae: 14.079755, mean_q: 20.191572\n",
            " 144150/150000: episode: 20992, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.434040, mae: 13.926870, mean_q: 19.943012\n",
            " 144154/150000: episode: 20993, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.000 [0.000, 8.000],  loss: 1.269372, mae: 13.814065, mean_q: 19.930950\n",
            " 144159/150000: episode: 20994, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.600 [1.000, 8.000],  loss: 0.296928, mae: 13.782663, mean_q: 19.996784\n",
            " 144163/150000: episode: 20995, duration: 0.058s, episode steps:   4, steps per second:  70, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.000 [1.000, 7.000],  loss: 0.540572, mae: 13.603823, mean_q: 19.740944\n",
            " 144170/150000: episode: 20996, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.437497, mae: 13.859828, mean_q: 20.294382\n",
            " 144179/150000: episode: 20997, duration: 0.133s, episode steps:   9, steps per second:  68, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.417768, mae: 13.758458, mean_q: 20.067347\n",
            " 144184/150000: episode: 20998, duration: 0.073s, episode steps:   5, steps per second:  69, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.400 [0.000, 6.000],  loss: 0.367825, mae: 13.879466, mean_q: 19.949659\n",
            " 144192/150000: episode: 20999, duration: 0.114s, episode steps:   8, steps per second:  70, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.375 [1.000, 8.000],  loss: 0.275463, mae: 13.893691, mean_q: 20.012091\n",
            " 144200/150000: episode: 21000, duration: 0.124s, episode steps:   8, steps per second:  64, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.251249, mae: 13.867477, mean_q: 19.875233\n",
            " 144208/150000: episode: 21001, duration: 0.122s, episode steps:   8, steps per second:  65, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.250 [0.000, 8.000],  loss: 0.352004, mae: 13.705339, mean_q: 19.964941\n",
            " 144215/150000: episode: 21002, duration: 0.098s, episode steps:   7, steps per second:  71, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.429 [0.000, 7.000],  loss: 0.288458, mae: 13.814157, mean_q: 19.940037\n",
            " 144223/150000: episode: 21003, duration: 0.128s, episode steps:   8, steps per second:  63, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.299396, mae: 13.997612, mean_q: 20.086620\n",
            " 144232/150000: episode: 21004, duration: 0.139s, episode steps:   9, steps per second:  65, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.419166, mae: 14.147943, mean_q: 20.091394\n",
            " 144238/150000: episode: 21005, duration: 0.084s, episode steps:   6, steps per second:  71, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.833 [0.000, 8.000],  loss: 0.342839, mae: 13.768658, mean_q: 20.154394\n",
            " 144246/150000: episode: 21006, duration: 0.137s, episode steps:   8, steps per second:  58, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.250 [0.000, 8.000],  loss: 0.306992, mae: 14.067495, mean_q: 20.045374\n",
            " 144254/150000: episode: 21007, duration: 0.120s, episode steps:   8, steps per second:  66, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.256277, mae: 13.838131, mean_q: 19.993671\n",
            " 144263/150000: episode: 21008, duration: 0.134s, episode steps:   9, steps per second:  67, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.546399, mae: 13.901259, mean_q: 20.212093\n",
            " 144269/150000: episode: 21009, duration: 0.086s, episode steps:   6, steps per second:  70, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.667 [0.000, 7.000],  loss: 0.252141, mae: 14.321530, mean_q: 19.785547\n",
            " 144276/150000: episode: 21010, duration: 0.100s, episode steps:   7, steps per second:  70, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.611491, mae: 13.756502, mean_q: 20.139101\n",
            " 144284/150000: episode: 21011, duration: 0.119s, episode steps:   8, steps per second:  67, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.385514, mae: 13.863303, mean_q: 19.837179\n",
            " 144292/150000: episode: 21012, duration: 0.103s, episode steps:   8, steps per second:  78, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.720630, mae: 14.104362, mean_q: 20.003498\n",
            " 144299/150000: episode: 21013, duration: 0.101s, episode steps:   7, steps per second:  69, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.359343, mae: 13.541792, mean_q: 19.908682\n",
            " 144308/150000: episode: 21014, duration: 0.128s, episode steps:   9, steps per second:  71, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.481412, mae: 14.058579, mean_q: 20.114883\n",
            " 144317/150000: episode: 21015, duration: 0.157s, episode steps:   9, steps per second:  57, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.257362, mae: 13.760407, mean_q: 20.031990\n",
            " 144323/150000: episode: 21016, duration: 0.084s, episode steps:   6, steps per second:  72, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.292481, mae: 13.528603, mean_q: 19.970663\n",
            " 144329/150000: episode: 21017, duration: 0.085s, episode steps:   6, steps per second:  70, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.333 [0.000, 8.000],  loss: 0.531687, mae: 13.905879, mean_q: 20.033596\n",
            " 144336/150000: episode: 21018, duration: 0.112s, episode steps:   7, steps per second:  63, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.246700, mae: 13.963541, mean_q: 20.024570\n",
            " 144344/150000: episode: 21019, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.425002, mae: 13.735218, mean_q: 19.927250\n",
            " 144353/150000: episode: 21020, duration: 0.078s, episode steps:   9, steps per second: 115, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.521070, mae: 13.913179, mean_q: 19.953987\n",
            " 144358/150000: episode: 21021, duration: 0.047s, episode steps:   5, steps per second: 106, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.000 [0.000, 7.000],  loss: 0.260504, mae: 14.082985, mean_q: 19.634785\n",
            " 144367/150000: episode: 21022, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.596952, mae: 13.923643, mean_q: 20.088472\n",
            " 144374/150000: episode: 21023, duration: 0.076s, episode steps:   7, steps per second:  92, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.286 [0.000, 8.000],  loss: 0.271225, mae: 13.650863, mean_q: 20.053310\n",
            " 144379/150000: episode: 21024, duration: 0.060s, episode steps:   5, steps per second:  83, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.000 [2.000, 7.000],  loss: 0.552345, mae: 13.778872, mean_q: 19.874407\n",
            " 144385/150000: episode: 21025, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.394185, mae: 13.853024, mean_q: 20.013874\n",
            " 144390/150000: episode: 21026, duration: 0.049s, episode steps:   5, steps per second: 102, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 8.000],  loss: 0.249794, mae: 13.956141, mean_q: 20.264221\n",
            " 144398/150000: episode: 21027, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.532956, mae: 13.580097, mean_q: 19.838968\n",
            " 144405/150000: episode: 21028, duration: 0.079s, episode steps:   7, steps per second:  89, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.310402, mae: 14.270028, mean_q: 19.924107\n",
            " 144413/150000: episode: 21029, duration: 0.081s, episode steps:   8, steps per second:  98, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.544966, mae: 13.951713, mean_q: 20.258770\n",
            " 144419/150000: episode: 21030, duration: 0.055s, episode steps:   6, steps per second: 108, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.563086, mae: 13.803195, mean_q: 19.911057\n",
            " 144427/150000: episode: 21031, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.625 [0.000, 7.000],  loss: 0.255980, mae: 13.849729, mean_q: 20.034180\n",
            " 144436/150000: episode: 21032, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.267044, mae: 13.885596, mean_q: 19.970760\n",
            " 144441/150000: episode: 21033, duration: 0.051s, episode steps:   5, steps per second:  98, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.300584, mae: 14.055023, mean_q: 20.062664\n",
            " 144447/150000: episode: 21034, duration: 0.061s, episode steps:   6, steps per second:  98, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.447018, mae: 13.958080, mean_q: 19.960613\n",
            " 144452/150000: episode: 21035, duration: 0.061s, episode steps:   5, steps per second:  82, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.290134, mae: 13.855273, mean_q: 19.939236\n",
            " 144457/150000: episode: 21036, duration: 0.049s, episode steps:   5, steps per second: 102, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.304707, mae: 13.935476, mean_q: 20.011465\n",
            " 144464/150000: episode: 21037, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.880268, mae: 13.675924, mean_q: 19.926973\n",
            " 144471/150000: episode: 21038, duration: 0.074s, episode steps:   7, steps per second:  95, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.857 [2.000, 8.000],  loss: 0.224675, mae: 13.877714, mean_q: 20.043808\n",
            " 144479/150000: episode: 21039, duration: 0.074s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.434390, mae: 14.181072, mean_q: 20.133801\n",
            " 144487/150000: episode: 21040, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.318204, mae: 13.933229, mean_q: 20.049004\n",
            " 144496/150000: episode: 21041, duration: 0.104s, episode steps:   9, steps per second:  87, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.474484, mae: 13.816214, mean_q: 20.147411\n",
            " 144502/150000: episode: 21042, duration: 0.055s, episode steps:   6, steps per second: 110, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.333 [2.000, 8.000],  loss: 0.373282, mae: 14.113861, mean_q: 19.813406\n",
            " 144508/150000: episode: 21043, duration: 0.053s, episode steps:   6, steps per second: 114, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.445987, mae: 14.092795, mean_q: 20.175844\n",
            " 144516/150000: episode: 21044, duration: 0.081s, episode steps:   8, steps per second:  99, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.480186, mae: 13.847992, mean_q: 19.600525\n",
            " 144524/150000: episode: 21045, duration: 0.083s, episode steps:   8, steps per second:  97, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.412065, mae: 14.124393, mean_q: 19.995806\n",
            " 144529/150000: episode: 21046, duration: 0.048s, episode steps:   5, steps per second: 104, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [2.000, 8.000],  loss: 0.260536, mae: 13.698853, mean_q: 19.920303\n",
            " 144536/150000: episode: 21047, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.262134, mae: 13.811336, mean_q: 20.166105\n",
            " 144544/150000: episode: 21048, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.251954, mae: 13.922451, mean_q: 20.139845\n",
            " 144553/150000: episode: 21049, duration: 0.103s, episode steps:   9, steps per second:  87, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.486213, mae: 13.941736, mean_q: 20.006241\n",
            " 144561/150000: episode: 21050, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.817439, mae: 13.643677, mean_q: 19.854240\n",
            " 144568/150000: episode: 21051, duration: 0.065s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.340785, mae: 14.139221, mean_q: 20.120453\n",
            " 144575/150000: episode: 21052, duration: 0.070s, episode steps:   7, steps per second: 100, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [1.000, 8.000],  loss: 0.448263, mae: 14.013906, mean_q: 19.824930\n",
            " 144580/150000: episode: 21053, duration: 0.064s, episode steps:   5, steps per second:  78, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.365991, mae: 13.500234, mean_q: 20.055948\n",
            " 144586/150000: episode: 21054, duration: 0.061s, episode steps:   6, steps per second:  98, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [1.000, 7.000],  loss: 0.263016, mae: 14.012764, mean_q: 20.060577\n",
            " 144591/150000: episode: 21055, duration: 0.054s, episode steps:   5, steps per second:  93, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.000 [0.000, 7.000],  loss: 0.637540, mae: 14.169998, mean_q: 19.878149\n",
            " 144596/150000: episode: 21056, duration: 0.074s, episode steps:   5, steps per second:  68, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.400 [0.000, 6.000],  loss: 0.552321, mae: 13.623256, mean_q: 20.029627\n",
            " 144602/150000: episode: 21057, duration: 0.060s, episode steps:   6, steps per second: 101, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.167 [1.000, 8.000],  loss: 0.664842, mae: 13.597487, mean_q: 19.985292\n",
            " 144609/150000: episode: 21058, duration: 0.071s, episode steps:   7, steps per second:  99, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.466835, mae: 14.114975, mean_q: 19.908171\n",
            " 144611/150000: episode: 21059, duration: 0.028s, episode steps:   2, steps per second:  71, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 4.000 [4.000, 4.000],  loss: 0.306944, mae: 14.145307, mean_q: 20.252205\n",
            " 144618/150000: episode: 21060, duration: 0.075s, episode steps:   7, steps per second:  94, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.439093, mae: 13.669091, mean_q: 20.075705\n",
            " 144625/150000: episode: 21061, duration: 0.063s, episode steps:   7, steps per second: 111, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 5.286 [1.000, 8.000],  loss: 0.382157, mae: 13.684107, mean_q: 19.920887\n",
            " 144632/150000: episode: 21062, duration: 0.068s, episode steps:   7, steps per second: 104, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 7.000],  loss: 0.265544, mae: 14.135405, mean_q: 20.190722\n",
            " 144641/150000: episode: 21063, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.538125, mae: 13.955928, mean_q: 19.872658\n",
            " 144650/150000: episode: 21064, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.490194, mae: 13.707038, mean_q: 19.870253\n",
            " 144658/150000: episode: 21065, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.582187, mae: 13.753595, mean_q: 19.692482\n",
            " 144664/150000: episode: 21066, duration: 0.074s, episode steps:   6, steps per second:  81, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.667 [0.000, 6.000],  loss: 0.391213, mae: 13.947114, mean_q: 19.835932\n",
            " 144672/150000: episode: 21067, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.409901, mae: 13.713579, mean_q: 20.079384\n",
            " 144678/150000: episode: 21068, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [1.000, 8.000],  loss: 0.260329, mae: 13.977028, mean_q: 19.990366\n",
            " 144683/150000: episode: 21069, duration: 0.049s, episode steps:   5, steps per second: 103, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.800 [2.000, 7.000],  loss: 0.208388, mae: 13.560971, mean_q: 20.085003\n",
            " 144689/150000: episode: 21070, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 2.667 [0.000, 8.000],  loss: 0.360136, mae: 13.868203, mean_q: 19.886454\n",
            " 144697/150000: episode: 21071, duration: 0.070s, episode steps:   8, steps per second: 114, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.731469, mae: 13.801232, mean_q: 19.879505\n",
            " 144705/150000: episode: 21072, duration: 0.070s, episode steps:   8, steps per second: 115, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.422728, mae: 13.914900, mean_q: 19.897697\n",
            " 144714/150000: episode: 21073, duration: 0.105s, episode steps:   9, steps per second:  86, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.275331, mae: 14.017756, mean_q: 20.136116\n",
            " 144722/150000: episode: 21074, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.193552, mae: 13.993933, mean_q: 19.894333\n",
            " 144727/150000: episode: 21075, duration: 0.048s, episode steps:   5, steps per second: 105, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.600 [1.000, 8.000],  loss: 0.207248, mae: 14.172613, mean_q: 19.669615\n",
            " 144736/150000: episode: 21076, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.345592, mae: 14.003110, mean_q: 19.907406\n",
            " 144744/150000: episode: 21077, duration: 0.080s, episode steps:   8, steps per second: 101, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.542988, mae: 13.686437, mean_q: 19.897427\n",
            " 144752/150000: episode: 21078, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.284812, mae: 13.903358, mean_q: 19.877476\n",
            " 144758/150000: episode: 21079, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.707285, mae: 13.754799, mean_q: 20.075136\n",
            " 144763/150000: episode: 21080, duration: 0.057s, episode steps:   5, steps per second:  88, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.600 [0.000, 7.000],  loss: 0.221736, mae: 13.924581, mean_q: 19.778500\n",
            " 144772/150000: episode: 21081, duration: 0.081s, episode steps:   9, steps per second: 112, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.286590, mae: 13.503495, mean_q: 19.971972\n",
            " 144781/150000: episode: 21082, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.221039, mae: 13.805917, mean_q: 19.962212\n",
            " 144788/150000: episode: 21083, duration: 0.069s, episode steps:   7, steps per second: 101, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.697344, mae: 13.743912, mean_q: 19.923830\n",
            " 144797/150000: episode: 21084, duration: 0.087s, episode steps:   9, steps per second: 103, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.444 [1.000, 8.000],  loss: 0.616466, mae: 13.777122, mean_q: 19.879990\n",
            " 144803/150000: episode: 21085, duration: 0.065s, episode steps:   6, steps per second:  92, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [1.000, 8.000],  loss: 0.404367, mae: 14.039010, mean_q: 20.130285\n",
            " 144812/150000: episode: 21086, duration: 0.085s, episode steps:   9, steps per second: 105, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.418417, mae: 14.136472, mean_q: 20.028273\n",
            " 144820/150000: episode: 21087, duration: 0.081s, episode steps:   8, steps per second:  98, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.391527, mae: 13.923916, mean_q: 19.962307\n",
            " 144829/150000: episode: 21088, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.667 [0.000, 7.000],  loss: 0.401124, mae: 13.701160, mean_q: 19.924776\n",
            " 144836/150000: episode: 21089, duration: 0.063s, episode steps:   7, steps per second: 111, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.335920, mae: 13.991713, mean_q: 20.053392\n",
            " 144842/150000: episode: 21090, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.254360, mae: 13.941784, mean_q: 19.875549\n",
            " 144850/150000: episode: 21091, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.555688, mae: 14.166197, mean_q: 19.973507\n",
            " 144856/150000: episode: 21092, duration: 0.066s, episode steps:   6, steps per second:  91, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.544934, mae: 14.131490, mean_q: 20.194448\n",
            " 144861/150000: episode: 21093, duration: 0.048s, episode steps:   5, steps per second: 105, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.419685, mae: 13.882268, mean_q: 20.149677\n",
            " 144870/150000: episode: 21094, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.371173, mae: 13.699104, mean_q: 19.920572\n",
            " 144879/150000: episode: 21095, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.333 [0.000, 8.000],  loss: 0.198989, mae: 14.130103, mean_q: 20.095903\n",
            " 144887/150000: episode: 21096, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.377813, mae: 13.938236, mean_q: 19.802710\n",
            " 144894/150000: episode: 21097, duration: 0.071s, episode steps:   7, steps per second:  99, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.337558, mae: 13.751106, mean_q: 20.067749\n",
            " 144901/150000: episode: 21098, duration: 0.077s, episode steps:   7, steps per second:  91, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.348518, mae: 13.591772, mean_q: 19.978252\n",
            " 144908/150000: episode: 21099, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.714 [0.000, 8.000],  loss: 0.325709, mae: 13.775302, mean_q: 19.961393\n",
            " 144916/150000: episode: 21100, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.576199, mae: 13.453545, mean_q: 19.857561\n",
            " 144920/150000: episode: 21101, duration: 0.049s, episode steps:   4, steps per second:  81, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.250 [2.000, 7.000],  loss: 0.653445, mae: 14.360358, mean_q: 19.881155\n",
            " 144929/150000: episode: 21102, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.348620, mae: 13.876568, mean_q: 20.156685\n",
            " 144936/150000: episode: 21103, duration: 0.065s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.328528, mae: 14.000426, mean_q: 19.964512\n",
            " 144945/150000: episode: 21104, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.588219, mae: 13.687489, mean_q: 20.046722\n",
            " 144949/150000: episode: 21105, duration: 0.040s, episode steps:   4, steps per second: 101, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.250 [0.000, 7.000],  loss: 0.277790, mae: 13.825142, mean_q: 20.025074\n",
            " 144955/150000: episode: 21106, duration: 0.069s, episode steps:   6, steps per second:  87, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.000 [0.000, 8.000],  loss: 0.322039, mae: 13.596728, mean_q: 20.017092\n",
            " 144962/150000: episode: 21107, duration: 0.070s, episode steps:   7, steps per second: 100, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.449325, mae: 13.701111, mean_q: 19.819685\n",
            " 144971/150000: episode: 21108, duration: 0.082s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.580276, mae: 14.050726, mean_q: 19.981125\n",
            " 144977/150000: episode: 21109, duration: 0.067s, episode steps:   6, steps per second:  90, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [2.000, 7.000],  loss: 0.212063, mae: 13.910725, mean_q: 19.860802\n",
            " 144985/150000: episode: 21110, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.125 [1.000, 8.000],  loss: 0.358966, mae: 13.783606, mean_q: 20.007215\n",
            " 144991/150000: episode: 21111, duration: 0.074s, episode steps:   6, steps per second:  82, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [1.000, 8.000],  loss: 0.230933, mae: 13.684115, mean_q: 20.205755\n",
            " 145000/150000: episode: 21112, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.167565, mae: 13.929834, mean_q: 19.940598\n",
            " 145006/150000: episode: 21113, duration: 0.062s, episode steps:   6, steps per second:  98, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [0.000, 7.000],  loss: 0.284300, mae: 14.075691, mean_q: 19.852854\n",
            " 145015/150000: episode: 21114, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.528675, mae: 13.740581, mean_q: 19.930914\n",
            " 145021/150000: episode: 21115, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [0.000, 8.000],  loss: 0.457533, mae: 13.960648, mean_q: 19.553314\n",
            " 145027/150000: episode: 21116, duration: 0.061s, episode steps:   6, steps per second:  98, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 6.000],  loss: 0.248749, mae: 13.974892, mean_q: 20.160271\n",
            " 145034/150000: episode: 21117, duration: 0.068s, episode steps:   7, steps per second: 102, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.327437, mae: 14.131059, mean_q: 20.037033\n",
            " 145040/150000: episode: 21118, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2.667 [0.000, 6.000],  loss: 0.177931, mae: 13.734638, mean_q: 19.997652\n",
            " 145049/150000: episode: 21119, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.382363, mae: 13.828849, mean_q: 20.082211\n",
            " 145055/150000: episode: 21120, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [1.000, 7.000],  loss: 0.347794, mae: 13.806950, mean_q: 19.857893\n",
            " 145062/150000: episode: 21121, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.356131, mae: 14.302843, mean_q: 19.977827\n",
            " 145068/150000: episode: 21122, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.189203, mae: 13.668458, mean_q: 19.889772\n",
            " 145077/150000: episode: 21123, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.373912, mae: 13.902772, mean_q: 19.887005\n",
            " 145086/150000: episode: 21124, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.388088, mae: 13.765430, mean_q: 20.079748\n",
            " 145091/150000: episode: 21125, duration: 0.048s, episode steps:   5, steps per second: 105, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.800 [2.000, 8.000],  loss: 0.250356, mae: 13.732590, mean_q: 19.998564\n",
            " 145096/150000: episode: 21126, duration: 0.046s, episode steps:   5, steps per second: 109, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.200 [1.000, 7.000],  loss: 0.301981, mae: 13.805733, mean_q: 19.955376\n",
            " 145103/150000: episode: 21127, duration: 0.079s, episode steps:   7, steps per second:  89, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.635235, mae: 13.966974, mean_q: 19.697172\n",
            " 145111/150000: episode: 21128, duration: 0.070s, episode steps:   8, steps per second: 114, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.390534, mae: 14.175537, mean_q: 19.701889\n",
            " 145120/150000: episode: 21129, duration: 0.074s, episode steps:   9, steps per second: 121, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.333 [0.000, 8.000],  loss: 0.272590, mae: 13.884938, mean_q: 19.968384\n",
            " 145126/150000: episode: 21130, duration: 0.072s, episode steps:   6, steps per second:  83, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 7.000],  loss: 0.546845, mae: 14.027145, mean_q: 19.764574\n",
            " 145131/150000: episode: 21131, duration: 0.053s, episode steps:   5, steps per second:  94, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.400 [2.000, 7.000],  loss: 0.295570, mae: 13.901746, mean_q: 19.805254\n",
            " 145137/150000: episode: 21132, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [1.000, 7.000],  loss: 0.208475, mae: 14.344055, mean_q: 20.013182\n",
            " 145143/150000: episode: 21133, duration: 0.053s, episode steps:   6, steps per second: 113, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.539667, mae: 14.240035, mean_q: 20.043842\n",
            " 145152/150000: episode: 21134, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.412685, mae: 14.105037, mean_q: 19.955502\n",
            " 145161/150000: episode: 21135, duration: 0.081s, episode steps:   9, steps per second: 112, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.263066, mae: 14.044034, mean_q: 19.895107\n",
            " 145169/150000: episode: 21136, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.266699, mae: 14.137241, mean_q: 20.247272\n",
            " 145174/150000: episode: 21137, duration: 0.047s, episode steps:   5, steps per second: 106, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.400 [0.000, 7.000],  loss: 0.248005, mae: 14.050624, mean_q: 19.903046\n",
            " 145181/150000: episode: 21138, duration: 0.074s, episode steps:   7, steps per second:  95, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.436527, mae: 14.094772, mean_q: 19.527981\n",
            " 145188/150000: episode: 21139, duration: 0.068s, episode steps:   7, steps per second: 103, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.388177, mae: 13.964074, mean_q: 19.993589\n",
            " 145192/150000: episode: 21140, duration: 0.040s, episode steps:   4, steps per second: 100, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.500 [0.000, 6.000],  loss: 0.454260, mae: 14.076813, mean_q: 20.082142\n",
            " 145199/150000: episode: 21141, duration: 0.078s, episode steps:   7, steps per second:  90, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.320188, mae: 13.812472, mean_q: 19.656673\n",
            " 145207/150000: episode: 21142, duration: 0.068s, episode steps:   8, steps per second: 117, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.482653, mae: 14.195698, mean_q: 20.061310\n",
            " 145216/150000: episode: 21143, duration: 0.078s, episode steps:   9, steps per second: 115, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.319052, mae: 13.877387, mean_q: 19.977812\n",
            " 145223/150000: episode: 21144, duration: 0.067s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.266989, mae: 14.070271, mean_q: 19.998219\n",
            " 145231/150000: episode: 21145, duration: 0.098s, episode steps:   8, steps per second:  81, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.388679, mae: 13.646987, mean_q: 19.739166\n",
            " 145237/150000: episode: 21146, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [2.000, 8.000],  loss: 0.300119, mae: 13.795872, mean_q: 19.739082\n",
            " 145244/150000: episode: 21147, duration: 0.069s, episode steps:   7, steps per second: 102, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.315485, mae: 13.886977, mean_q: 19.889957\n",
            " 145253/150000: episode: 21148, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.309366, mae: 13.957062, mean_q: 19.789181\n",
            " 145260/150000: episode: 21149, duration: 0.063s, episode steps:   7, steps per second: 111, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.175928, mae: 13.775617, mean_q: 19.853514\n",
            " 145268/150000: episode: 21150, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.375 [0.000, 8.000],  loss: 0.313351, mae: 13.852747, mean_q: 20.040346\n",
            " 145276/150000: episode: 21151, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.193522, mae: 13.817399, mean_q: 19.958366\n",
            " 145284/150000: episode: 21152, duration: 0.069s, episode steps:   8, steps per second: 115, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.292438, mae: 13.877987, mean_q: 19.796406\n",
            " 145288/150000: episode: 21153, duration: 0.040s, episode steps:   4, steps per second: 101, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 6.250 [2.000, 8.000],  loss: 0.166568, mae: 13.864182, mean_q: 19.996580\n",
            " 145295/150000: episode: 21154, duration: 0.083s, episode steps:   7, steps per second:  84, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 7.000],  loss: 0.247755, mae: 13.849030, mean_q: 19.864393\n",
            " 145303/150000: episode: 21155, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.125 [0.000, 8.000],  loss: 0.240633, mae: 13.843418, mean_q: 20.074078\n",
            " 145308/150000: episode: 21156, duration: 0.049s, episode steps:   5, steps per second: 102, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.400 [2.000, 8.000],  loss: 0.242994, mae: 13.764221, mean_q: 19.988541\n",
            " 145313/150000: episode: 21157, duration: 0.051s, episode steps:   5, steps per second:  97, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.400 [2.000, 8.000],  loss: 0.216257, mae: 13.875429, mean_q: 19.896088\n",
            " 145321/150000: episode: 21158, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.311511, mae: 13.818412, mean_q: 19.941664\n",
            " 145327/150000: episode: 21159, duration: 0.055s, episode steps:   6, steps per second: 110, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.267342, mae: 14.049563, mean_q: 20.068991\n",
            " 145334/150000: episode: 21160, duration: 0.084s, episode steps:   7, steps per second:  83, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.273273, mae: 13.785792, mean_q: 19.822908\n",
            " 145343/150000: episode: 21161, duration: 0.133s, episode steps:   9, steps per second:  68, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.258510, mae: 13.851851, mean_q: 19.772938\n",
            " 145348/150000: episode: 21162, duration: 0.074s, episode steps:   5, steps per second:  68, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.205859, mae: 13.949324, mean_q: 19.728403\n",
            " 145357/150000: episode: 21163, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.238427, mae: 14.127876, mean_q: 19.831175\n",
            " 145364/150000: episode: 21164, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.286 [0.000, 8.000],  loss: 0.402447, mae: 13.701445, mean_q: 19.813522\n",
            " 145373/150000: episode: 21165, duration: 0.129s, episode steps:   9, steps per second:  70, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.345365, mae: 13.875443, mean_q: 19.940407\n",
            " 145379/150000: episode: 21166, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [2.000, 7.000],  loss: 0.258266, mae: 13.798642, mean_q: 20.188246\n",
            " 145386/150000: episode: 21167, duration: 0.104s, episode steps:   7, steps per second:  67, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.436390, mae: 14.242422, mean_q: 19.839855\n",
            " 145392/150000: episode: 21168, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.241355, mae: 14.026078, mean_q: 19.924650\n",
            " 145398/150000: episode: 21169, duration: 0.089s, episode steps:   6, steps per second:  67, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [1.000, 7.000],  loss: 0.411399, mae: 14.141301, mean_q: 19.650469\n",
            " 145406/150000: episode: 21170, duration: 0.108s, episode steps:   8, steps per second:  74, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.546412, mae: 14.051794, mean_q: 19.991730\n",
            " 145415/150000: episode: 21171, duration: 0.122s, episode steps:   9, steps per second:  74, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 1.011256, mae: 13.903796, mean_q: 19.490040\n",
            " 145422/150000: episode: 21172, duration: 0.087s, episode steps:   7, steps per second:  81, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.436738, mae: 13.841895, mean_q: 19.968676\n",
            " 145429/150000: episode: 21173, duration: 0.090s, episode steps:   7, steps per second:  78, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.286 [0.000, 6.000],  loss: 0.277577, mae: 13.581960, mean_q: 19.988661\n",
            " 145435/150000: episode: 21174, duration: 0.086s, episode steps:   6, steps per second:  70, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.500 [0.000, 8.000],  loss: 0.432364, mae: 14.353413, mean_q: 19.867729\n",
            " 145443/150000: episode: 21175, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 5.375 [2.000, 8.000],  loss: 0.709100, mae: 13.958828, mean_q: 19.822815\n",
            " 145451/150000: episode: 21176, duration: 0.118s, episode steps:   8, steps per second:  68, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.625 [2.000, 8.000],  loss: 0.354177, mae: 13.891733, mean_q: 19.838615\n",
            " 145460/150000: episode: 21177, duration: 0.122s, episode steps:   9, steps per second:  74, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.623380, mae: 13.786082, mean_q: 19.858299\n",
            " 145465/150000: episode: 21178, duration: 0.065s, episode steps:   5, steps per second:  77, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.337430, mae: 13.607840, mean_q: 19.936468\n",
            " 145472/150000: episode: 21179, duration: 0.103s, episode steps:   7, steps per second:  68, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.274488, mae: 13.978640, mean_q: 20.038630\n",
            " 145478/150000: episode: 21180, duration: 0.091s, episode steps:   6, steps per second:  66, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.000 [1.000, 7.000],  loss: 0.346679, mae: 13.742854, mean_q: 19.864801\n",
            " 145486/150000: episode: 21181, duration: 0.133s, episode steps:   8, steps per second:  60, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.576561, mae: 14.330002, mean_q: 19.842194\n",
            " 145495/150000: episode: 21182, duration: 0.122s, episode steps:   9, steps per second:  74, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.282731, mae: 13.963176, mean_q: 20.056450\n",
            " 145504/150000: episode: 21183, duration: 0.128s, episode steps:   9, steps per second:  70, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.444 [0.000, 8.000],  loss: 0.336494, mae: 13.778288, mean_q: 20.018757\n",
            " 145510/150000: episode: 21184, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 8.000],  loss: 0.813297, mae: 13.977031, mean_q: 19.649374\n",
            " 145517/150000: episode: 21185, duration: 0.094s, episode steps:   7, steps per second:  75, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 2.857 [0.000, 6.000],  loss: 0.437192, mae: 13.790509, mean_q: 19.822504\n",
            " 145523/150000: episode: 21186, duration: 0.104s, episode steps:   6, steps per second:  58, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.609473, mae: 14.382993, mean_q: 19.951853\n",
            " 145530/150000: episode: 21187, duration: 0.099s, episode steps:   7, steps per second:  71, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.283566, mae: 13.814575, mean_q: 19.950989\n",
            " 145538/150000: episode: 21188, duration: 0.121s, episode steps:   8, steps per second:  66, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.000 [0.000, 8.000],  loss: 0.319228, mae: 13.645661, mean_q: 20.000870\n",
            " 145544/150000: episode: 21189, duration: 0.093s, episode steps:   6, steps per second:  65, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.335423, mae: 13.897686, mean_q: 19.769678\n",
            " 145551/150000: episode: 21190, duration: 0.129s, episode steps:   7, steps per second:  54, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.318453, mae: 14.010829, mean_q: 19.896753\n",
            " 145557/150000: episode: 21191, duration: 0.099s, episode steps:   6, steps per second:  61, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.333 [2.000, 8.000],  loss: 0.413328, mae: 13.769422, mean_q: 19.851196\n",
            " 145563/150000: episode: 21192, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.214793, mae: 14.046271, mean_q: 19.937361\n",
            " 145570/150000: episode: 21193, duration: 0.082s, episode steps:   7, steps per second:  86, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.524658, mae: 13.722512, mean_q: 19.830614\n",
            " 145575/150000: episode: 21194, duration: 0.048s, episode steps:   5, steps per second: 105, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.308302, mae: 13.724714, mean_q: 19.766460\n",
            " 145580/150000: episode: 21195, duration: 0.046s, episode steps:   5, steps per second: 109, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.600 [2.000, 8.000],  loss: 0.271845, mae: 14.001633, mean_q: 20.054632\n",
            " 145587/150000: episode: 21196, duration: 0.069s, episode steps:   7, steps per second: 101, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.714 [1.000, 7.000],  loss: 0.300670, mae: 13.899922, mean_q: 19.845882\n",
            " 145593/150000: episode: 21197, duration: 0.069s, episode steps:   6, steps per second:  87, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 7.000],  loss: 0.438730, mae: 13.858738, mean_q: 19.990488\n",
            " 145602/150000: episode: 21198, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.230354, mae: 14.119761, mean_q: 19.718760\n",
            " 145607/150000: episode: 21199, duration: 0.047s, episode steps:   5, steps per second: 107, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.200 [2.000, 7.000],  loss: 0.618016, mae: 14.010782, mean_q: 20.095415\n",
            " 145613/150000: episode: 21200, duration: 0.055s, episode steps:   6, steps per second: 108, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.277934, mae: 13.829907, mean_q: 19.772699\n",
            " 145619/150000: episode: 21201, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.000 [1.000, 8.000],  loss: 0.455583, mae: 13.549323, mean_q: 19.883928\n",
            " 145623/150000: episode: 21202, duration: 0.040s, episode steps:   4, steps per second: 100, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.000 [3.000, 7.000],  loss: 1.152861, mae: 13.590973, mean_q: 19.798552\n",
            " 145632/150000: episode: 21203, duration: 0.087s, episode steps:   9, steps per second: 104, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.542106, mae: 13.808439, mean_q: 19.609955\n",
            " 145640/150000: episode: 21204, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.361317, mae: 14.035644, mean_q: 20.063789\n",
            " 145648/150000: episode: 21205, duration: 0.094s, episode steps:   8, steps per second:  85, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.265649, mae: 13.959367, mean_q: 19.750645\n",
            " 145656/150000: episode: 21206, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.375 [0.000, 8.000],  loss: 0.457419, mae: 13.654463, mean_q: 19.894421\n",
            " 145665/150000: episode: 21207, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.277292, mae: 14.024629, mean_q: 20.048077\n",
            " 145673/150000: episode: 21208, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.261091, mae: 14.012701, mean_q: 20.068493\n",
            " 145681/150000: episode: 21209, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.419893, mae: 13.859369, mean_q: 19.782555\n",
            " 145690/150000: episode: 21210, duration: 0.130s, episode steps:   9, steps per second:  69, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.434576, mae: 13.590177, mean_q: 19.856672\n",
            " 145699/150000: episode: 21211, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.221588, mae: 13.851381, mean_q: 19.931608\n",
            " 145705/150000: episode: 21212, duration: 0.077s, episode steps:   6, steps per second:  78, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.833 [1.000, 8.000],  loss: 0.236145, mae: 14.072192, mean_q: 19.835861\n",
            " 145714/150000: episode: 21213, duration: 0.143s, episode steps:   9, steps per second:  63, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.111 [0.000, 7.000],  loss: 0.409947, mae: 14.043546, mean_q: 19.774206\n",
            " 145720/150000: episode: 21214, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.333 [0.000, 6.000],  loss: 0.219240, mae: 14.132401, mean_q: 19.904436\n",
            " 145728/150000: episode: 21215, duration: 0.106s, episode steps:   8, steps per second:  75, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.157066, mae: 13.957176, mean_q: 19.883869\n",
            " 145737/150000: episode: 21216, duration: 0.127s, episode steps:   9, steps per second:  71, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.601447, mae: 13.953182, mean_q: 19.902617\n",
            " 145744/150000: episode: 21217, duration: 0.090s, episode steps:   7, steps per second:  78, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.571 [1.000, 7.000],  loss: 0.335576, mae: 14.149353, mean_q: 19.989620\n",
            " 145749/150000: episode: 21218, duration: 0.064s, episode steps:   5, steps per second:  78, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.600 [0.000, 6.000],  loss: 0.336427, mae: 13.751928, mean_q: 20.030298\n",
            " 145757/150000: episode: 21219, duration: 0.120s, episode steps:   8, steps per second:  67, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.263766, mae: 14.174375, mean_q: 20.129040\n",
            " 145764/150000: episode: 21220, duration: 0.086s, episode steps:   7, steps per second:  81, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.203086, mae: 13.823936, mean_q: 19.867035\n",
            " 145769/150000: episode: 21221, duration: 0.063s, episode steps:   5, steps per second:  79, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [1.000, 8.000],  loss: 0.459440, mae: 13.667589, mean_q: 19.842503\n",
            " 145774/150000: episode: 21222, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.600 [3.000, 8.000],  loss: 0.351138, mae: 13.833798, mean_q: 19.647720\n",
            " 145781/150000: episode: 21223, duration: 0.088s, episode steps:   7, steps per second:  79, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.751730, mae: 13.893511, mean_q: 20.060575\n",
            " 145789/150000: episode: 21224, duration: 0.098s, episode steps:   8, steps per second:  82, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.236088, mae: 13.986734, mean_q: 19.947634\n",
            " 145798/150000: episode: 21225, duration: 0.118s, episode steps:   9, steps per second:  76, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.206157, mae: 13.856837, mean_q: 19.835428\n",
            " 145804/150000: episode: 21226, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [1.000, 8.000],  loss: 0.528595, mae: 13.753682, mean_q: 19.976938\n",
            " 145809/150000: episode: 21227, duration: 0.081s, episode steps:   5, steps per second:  62, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.400 [0.000, 6.000],  loss: 0.326108, mae: 13.802419, mean_q: 20.004522\n",
            " 145816/150000: episode: 21228, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.257417, mae: 14.102980, mean_q: 20.030888\n",
            " 145822/150000: episode: 21229, duration: 0.078s, episode steps:   6, steps per second:  77, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.642967, mae: 13.944631, mean_q: 20.009718\n",
            " 145830/150000: episode: 21230, duration: 0.131s, episode steps:   8, steps per second:  61, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.183421, mae: 14.077234, mean_q: 19.761765\n",
            " 145839/150000: episode: 21231, duration: 0.126s, episode steps:   9, steps per second:  71, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.324027, mae: 13.699287, mean_q: 19.953873\n",
            " 145848/150000: episode: 21232, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.243684, mae: 13.998958, mean_q: 19.884983\n",
            " 145856/150000: episode: 21233, duration: 0.112s, episode steps:   8, steps per second:  71, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.463981, mae: 14.159840, mean_q: 19.707436\n",
            " 145864/150000: episode: 21234, duration: 0.115s, episode steps:   8, steps per second:  70, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.750 [0.000, 8.000],  loss: 0.244715, mae: 13.931086, mean_q: 19.964687\n",
            " 145871/150000: episode: 21235, duration: 0.102s, episode steps:   7, steps per second:  69, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.347508, mae: 13.978995, mean_q: 19.910212\n",
            " 145876/150000: episode: 21236, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.908970, mae: 13.783948, mean_q: 19.818329\n",
            " 145882/150000: episode: 21237, duration: 0.092s, episode steps:   6, steps per second:  65, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [1.000, 8.000],  loss: 0.423145, mae: 13.999806, mean_q: 20.253233\n",
            " 145889/150000: episode: 21238, duration: 0.105s, episode steps:   7, steps per second:  67, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.585107, mae: 13.606691, mean_q: 19.891296\n",
            " 145892/150000: episode: 21239, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 6.667 [4.000, 8.000],  loss: 0.412385, mae: 13.568738, mean_q: 20.021097\n",
            " 145900/150000: episode: 21240, duration: 0.130s, episode steps:   8, steps per second:  62, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.324322, mae: 13.784119, mean_q: 20.093441\n",
            " 145902/150000: episode: 21241, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 3.000 [3.000, 3.000],  loss: 0.287015, mae: 13.395674, mean_q: 19.668266\n",
            " 145906/150000: episode: 21242, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.250 [4.000, 8.000],  loss: 0.575619, mae: 13.538884, mean_q: 19.571012\n",
            " 145913/150000: episode: 21243, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.857 [0.000, 8.000],  loss: 0.266827, mae: 13.924438, mean_q: 20.095373\n",
            " 145921/150000: episode: 21244, duration: 0.113s, episode steps:   8, steps per second:  71, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.382944, mae: 13.787046, mean_q: 19.663158\n",
            " 145928/150000: episode: 21245, duration: 0.100s, episode steps:   7, steps per second:  70, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.317688, mae: 13.819639, mean_q: 20.031307\n",
            " 145936/150000: episode: 21246, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.204621, mae: 13.993429, mean_q: 19.941097\n",
            " 145943/150000: episode: 21247, duration: 0.095s, episode steps:   7, steps per second:  73, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.247906, mae: 13.884512, mean_q: 19.858242\n",
            " 145948/150000: episode: 21248, duration: 0.050s, episode steps:   5, steps per second:  99, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.800 [1.000, 7.000],  loss: 0.467176, mae: 13.584661, mean_q: 19.983923\n",
            " 145955/150000: episode: 21249, duration: 0.067s, episode steps:   7, steps per second: 104, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.262999, mae: 13.797551, mean_q: 19.890865\n",
            " 145964/150000: episode: 21250, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.292748, mae: 13.944519, mean_q: 20.151382\n",
            " 145969/150000: episode: 21251, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.279556, mae: 13.863544, mean_q: 19.792500\n",
            " 145976/150000: episode: 21252, duration: 0.075s, episode steps:   7, steps per second:  93, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [1.000, 8.000],  loss: 0.212037, mae: 13.779061, mean_q: 19.959747\n",
            " 145982/150000: episode: 21253, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 6.000 [2.000, 8.000],  loss: 0.476305, mae: 13.725697, mean_q: 19.830782\n",
            " 145989/150000: episode: 21254, duration: 0.077s, episode steps:   7, steps per second:  91, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [1.000, 8.000],  loss: 0.358034, mae: 13.847497, mean_q: 19.928167\n",
            " 145995/150000: episode: 21255, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2.833 [0.000, 6.000],  loss: 0.894124, mae: 13.479999, mean_q: 19.865870\n",
            " 146002/150000: episode: 21256, duration: 0.067s, episode steps:   7, steps per second: 104, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.143 [0.000, 8.000],  loss: 0.643301, mae: 13.812810, mean_q: 19.744455\n",
            " 146008/150000: episode: 21257, duration: 0.071s, episode steps:   6, steps per second:  84, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [1.000, 8.000],  loss: 0.620738, mae: 14.156101, mean_q: 20.014614\n",
            " 146014/150000: episode: 21258, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.408052, mae: 13.939288, mean_q: 19.937735\n",
            " 146020/150000: episode: 21259, duration: 0.059s, episode steps:   6, steps per second: 102, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 2.500 [0.000, 7.000],  loss: 0.230282, mae: 13.754687, mean_q: 19.944197\n",
            " 146026/150000: episode: 21260, duration: 0.063s, episode steps:   6, steps per second:  95, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 7.000],  loss: 0.274891, mae: 13.846362, mean_q: 20.119675\n",
            " 146033/150000: episode: 21261, duration: 0.079s, episode steps:   7, steps per second:  89, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.143 [0.000, 8.000],  loss: 0.507394, mae: 13.753448, mean_q: 19.849722\n",
            " 146038/150000: episode: 21262, duration: 0.046s, episode steps:   5, steps per second: 108, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 6.600 [5.000, 8.000],  loss: 0.275441, mae: 13.870196, mean_q: 20.078377\n",
            " 146043/150000: episode: 21263, duration: 0.047s, episode steps:   5, steps per second: 105, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [0.000, 8.000],  loss: 0.344693, mae: 14.059016, mean_q: 20.141590\n",
            " 146049/150000: episode: 21264, duration: 0.055s, episode steps:   6, steps per second: 110, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.500 [1.000, 7.000],  loss: 0.372291, mae: 13.938062, mean_q: 19.949694\n",
            " 146057/150000: episode: 21265, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.000 [0.000, 8.000],  loss: 0.479155, mae: 14.023080, mean_q: 19.971378\n",
            " 146064/150000: episode: 21266, duration: 0.081s, episode steps:   7, steps per second:  87, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.857 [1.000, 8.000],  loss: 0.282322, mae: 13.764694, mean_q: 19.754927\n",
            " 146068/150000: episode: 21267, duration: 0.051s, episode steps:   4, steps per second:  79, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.750 [2.000, 8.000],  loss: 0.283192, mae: 14.290768, mean_q: 19.899971\n",
            " 146075/150000: episode: 21268, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.257263, mae: 13.783063, mean_q: 20.107336\n",
            " 146081/150000: episode: 21269, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.309449, mae: 13.851598, mean_q: 20.111624\n",
            " 146088/150000: episode: 21270, duration: 0.069s, episode steps:   7, steps per second: 102, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.317951, mae: 13.726440, mean_q: 19.838823\n",
            " 146095/150000: episode: 21271, duration: 0.065s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.195345, mae: 14.011885, mean_q: 19.797485\n",
            " 146100/150000: episode: 21272, duration: 0.048s, episode steps:   5, steps per second: 103, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [0.000, 8.000],  loss: 0.561280, mae: 13.599783, mean_q: 19.707018\n",
            " 146107/150000: episode: 21273, duration: 0.083s, episode steps:   7, steps per second:  85, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.714 [1.000, 8.000],  loss: 0.399253, mae: 14.080927, mean_q: 20.062475\n",
            " 146114/150000: episode: 21274, duration: 0.067s, episode steps:   7, steps per second: 104, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.571 [0.000, 7.000],  loss: 1.201132, mae: 14.127015, mean_q: 19.841623\n",
            " 146120/150000: episode: 21275, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [0.000, 8.000],  loss: 0.649575, mae: 13.814358, mean_q: 19.897303\n",
            " 146127/150000: episode: 21276, duration: 0.063s, episode steps:   7, steps per second: 111, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.623606, mae: 13.928815, mean_q: 20.094828\n",
            " 146134/150000: episode: 21277, duration: 0.076s, episode steps:   7, steps per second:  92, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.434694, mae: 13.871886, mean_q: 19.837164\n",
            " 146140/150000: episode: 21278, duration: 0.064s, episode steps:   6, steps per second:  94, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.441417, mae: 13.829421, mean_q: 19.752272\n",
            " 146147/150000: episode: 21279, duration: 0.070s, episode steps:   7, steps per second: 100, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.350354, mae: 13.803613, mean_q: 19.885881\n",
            " 146155/150000: episode: 21280, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.336176, mae: 14.053053, mean_q: 19.956287\n",
            " 146162/150000: episode: 21281, duration: 0.080s, episode steps:   7, steps per second:  88, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.206878, mae: 13.783667, mean_q: 19.994732\n",
            " 146170/150000: episode: 21282, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.217749, mae: 13.893590, mean_q: 19.828392\n",
            " 146177/150000: episode: 21283, duration: 0.078s, episode steps:   7, steps per second:  90, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.143 [0.000, 8.000],  loss: 0.551246, mae: 13.903152, mean_q: 19.682570\n",
            " 146184/150000: episode: 21284, duration: 0.063s, episode steps:   7, steps per second: 111, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 1.203141, mae: 13.918765, mean_q: 19.838165\n",
            " 146189/150000: episode: 21285, duration: 0.048s, episode steps:   5, steps per second: 105, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.400 [2.000, 8.000],  loss: 0.254611, mae: 13.816801, mean_q: 19.940378\n",
            " 146193/150000: episode: 21286, duration: 0.041s, episode steps:   4, steps per second:  97, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.750 [4.000, 8.000],  loss: 0.290519, mae: 13.739298, mean_q: 20.190624\n",
            " 146198/150000: episode: 21287, duration: 0.052s, episode steps:   5, steps per second:  96, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.600 [0.000, 6.000],  loss: 0.404790, mae: 14.108617, mean_q: 20.059486\n",
            " 146207/150000: episode: 21288, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.345131, mae: 14.139637, mean_q: 19.877707\n",
            " 146214/150000: episode: 21289, duration: 0.070s, episode steps:   7, steps per second: 100, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.539455, mae: 13.435865, mean_q: 19.971880\n",
            " 146220/150000: episode: 21290, duration: 0.067s, episode steps:   6, steps per second:  90, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.333 [1.000, 8.000],  loss: 0.847274, mae: 13.853604, mean_q: 19.859030\n",
            " 146228/150000: episode: 21291, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.000 [0.000, 8.000],  loss: 0.302386, mae: 13.935024, mean_q: 20.018162\n",
            " 146236/150000: episode: 21292, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.750 [0.000, 8.000],  loss: 0.300525, mae: 14.110235, mean_q: 20.051254\n",
            " 146242/150000: episode: 21293, duration: 0.066s, episode steps:   6, steps per second:  90, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.833 [0.000, 8.000],  loss: 0.335263, mae: 13.934120, mean_q: 19.830687\n",
            " 146249/150000: episode: 21294, duration: 0.068s, episode steps:   7, steps per second: 102, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.857 [0.000, 8.000],  loss: 0.390168, mae: 13.877261, mean_q: 20.017597\n",
            " 146256/150000: episode: 21295, duration: 0.066s, episode steps:   7, steps per second: 107, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.571 [1.000, 6.000],  loss: 0.505869, mae: 14.027926, mean_q: 20.012817\n",
            " 146263/150000: episode: 21296, duration: 0.098s, episode steps:   7, steps per second:  71, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.143 [0.000, 8.000],  loss: 0.340443, mae: 13.743644, mean_q: 19.759434\n",
            " 146269/150000: episode: 21297, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.000 [2.000, 8.000],  loss: 0.603781, mae: 13.753551, mean_q: 20.125479\n",
            " 146276/150000: episode: 21298, duration: 0.068s, episode steps:   7, steps per second: 103, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.409073, mae: 13.785068, mean_q: 19.814581\n",
            " 146282/150000: episode: 21299, duration: 0.056s, episode steps:   6, steps per second: 106, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 6.000],  loss: 0.291585, mae: 13.939731, mean_q: 19.957911\n",
            " 146288/150000: episode: 21300, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [1.000, 8.000],  loss: 0.391688, mae: 13.955810, mean_q: 19.939867\n",
            " 146293/150000: episode: 21301, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.800 [0.000, 5.000],  loss: 0.269178, mae: 13.722730, mean_q: 19.791300\n",
            " 146301/150000: episode: 21302, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.445261, mae: 13.821968, mean_q: 19.843716\n",
            " 146309/150000: episode: 21303, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.243344, mae: 14.168847, mean_q: 20.127251\n",
            " 146316/150000: episode: 21304, duration: 0.082s, episode steps:   7, steps per second:  85, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.317162, mae: 13.927530, mean_q: 19.777752\n",
            " 146324/150000: episode: 21305, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.625 [0.000, 8.000],  loss: 0.395476, mae: 13.619860, mean_q: 19.726490\n",
            " 146333/150000: episode: 21306, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.502199, mae: 13.995615, mean_q: 19.920448\n",
            " 146341/150000: episode: 21307, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.290037, mae: 14.184295, mean_q: 20.100138\n",
            " 146346/150000: episode: 21308, duration: 0.054s, episode steps:   5, steps per second:  93, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.800 [0.000, 5.000],  loss: 0.300941, mae: 13.687884, mean_q: 20.007511\n",
            " 146353/150000: episode: 21309, duration: 0.065s, episode steps:   7, steps per second: 107, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.571 [0.000, 7.000],  loss: 0.440548, mae: 13.589976, mean_q: 19.648346\n",
            " 146360/150000: episode: 21310, duration: 0.075s, episode steps:   7, steps per second:  93, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.429 [0.000, 8.000],  loss: 0.267098, mae: 13.801619, mean_q: 20.015532\n",
            " 146364/150000: episode: 21311, duration: 0.064s, episode steps:   4, steps per second:  62, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 6.500 [4.000, 8.000],  loss: 0.239314, mae: 14.020327, mean_q: 19.722984\n",
            " 146371/150000: episode: 21312, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.538431, mae: 13.765672, mean_q: 19.863831\n",
            " 146379/150000: episode: 21313, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.301367, mae: 14.080540, mean_q: 19.778088\n",
            " 146386/150000: episode: 21314, duration: 0.061s, episode steps:   7, steps per second: 115, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 2.714 [0.000, 6.000],  loss: 0.385097, mae: 14.132976, mean_q: 20.071110\n",
            " 146392/150000: episode: 21315, duration: 0.066s, episode steps:   6, steps per second:  90, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.308776, mae: 13.994882, mean_q: 19.815004\n",
            " 146400/150000: episode: 21316, duration: 0.070s, episode steps:   8, steps per second: 115, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.334184, mae: 13.927515, mean_q: 19.941957\n",
            " 146407/150000: episode: 21317, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.400977, mae: 13.830488, mean_q: 19.906790\n",
            " 146413/150000: episode: 21318, duration: 0.053s, episode steps:   6, steps per second: 112, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.667 [2.000, 7.000],  loss: 0.342598, mae: 14.325480, mean_q: 19.794416\n",
            " 146422/150000: episode: 21319, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.568908, mae: 14.166271, mean_q: 19.900640\n",
            " 146431/150000: episode: 21320, duration: 0.080s, episode steps:   9, steps per second: 112, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.251885, mae: 13.976247, mean_q: 19.949270\n",
            " 146439/150000: episode: 21321, duration: 0.093s, episode steps:   8, steps per second:  86, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.767436, mae: 13.821292, mean_q: 20.043936\n",
            " 146443/150000: episode: 21322, duration: 0.080s, episode steps:   4, steps per second:  50, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.250 [2.000, 8.000],  loss: 0.211758, mae: 13.928077, mean_q: 19.898653\n",
            " 146450/150000: episode: 21323, duration: 0.107s, episode steps:   7, steps per second:  65, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.710873, mae: 14.254632, mean_q: 19.941061\n",
            " 146458/150000: episode: 21324, duration: 0.129s, episode steps:   8, steps per second:  62, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.250 [0.000, 8.000],  loss: 0.601520, mae: 14.057695, mean_q: 19.792122\n",
            " 146461/150000: episode: 21325, duration: 0.053s, episode steps:   3, steps per second:  56, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 7.333 [7.000, 8.000],  loss: 0.320756, mae: 14.037318, mean_q: 19.980749\n",
            " 146468/150000: episode: 21326, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 7.000],  loss: 0.524430, mae: 13.955890, mean_q: 19.943985\n",
            " 146476/150000: episode: 21327, duration: 0.122s, episode steps:   8, steps per second:  66, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.343041, mae: 14.061234, mean_q: 20.065365\n",
            " 146485/150000: episode: 21328, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.340135, mae: 13.537937, mean_q: 19.882851\n",
            " 146491/150000: episode: 21329, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 6.000],  loss: 0.429335, mae: 13.811872, mean_q: 19.791292\n",
            " 146496/150000: episode: 21330, duration: 0.076s, episode steps:   5, steps per second:  66, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.000 [3.000, 8.000],  loss: 0.644914, mae: 13.861288, mean_q: 19.636486\n",
            " 146501/150000: episode: 21331, duration: 0.068s, episode steps:   5, steps per second:  73, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.200 [2.000, 6.000],  loss: 0.838229, mae: 13.729927, mean_q: 20.100035\n",
            " 146508/150000: episode: 21332, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.143 [0.000, 8.000],  loss: 0.277618, mae: 13.826889, mean_q: 20.065105\n",
            " 146516/150000: episode: 21333, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.292066, mae: 13.716292, mean_q: 19.942022\n",
            " 146524/150000: episode: 21334, duration: 0.097s, episode steps:   8, steps per second:  82, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.287223, mae: 13.614639, mean_q: 19.760786\n",
            " 146531/150000: episode: 21335, duration: 0.089s, episode steps:   7, steps per second:  78, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.201419, mae: 14.107373, mean_q: 19.954956\n",
            " 146539/150000: episode: 21336, duration: 0.111s, episode steps:   8, steps per second:  72, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.466198, mae: 13.769669, mean_q: 19.780699\n",
            " 146545/150000: episode: 21337, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.306771, mae: 13.688001, mean_q: 19.706978\n",
            " 146552/150000: episode: 21338, duration: 0.087s, episode steps:   7, steps per second:  81, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [1.000, 8.000],  loss: 0.421327, mae: 13.881300, mean_q: 20.087177\n",
            " 146556/150000: episode: 21339, duration: 0.064s, episode steps:   4, steps per second:  63, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.250 [2.000, 8.000],  loss: 0.247705, mae: 14.401931, mean_q: 19.890110\n",
            " 146560/150000: episode: 21340, duration: 0.059s, episode steps:   4, steps per second:  68, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 1.250 [0.000, 3.000],  loss: 0.444160, mae: 13.868323, mean_q: 19.532721\n",
            " 146565/150000: episode: 21341, duration: 0.073s, episode steps:   5, steps per second:  69, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.178999, mae: 14.123331, mean_q: 19.994995\n",
            " 146574/150000: episode: 21342, duration: 0.130s, episode steps:   9, steps per second:  69, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.260360, mae: 13.936387, mean_q: 19.941669\n",
            " 146582/150000: episode: 21343, duration: 0.117s, episode steps:   8, steps per second:  69, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.250 [0.000, 7.000],  loss: 0.699242, mae: 13.437503, mean_q: 19.765751\n",
            " 146587/150000: episode: 21344, duration: 0.076s, episode steps:   5, steps per second:  66, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.431838, mae: 13.569166, mean_q: 20.076115\n",
            " 146593/150000: episode: 21345, duration: 0.128s, episode steps:   6, steps per second:  47, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [1.000, 8.000],  loss: 0.321358, mae: 14.235827, mean_q: 20.184288\n",
            " 146601/150000: episode: 21346, duration: 0.130s, episode steps:   8, steps per second:  62, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.409492, mae: 14.061172, mean_q: 20.079344\n",
            " 146610/150000: episode: 21347, duration: 0.146s, episode steps:   9, steps per second:  62, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.608337, mae: 13.763391, mean_q: 19.912983\n",
            " 146615/150000: episode: 21348, duration: 0.071s, episode steps:   5, steps per second:  71, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.434361, mae: 14.008150, mean_q: 20.032084\n",
            " 146620/150000: episode: 21349, duration: 0.071s, episode steps:   5, steps per second:  71, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.200 [0.000, 4.000],  loss: 0.281259, mae: 14.019987, mean_q: 19.919621\n",
            " 146628/150000: episode: 21350, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.228911, mae: 14.041792, mean_q: 19.933245\n",
            " 146637/150000: episode: 21351, duration: 0.116s, episode steps:   9, steps per second:  77, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.405372, mae: 13.928399, mean_q: 20.022266\n",
            " 146645/150000: episode: 21352, duration: 0.109s, episode steps:   8, steps per second:  73, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.250 [0.000, 8.000],  loss: 0.247058, mae: 14.224054, mean_q: 19.910789\n",
            " 146651/150000: episode: 21353, duration: 0.083s, episode steps:   6, steps per second:  73, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.316378, mae: 14.129822, mean_q: 19.843882\n",
            " 146656/150000: episode: 21354, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.247493, mae: 13.731100, mean_q: 19.903776\n",
            " 146665/150000: episode: 21355, duration: 0.144s, episode steps:   9, steps per second:  62, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.234722, mae: 13.594997, mean_q: 20.074512\n",
            " 146671/150000: episode: 21356, duration: 0.091s, episode steps:   6, steps per second:  66, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.000 [2.000, 6.000],  loss: 0.379622, mae: 13.897587, mean_q: 19.913403\n",
            " 146676/150000: episode: 21357, duration: 0.076s, episode steps:   5, steps per second:  66, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.236323, mae: 14.045291, mean_q: 19.907879\n",
            " 146683/150000: episode: 21358, duration: 0.099s, episode steps:   7, steps per second:  71, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 5.143 [3.000, 8.000],  loss: 0.332273, mae: 13.838090, mean_q: 19.931273\n",
            " 146692/150000: episode: 21359, duration: 0.082s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.531429, mae: 13.927671, mean_q: 19.970743\n",
            " 146699/150000: episode: 21360, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.291900, mae: 13.860900, mean_q: 19.896748\n",
            " 146705/150000: episode: 21361, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.277457, mae: 13.889042, mean_q: 19.921268\n",
            " 146708/150000: episode: 21362, duration: 0.045s, episode steps:   3, steps per second:  67, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 4.000 [2.000, 5.000],  loss: 0.429159, mae: 13.731273, mean_q: 19.886786\n",
            " 146713/150000: episode: 21363, duration: 0.051s, episode steps:   5, steps per second:  97, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.800 [1.000, 7.000],  loss: 0.387307, mae: 14.096791, mean_q: 19.886948\n",
            " 146719/150000: episode: 21364, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.833 [0.000, 7.000],  loss: 0.746483, mae: 13.554245, mean_q: 19.833185\n",
            " 146726/150000: episode: 21365, duration: 0.072s, episode steps:   7, steps per second:  97, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 2.143 [0.000, 5.000],  loss: 0.400955, mae: 13.667371, mean_q: 19.888842\n",
            " 146734/150000: episode: 21366, duration: 0.079s, episode steps:   8, steps per second: 101, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.653424, mae: 13.965874, mean_q: 19.861832\n",
            " 146743/150000: episode: 21367, duration: 0.095s, episode steps:   9, steps per second:  94, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.310389, mae: 13.823870, mean_q: 20.133316\n",
            " 146750/150000: episode: 21368, duration: 0.084s, episode steps:   7, steps per second:  83, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.143 [1.000, 7.000],  loss: 0.766797, mae: 13.811395, mean_q: 19.887342\n",
            " 146755/150000: episode: 21369, duration: 0.049s, episode steps:   5, steps per second: 101, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.410973, mae: 13.941507, mean_q: 19.796642\n",
            " 146761/150000: episode: 21370, duration: 0.057s, episode steps:   6, steps per second: 106, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.333 [2.000, 8.000],  loss: 0.265165, mae: 13.802625, mean_q: 20.044907\n",
            " 146770/150000: episode: 21371, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.289399, mae: 13.806983, mean_q: 19.853504\n",
            " 146777/150000: episode: 21372, duration: 0.090s, episode steps:   7, steps per second:  78, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.580946, mae: 14.026276, mean_q: 19.874172\n",
            " 146783/150000: episode: 21373, duration: 0.057s, episode steps:   6, steps per second: 106, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.167 [2.000, 8.000],  loss: 0.548024, mae: 13.735520, mean_q: 19.928888\n",
            " 146790/150000: episode: 21374, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [0.000, 8.000],  loss: 0.456518, mae: 13.928162, mean_q: 20.011332\n",
            " 146798/150000: episode: 21375, duration: 0.071s, episode steps:   8, steps per second: 112, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.728324, mae: 13.690002, mean_q: 19.862839\n",
            " 146807/150000: episode: 21376, duration: 0.097s, episode steps:   9, steps per second:  93, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.468261, mae: 13.521961, mean_q: 19.861122\n",
            " 146809/150000: episode: 21377, duration: 0.027s, episode steps:   2, steps per second:  75, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.236505, mae: 13.871799, mean_q: 20.074839\n",
            " 146817/150000: episode: 21378, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.252845, mae: 13.878456, mean_q: 20.154881\n",
            " 146826/150000: episode: 21379, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.395034, mae: 13.500974, mean_q: 19.945429\n",
            " 146830/150000: episode: 21380, duration: 0.055s, episode steps:   4, steps per second:  72, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 2.000 [1.000, 3.000],  loss: 0.725831, mae: 14.079507, mean_q: 20.146307\n",
            " 146837/150000: episode: 21381, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.465361, mae: 13.910916, mean_q: 19.650482\n",
            " 146845/150000: episode: 21382, duration: 0.083s, episode steps:   8, steps per second:  96, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.674277, mae: 14.108079, mean_q: 20.173550\n",
            " 146851/150000: episode: 21383, duration: 0.062s, episode steps:   6, steps per second:  97, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2.667 [0.000, 6.000],  loss: 0.530498, mae: 14.048251, mean_q: 19.798090\n",
            " 146857/150000: episode: 21384, duration: 0.062s, episode steps:   6, steps per second:  97, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.434221, mae: 14.279809, mean_q: 20.109804\n",
            " 146866/150000: episode: 21385, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.338529, mae: 13.848656, mean_q: 19.838875\n",
            " 146873/150000: episode: 21386, duration: 0.074s, episode steps:   7, steps per second:  94, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.857 [1.000, 8.000],  loss: 0.314267, mae: 14.005429, mean_q: 19.910429\n",
            " 146880/150000: episode: 21387, duration: 0.078s, episode steps:   7, steps per second:  90, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.000 [0.000, 7.000],  loss: 0.175897, mae: 13.657293, mean_q: 19.889465\n",
            " 146888/150000: episode: 21388, duration: 0.072s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.325552, mae: 13.651134, mean_q: 20.014256\n",
            " 146895/150000: episode: 21389, duration: 0.073s, episode steps:   7, steps per second:  96, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.298437, mae: 13.728605, mean_q: 20.036312\n",
            " 146904/150000: episode: 21390, duration: 0.084s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.861096, mae: 14.077866, mean_q: 19.876320\n",
            " 146911/150000: episode: 21391, duration: 0.063s, episode steps:   7, steps per second: 112, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.314260, mae: 13.949980, mean_q: 19.874147\n",
            " 146920/150000: episode: 21392, duration: 0.086s, episode steps:   9, steps per second: 104, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.203654, mae: 13.945977, mean_q: 19.954821\n",
            " 146928/150000: episode: 21393, duration: 0.071s, episode steps:   8, steps per second: 112, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.305086, mae: 13.896465, mean_q: 19.795128\n",
            " 146937/150000: episode: 21394, duration: 0.089s, episode steps:   9, steps per second: 102, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.227399, mae: 14.211771, mean_q: 20.033569\n",
            " 146946/150000: episode: 21395, duration: 0.103s, episode steps:   9, steps per second:  88, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.416797, mae: 13.860090, mean_q: 19.870083\n",
            " 146953/150000: episode: 21396, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.143 [0.000, 6.000],  loss: 0.264102, mae: 14.398789, mean_q: 19.939854\n",
            " 146959/150000: episode: 21397, duration: 0.055s, episode steps:   6, steps per second: 110, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [1.000, 8.000],  loss: 0.717551, mae: 14.047550, mean_q: 20.148092\n",
            " 146968/150000: episode: 21398, duration: 0.081s, episode steps:   9, steps per second: 112, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.556 [0.000, 8.000],  loss: 0.560912, mae: 14.086357, mean_q: 20.080633\n",
            " 146975/150000: episode: 21399, duration: 0.073s, episode steps:   7, steps per second:  96, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.977304, mae: 13.798958, mean_q: 20.049580\n",
            " 146984/150000: episode: 21400, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.374391, mae: 13.778868, mean_q: 19.675327\n",
            " 146988/150000: episode: 21401, duration: 0.040s, episode steps:   4, steps per second: 101, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.500 [0.000, 6.000],  loss: 0.611810, mae: 13.559295, mean_q: 20.424774\n",
            " 146996/150000: episode: 21402, duration: 0.073s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.265486, mae: 13.800335, mean_q: 19.875805\n",
            " 147002/150000: episode: 21403, duration: 0.067s, episode steps:   6, steps per second:  90, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [1.000, 7.000],  loss: 0.275512, mae: 13.988175, mean_q: 19.952400\n",
            " 147008/150000: episode: 21404, duration: 0.056s, episode steps:   6, steps per second: 108, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.458160, mae: 14.078189, mean_q: 19.785555\n",
            " 147013/150000: episode: 21405, duration: 0.048s, episode steps:   5, steps per second: 103, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.800 [0.000, 8.000],  loss: 0.383806, mae: 14.018372, mean_q: 19.930645\n",
            " 147019/150000: episode: 21406, duration: 0.068s, episode steps:   6, steps per second:  88, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.539054, mae: 13.964992, mean_q: 19.850740\n",
            " 147028/150000: episode: 21407, duration: 0.088s, episode steps:   9, steps per second: 103, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.111 [1.000, 8.000],  loss: 0.631812, mae: 13.760487, mean_q: 19.926741\n",
            " 147036/150000: episode: 21408, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.480199, mae: 13.615770, mean_q: 20.033600\n",
            " 147041/150000: episode: 21409, duration: 0.065s, episode steps:   5, steps per second:  77, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.000 [2.000, 7.000],  loss: 0.833212, mae: 13.873342, mean_q: 20.016180\n",
            " 147047/150000: episode: 21410, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.913741, mae: 13.850924, mean_q: 19.791227\n",
            " 147055/150000: episode: 21411, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.380088, mae: 13.822273, mean_q: 20.087078\n",
            " 147064/150000: episode: 21412, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.323766, mae: 13.943546, mean_q: 20.026913\n",
            " 147069/150000: episode: 21413, duration: 0.051s, episode steps:   5, steps per second:  99, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.800 [2.000, 8.000],  loss: 0.600809, mae: 13.548528, mean_q: 19.864321\n",
            " 147075/150000: episode: 21414, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.500 [1.000, 7.000],  loss: 0.322541, mae: 14.154118, mean_q: 19.846933\n",
            " 147082/150000: episode: 21415, duration: 0.066s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.290890, mae: 14.225119, mean_q: 20.193888\n",
            " 147090/150000: episode: 21416, duration: 0.097s, episode steps:   8, steps per second:  82, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.250 [0.000, 8.000],  loss: 0.487702, mae: 13.799870, mean_q: 19.803104\n",
            " 147099/150000: episode: 21417, duration: 0.077s, episode steps:   9, steps per second: 117, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.465465, mae: 14.012238, mean_q: 20.183826\n",
            " 147108/150000: episode: 21418, duration: 0.074s, episode steps:   9, steps per second: 122, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.398032, mae: 13.669975, mean_q: 19.961300\n",
            " 147116/150000: episode: 21419, duration: 0.085s, episode steps:   8, steps per second:  94, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.418611, mae: 13.751507, mean_q: 19.994738\n",
            " 147124/150000: episode: 21420, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.451931, mae: 13.950077, mean_q: 19.868092\n",
            " 147130/150000: episode: 21421, duration: 0.054s, episode steps:   6, steps per second: 111, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 6.000],  loss: 0.255366, mae: 13.836433, mean_q: 20.053961\n",
            " 147136/150000: episode: 21422, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.849198, mae: 13.912311, mean_q: 19.873507\n",
            " 147144/150000: episode: 21423, duration: 0.091s, episode steps:   8, steps per second:  88, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.398142, mae: 13.963343, mean_q: 20.028633\n",
            " 147152/150000: episode: 21424, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.292653, mae: 13.849236, mean_q: 19.998463\n",
            " 147161/150000: episode: 21425, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.222 [1.000, 8.000],  loss: 0.546243, mae: 13.900065, mean_q: 20.085245\n",
            " 147170/150000: episode: 21426, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.444216, mae: 14.063087, mean_q: 20.165203\n",
            " 147177/150000: episode: 21427, duration: 0.066s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [1.000, 8.000],  loss: 0.393272, mae: 14.190193, mean_q: 19.863934\n",
            " 147179/150000: episode: 21428, duration: 0.025s, episode steps:   2, steps per second:  79, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 0.161702, mae: 13.933689, mean_q: 20.255650\n",
            " 147188/150000: episode: 21429, duration: 0.081s, episode steps:   9, steps per second: 112, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.297430, mae: 13.935453, mean_q: 19.804073\n",
            " 147196/150000: episode: 21430, duration: 0.089s, episode steps:   8, steps per second:  90, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.250 [0.000, 8.000],  loss: 0.200080, mae: 13.957275, mean_q: 19.916609\n",
            " 147203/150000: episode: 21431, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.198074, mae: 13.555674, mean_q: 19.981524\n",
            " 147208/150000: episode: 21432, duration: 0.048s, episode steps:   5, steps per second: 104, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.200 [0.000, 8.000],  loss: 0.210856, mae: 14.060654, mean_q: 20.014324\n",
            " 147216/150000: episode: 21433, duration: 0.071s, episode steps:   8, steps per second: 112, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.217949, mae: 14.156490, mean_q: 19.903999\n",
            " 147223/150000: episode: 21434, duration: 0.076s, episode steps:   7, steps per second:  92, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.674429, mae: 14.108659, mean_q: 19.913755\n",
            " 147228/150000: episode: 21435, duration: 0.047s, episode steps:   5, steps per second: 106, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.233247, mae: 13.908346, mean_q: 19.962589\n",
            " 147236/150000: episode: 21436, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.497343, mae: 13.892810, mean_q: 19.997253\n",
            " 147245/150000: episode: 21437, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.433375, mae: 13.963230, mean_q: 19.728611\n",
            " 147252/150000: episode: 21438, duration: 0.067s, episode steps:   7, steps per second: 104, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.286 [0.000, 8.000],  loss: 0.600565, mae: 13.523004, mean_q: 20.179918\n",
            " 147260/150000: episode: 21439, duration: 0.075s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.368280, mae: 13.869497, mean_q: 20.066036\n",
            " 147263/150000: episode: 21440, duration: 0.033s, episode steps:   3, steps per second:  90, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 3.333 [2.000, 4.000],  loss: 0.311636, mae: 13.717358, mean_q: 20.035379\n",
            " 147271/150000: episode: 21441, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.499478, mae: 14.176634, mean_q: 20.055271\n",
            " 147280/150000: episode: 21442, duration: 0.083s, episode steps:   9, steps per second: 108, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.299482, mae: 14.062756, mean_q: 19.946510\n",
            " 147286/150000: episode: 21443, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.333 [1.000, 6.000],  loss: 0.384282, mae: 14.040187, mean_q: 20.188128\n",
            " 147292/150000: episode: 21444, duration: 0.072s, episode steps:   6, steps per second:  83, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 2.000 [0.000, 5.000],  loss: 0.298061, mae: 13.830837, mean_q: 19.650476\n",
            " 147298/150000: episode: 21445, duration: 0.065s, episode steps:   6, steps per second:  93, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.500 [0.000, 8.000],  loss: 0.277711, mae: 14.207817, mean_q: 20.098272\n",
            " 147303/150000: episode: 21446, duration: 0.050s, episode steps:   5, steps per second: 101, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.800 [0.000, 7.000],  loss: 0.716237, mae: 13.781375, mean_q: 19.815292\n",
            " 147310/150000: episode: 21447, duration: 0.068s, episode steps:   7, steps per second: 102, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.338950, mae: 13.888680, mean_q: 19.902544\n",
            " 147317/150000: episode: 21448, duration: 0.079s, episode steps:   7, steps per second:  89, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.264016, mae: 13.712525, mean_q: 20.136013\n",
            " 147322/150000: episode: 21449, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.327745, mae: 13.605540, mean_q: 19.809223\n",
            " 147330/150000: episode: 21450, duration: 0.074s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.201051, mae: 13.686813, mean_q: 20.018461\n",
            " 147332/150000: episode: 21451, duration: 0.024s, episode steps:   2, steps per second:  84, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 0.000 [0.000, 0.000],  loss: 0.158066, mae: 13.904058, mean_q: 20.225876\n",
            " 147337/150000: episode: 21452, duration: 0.060s, episode steps:   5, steps per second:  84, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.470961, mae: 13.482577, mean_q: 19.832249\n",
            " 147344/150000: episode: 21453, duration: 0.079s, episode steps:   7, steps per second:  88, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.515843, mae: 14.112749, mean_q: 19.833204\n",
            " 147352/150000: episode: 21454, duration: 0.074s, episode steps:   8, steps per second: 109, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.228442, mae: 14.282192, mean_q: 20.114635\n",
            " 147359/150000: episode: 21455, duration: 0.075s, episode steps:   7, steps per second:  93, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.429 [0.000, 7.000],  loss: 0.275360, mae: 14.104233, mean_q: 19.731627\n",
            " 147362/150000: episode: 21456, duration: 0.034s, episode steps:   3, steps per second:  87, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 2.667 [2.000, 3.000],  loss: 0.449838, mae: 13.796249, mean_q: 20.039675\n",
            " 147370/150000: episode: 21457, duration: 0.078s, episode steps:   8, steps per second: 103, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.216061, mae: 13.905278, mean_q: 19.772671\n",
            " 147379/150000: episode: 21458, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.464144, mae: 13.888932, mean_q: 19.980337\n",
            " 147386/150000: episode: 21459, duration: 0.075s, episode steps:   7, steps per second:  93, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.273949, mae: 13.975752, mean_q: 19.929937\n",
            " 147395/150000: episode: 21460, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.291436, mae: 13.684001, mean_q: 20.036079\n",
            " 147404/150000: episode: 21461, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.216843, mae: 13.959785, mean_q: 20.004229\n",
            " 147408/150000: episode: 21462, duration: 0.052s, episode steps:   4, steps per second:  77, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 2.000 [0.000, 7.000],  loss: 0.376308, mae: 14.231279, mean_q: 19.889784\n",
            " 147414/150000: episode: 21463, duration: 0.062s, episode steps:   6, steps per second:  97, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.345004, mae: 13.738198, mean_q: 20.031801\n",
            " 147421/150000: episode: 21464, duration: 0.066s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.410681, mae: 13.618431, mean_q: 19.741144\n",
            " 147426/150000: episode: 21465, duration: 0.053s, episode steps:   5, steps per second:  95, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.800 [1.000, 7.000],  loss: 0.442183, mae: 13.536611, mean_q: 19.725819\n",
            " 147432/150000: episode: 21466, duration: 0.066s, episode steps:   6, steps per second:  91, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.000 [1.000, 8.000],  loss: 0.596512, mae: 14.152909, mean_q: 19.851847\n",
            " 147440/150000: episode: 21467, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.361808, mae: 14.012205, mean_q: 20.021200\n",
            " 147443/150000: episode: 21468, duration: 0.031s, episode steps:   3, steps per second:  96, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 5.000 [1.000, 7.000],  loss: 0.424531, mae: 13.626907, mean_q: 19.708424\n",
            " 147452/150000: episode: 21469, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.228556, mae: 14.046716, mean_q: 20.086712\n",
            " 147458/150000: episode: 21470, duration: 0.073s, episode steps:   6, steps per second:  82, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.496952, mae: 13.783703, mean_q: 20.067545\n",
            " 147463/150000: episode: 21471, duration: 0.049s, episode steps:   5, steps per second: 101, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.202541, mae: 13.780673, mean_q: 19.963144\n",
            " 147468/150000: episode: 21472, duration: 0.048s, episode steps:   5, steps per second: 104, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.800 [0.000, 5.000],  loss: 0.180963, mae: 14.045111, mean_q: 19.972538\n",
            " 147477/150000: episode: 21473, duration: 0.080s, episode steps:   9, steps per second: 112, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.198882, mae: 13.811349, mean_q: 20.111914\n",
            " 147484/150000: episode: 21474, duration: 0.080s, episode steps:   7, steps per second:  87, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [1.000, 8.000],  loss: 0.250596, mae: 13.723165, mean_q: 19.819826\n",
            " 147491/150000: episode: 21475, duration: 0.071s, episode steps:   7, steps per second:  99, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.444377, mae: 13.965360, mean_q: 19.918293\n",
            " 147499/150000: episode: 21476, duration: 0.072s, episode steps:   8, steps per second: 112, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.632666, mae: 13.974897, mean_q: 19.915508\n",
            " 147506/150000: episode: 21477, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.416312, mae: 13.753257, mean_q: 19.833435\n",
            " 147511/150000: episode: 21478, duration: 0.052s, episode steps:   5, steps per second:  95, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 8.000],  loss: 0.255709, mae: 14.037331, mean_q: 20.288013\n",
            " 147520/150000: episode: 21479, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.251152, mae: 13.949036, mean_q: 20.049494\n",
            " 147528/150000: episode: 21480, duration: 0.067s, episode steps:   8, steps per second: 120, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.418579, mae: 13.841516, mean_q: 19.979174\n",
            " 147537/150000: episode: 21481, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.486248, mae: 13.865302, mean_q: 20.062757\n",
            " 147541/150000: episode: 21482, duration: 0.041s, episode steps:   4, steps per second:  97, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 2.500 [0.000, 7.000],  loss: 0.496394, mae: 13.813167, mean_q: 20.025305\n",
            " 147550/150000: episode: 21483, duration: 0.085s, episode steps:   9, steps per second: 105, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.331293, mae: 14.207408, mean_q: 20.009193\n",
            " 147557/150000: episode: 21484, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 7.000],  loss: 0.400919, mae: 13.857901, mean_q: 19.986052\n",
            " 147563/150000: episode: 21485, duration: 0.065s, episode steps:   6, steps per second:  93, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 7.000],  loss: 0.423849, mae: 13.691298, mean_q: 19.986473\n",
            " 147570/150000: episode: 21486, duration: 0.067s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.571 [0.000, 8.000],  loss: 0.740592, mae: 13.767304, mean_q: 20.022451\n",
            " 147576/150000: episode: 21487, duration: 0.064s, episode steps:   6, steps per second:  93, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.685900, mae: 13.582592, mean_q: 19.716824\n",
            " 147583/150000: episode: 21488, duration: 0.068s, episode steps:   7, steps per second: 103, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [0.000, 8.000],  loss: 0.507868, mae: 13.785003, mean_q: 20.117899\n",
            " 147589/150000: episode: 21489, duration: 0.074s, episode steps:   6, steps per second:  81, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.500 [0.000, 6.000],  loss: 0.621501, mae: 14.109189, mean_q: 20.136080\n",
            " 147598/150000: episode: 21490, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.355803, mae: 13.677241, mean_q: 19.923962\n",
            " 147602/150000: episode: 21491, duration: 0.046s, episode steps:   4, steps per second:  87, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 1.500 [0.000, 5.000],  loss: 0.242534, mae: 14.193570, mean_q: 20.166260\n",
            " 147608/150000: episode: 21492, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.333 [1.000, 8.000],  loss: 0.279323, mae: 14.181487, mean_q: 20.017262\n",
            " 147612/150000: episode: 21493, duration: 0.054s, episode steps:   4, steps per second:  75, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.000 [2.000, 7.000],  loss: 0.671304, mae: 13.708797, mean_q: 19.921642\n",
            " 147620/150000: episode: 21494, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.125 [0.000, 8.000],  loss: 0.287656, mae: 14.112793, mean_q: 20.049082\n",
            " 147625/150000: episode: 21495, duration: 0.049s, episode steps:   5, steps per second: 103, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.666631, mae: 13.829664, mean_q: 19.808004\n",
            " 147632/150000: episode: 21496, duration: 0.068s, episode steps:   7, steps per second: 103, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 5.143 [1.000, 8.000],  loss: 0.380695, mae: 14.322174, mean_q: 19.901764\n",
            " 147639/150000: episode: 21497, duration: 0.087s, episode steps:   7, steps per second:  81, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.323440, mae: 14.093897, mean_q: 20.104916\n",
            " 147647/150000: episode: 21498, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.467299, mae: 14.010992, mean_q: 19.845352\n",
            " 147653/150000: episode: 21499, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.295636, mae: 13.634808, mean_q: 19.927969\n",
            " 147660/150000: episode: 21500, duration: 0.067s, episode steps:   7, steps per second: 104, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.538444, mae: 13.786147, mean_q: 19.992458\n",
            " 147666/150000: episode: 21501, duration: 0.069s, episode steps:   6, steps per second:  87, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.294781, mae: 13.749023, mean_q: 19.996742\n",
            " 147672/150000: episode: 21502, duration: 0.069s, episode steps:   6, steps per second:  87, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 7.000],  loss: 0.289155, mae: 13.774673, mean_q: 20.131422\n",
            " 147681/150000: episode: 21503, duration: 0.124s, episode steps:   9, steps per second:  73, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.222 [0.000, 8.000],  loss: 0.961162, mae: 13.963689, mean_q: 20.108263\n",
            " 147687/150000: episode: 21504, duration: 0.093s, episode steps:   6, steps per second:  65, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [0.000, 8.000],  loss: 0.293783, mae: 14.079940, mean_q: 20.061869\n",
            " 147693/150000: episode: 21505, duration: 0.081s, episode steps:   6, steps per second:  74, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.406386, mae: 13.957892, mean_q: 19.863504\n",
            " 147699/150000: episode: 21506, duration: 0.085s, episode steps:   6, steps per second:  70, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.000 [0.000, 7.000],  loss: 0.478971, mae: 13.673359, mean_q: 19.981543\n",
            " 147708/150000: episode: 21507, duration: 0.123s, episode steps:   9, steps per second:  73, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.293457, mae: 13.871314, mean_q: 20.140190\n",
            " 147717/150000: episode: 21508, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.328743, mae: 14.027411, mean_q: 20.014965\n",
            " 147724/150000: episode: 21509, duration: 0.097s, episode steps:   7, steps per second:  72, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.269868, mae: 14.175033, mean_q: 20.153818\n",
            " 147730/150000: episode: 21510, duration: 0.091s, episode steps:   6, steps per second:  66, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.260560, mae: 14.148388, mean_q: 20.070303\n",
            " 147737/150000: episode: 21511, duration: 0.086s, episode steps:   7, steps per second:  81, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.258469, mae: 13.974808, mean_q: 20.284044\n",
            " 147745/150000: episode: 21512, duration: 0.129s, episode steps:   8, steps per second:  62, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.804831, mae: 13.908489, mean_q: 19.945827\n",
            " 147750/150000: episode: 21513, duration: 0.064s, episode steps:   5, steps per second:  78, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.600 [2.000, 8.000],  loss: 0.274062, mae: 13.865103, mean_q: 19.931835\n",
            " 147757/150000: episode: 21514, duration: 0.086s, episode steps:   7, steps per second:  81, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.165836, mae: 14.150503, mean_q: 20.200287\n",
            " 147763/150000: episode: 21515, duration: 0.094s, episode steps:   6, steps per second:  64, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 2.833 [0.000, 6.000],  loss: 0.574716, mae: 14.121216, mean_q: 20.031527\n",
            " 147770/150000: episode: 21516, duration: 0.100s, episode steps:   7, steps per second:  70, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.219131, mae: 13.970649, mean_q: 20.013596\n",
            " 147775/150000: episode: 21517, duration: 0.067s, episode steps:   5, steps per second:  75, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.196673, mae: 13.643564, mean_q: 19.959581\n",
            " 147782/150000: episode: 21518, duration: 0.101s, episode steps:   7, steps per second:  69, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.429 [0.000, 7.000],  loss: 0.360687, mae: 13.785965, mean_q: 19.835861\n",
            " 147786/150000: episode: 21519, duration: 0.065s, episode steps:   4, steps per second:  61, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 2.500 [0.000, 7.000],  loss: 0.428735, mae: 13.521351, mean_q: 19.903366\n",
            " 147792/150000: episode: 21520, duration: 0.086s, episode steps:   6, steps per second:  70, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.303380, mae: 13.917374, mean_q: 19.851889\n",
            " 147800/150000: episode: 21521, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.588137, mae: 14.006353, mean_q: 19.872816\n",
            " 147809/150000: episode: 21522, duration: 0.112s, episode steps:   9, steps per second:  80, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.333458, mae: 14.178823, mean_q: 20.089861\n",
            " 147818/150000: episode: 21523, duration: 0.136s, episode steps:   9, steps per second:  66, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.486068, mae: 13.712856, mean_q: 20.000566\n",
            " 147824/150000: episode: 21524, duration: 0.079s, episode steps:   6, steps per second:  76, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 6.000],  loss: 0.570630, mae: 13.684028, mean_q: 19.947876\n",
            " 147829/150000: episode: 21525, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.400 [3.000, 6.000],  loss: 0.363554, mae: 13.903097, mean_q: 19.842287\n",
            " 147836/150000: episode: 21526, duration: 0.117s, episode steps:   7, steps per second:  60, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.429 [0.000, 8.000],  loss: 0.267365, mae: 14.027193, mean_q: 20.179136\n",
            " 147842/150000: episode: 21527, duration: 0.089s, episode steps:   6, steps per second:  67, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.833 [1.000, 8.000],  loss: 0.557854, mae: 13.908557, mean_q: 19.961409\n",
            " 147849/150000: episode: 21528, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.143 [0.000, 7.000],  loss: 0.395197, mae: 13.927594, mean_q: 20.035666\n",
            " 147856/150000: episode: 21529, duration: 0.116s, episode steps:   7, steps per second:  60, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.253979, mae: 13.816667, mean_q: 19.697296\n",
            " 147864/150000: episode: 21530, duration: 0.107s, episode steps:   8, steps per second:  75, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.323644, mae: 13.923679, mean_q: 19.995195\n",
            " 147868/150000: episode: 21531, duration: 0.058s, episode steps:   4, steps per second:  69, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.000 [0.000, 6.000],  loss: 0.504517, mae: 13.746826, mean_q: 19.727198\n",
            " 147873/150000: episode: 21532, duration: 0.091s, episode steps:   5, steps per second:  55, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.200 [0.000, 8.000],  loss: 0.898559, mae: 14.081167, mean_q: 20.024935\n",
            " 147881/150000: episode: 21533, duration: 0.104s, episode steps:   8, steps per second:  77, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.300012, mae: 13.945883, mean_q: 19.954899\n",
            " 147886/150000: episode: 21534, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.600 [0.000, 8.000],  loss: 0.270042, mae: 14.013858, mean_q: 20.016083\n",
            " 147892/150000: episode: 21535, duration: 0.109s, episode steps:   6, steps per second:  55, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 7.000],  loss: 0.370627, mae: 14.045856, mean_q: 20.044466\n",
            " 147901/150000: episode: 21536, duration: 0.119s, episode steps:   9, steps per second:  76, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.661160, mae: 13.961492, mean_q: 19.711601\n",
            " 147908/150000: episode: 21537, duration: 0.102s, episode steps:   7, steps per second:  69, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 2.714 [0.000, 6.000],  loss: 0.786031, mae: 13.862776, mean_q: 19.815046\n",
            " 147913/150000: episode: 21538, duration: 0.050s, episode steps:   5, steps per second:  99, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.490049, mae: 14.062429, mean_q: 20.052845\n",
            " 147922/150000: episode: 21539, duration: 0.085s, episode steps:   9, steps per second: 106, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.268022, mae: 13.884805, mean_q: 19.954887\n",
            " 147930/150000: episode: 21540, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.375 [0.000, 8.000],  loss: 0.756695, mae: 14.017252, mean_q: 19.830727\n",
            " 147934/150000: episode: 21541, duration: 0.045s, episode steps:   4, steps per second:  90, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.500 [3.000, 7.000],  loss: 0.331255, mae: 13.803911, mean_q: 19.851065\n",
            " 147940/150000: episode: 21542, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 8.000],  loss: 0.263609, mae: 14.358414, mean_q: 20.361355\n",
            " 147946/150000: episode: 21543, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [1.000, 8.000],  loss: 0.296926, mae: 13.954944, mean_q: 19.762667\n",
            " 147952/150000: episode: 21544, duration: 0.063s, episode steps:   6, steps per second:  96, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.167 [2.000, 8.000],  loss: 0.237236, mae: 14.395383, mean_q: 20.114525\n",
            " 147960/150000: episode: 21545, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.217471, mae: 13.946083, mean_q: 20.127659\n",
            " 147969/150000: episode: 21546, duration: 0.078s, episode steps:   9, steps per second: 115, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.701790, mae: 13.894732, mean_q: 19.748154\n",
            " 147976/150000: episode: 21547, duration: 0.076s, episode steps:   7, steps per second:  92, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.371374, mae: 14.050944, mean_q: 20.216816\n",
            " 147983/150000: episode: 21548, duration: 0.065s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.291611, mae: 13.947658, mean_q: 20.220282\n",
            " 147988/150000: episode: 21549, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.400 [0.000, 6.000],  loss: 0.327971, mae: 13.547129, mean_q: 19.841070\n",
            " 147994/150000: episode: 21550, duration: 0.057s, episode steps:   6, steps per second: 106, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.000 [0.000, 7.000],  loss: 0.242104, mae: 14.116193, mean_q: 19.976685\n",
            " 148001/150000: episode: 21551, duration: 0.073s, episode steps:   7, steps per second:  96, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.480669, mae: 13.666824, mean_q: 19.903881\n",
            " 148007/150000: episode: 21552, duration: 0.072s, episode steps:   6, steps per second:  83, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.000 [0.000, 7.000],  loss: 0.601210, mae: 13.914920, mean_q: 19.844852\n",
            " 148015/150000: episode: 21553, duration: 0.072s, episode steps:   8, steps per second: 112, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.281397, mae: 13.763011, mean_q: 19.935423\n",
            " 148021/150000: episode: 21554, duration: 0.054s, episode steps:   6, steps per second: 110, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [1.000, 8.000],  loss: 0.438130, mae: 13.940114, mean_q: 20.119005\n",
            " 148029/150000: episode: 21555, duration: 0.096s, episode steps:   8, steps per second:  83, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.319121, mae: 13.958540, mean_q: 19.908520\n",
            " 148035/150000: episode: 21556, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.333 [0.000, 7.000],  loss: 0.211856, mae: 14.014244, mean_q: 19.942041\n",
            " 148041/150000: episode: 21557, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.333 [0.000, 8.000],  loss: 0.326908, mae: 13.980980, mean_q: 20.036531\n",
            " 148047/150000: episode: 21558, duration: 0.056s, episode steps:   6, steps per second: 108, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 7.000],  loss: 0.321194, mae: 14.092671, mean_q: 20.043333\n",
            " 148054/150000: episode: 21559, duration: 0.079s, episode steps:   7, steps per second:  89, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 7.000],  loss: 0.227792, mae: 13.798004, mean_q: 19.868509\n",
            " 148060/150000: episode: 21560, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.167 [1.000, 8.000],  loss: 0.264460, mae: 13.772750, mean_q: 20.041071\n",
            " 148069/150000: episode: 21561, duration: 0.080s, episode steps:   9, steps per second: 112, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.111 [0.000, 8.000],  loss: 0.248313, mae: 14.034021, mean_q: 19.886908\n",
            " 148075/150000: episode: 21562, duration: 0.054s, episode steps:   6, steps per second: 111, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.167 [0.000, 8.000],  loss: 0.282176, mae: 14.090352, mean_q: 20.023798\n",
            " 148079/150000: episode: 21563, duration: 0.051s, episode steps:   4, steps per second:  79, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 5.000 [2.000, 8.000],  loss: 1.607344, mae: 13.835340, mean_q: 19.936436\n",
            " 148086/150000: episode: 21564, duration: 0.062s, episode steps:   7, steps per second: 113, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.310063, mae: 13.841594, mean_q: 19.718699\n",
            " 148094/150000: episode: 21565, duration: 0.068s, episode steps:   8, steps per second: 117, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.426127, mae: 13.776733, mean_q: 20.223869\n",
            " 148102/150000: episode: 21566, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.500 [0.000, 8.000],  loss: 0.312546, mae: 13.780441, mean_q: 19.787449\n",
            " 148109/150000: episode: 21567, duration: 0.077s, episode steps:   7, steps per second:  91, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.674227, mae: 14.298486, mean_q: 19.952433\n",
            " 148116/150000: episode: 21568, duration: 0.086s, episode steps:   7, steps per second:  81, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.399721, mae: 13.586573, mean_q: 19.937098\n",
            " 148123/150000: episode: 21569, duration: 0.080s, episode steps:   7, steps per second:  87, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.249148, mae: 13.983564, mean_q: 20.187534\n",
            " 148132/150000: episode: 21570, duration: 0.080s, episode steps:   9, steps per second: 112, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.273234, mae: 13.802813, mean_q: 20.039749\n",
            " 148141/150000: episode: 21571, duration: 0.078s, episode steps:   9, steps per second: 115, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 1.025316, mae: 13.768415, mean_q: 19.734810\n",
            " 148148/150000: episode: 21572, duration: 0.080s, episode steps:   7, steps per second:  87, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.372127, mae: 14.087779, mean_q: 20.138151\n",
            " 148155/150000: episode: 21573, duration: 0.071s, episode steps:   7, steps per second:  99, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [1.000, 8.000],  loss: 0.597533, mae: 13.771541, mean_q: 20.263136\n",
            " 148162/150000: episode: 21574, duration: 0.066s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [1.000, 8.000],  loss: 0.479289, mae: 13.913780, mean_q: 19.988247\n",
            " 148165/150000: episode: 21575, duration: 0.032s, episode steps:   3, steps per second:  93, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 3.333 [0.000, 5.000],  loss: 0.382810, mae: 13.279803, mean_q: 20.215475\n",
            " 148172/150000: episode: 21576, duration: 0.075s, episode steps:   7, steps per second:  93, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.857 [1.000, 8.000],  loss: 0.338621, mae: 13.699611, mean_q: 19.997499\n",
            " 148177/150000: episode: 21577, duration: 0.049s, episode steps:   5, steps per second: 103, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.779564, mae: 13.909910, mean_q: 19.694559\n",
            " 148185/150000: episode: 21578, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.500 [0.000, 8.000],  loss: 0.309263, mae: 13.947424, mean_q: 20.086086\n",
            " 148192/150000: episode: 21579, duration: 0.062s, episode steps:   7, steps per second: 114, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.260387, mae: 13.995893, mean_q: 20.043955\n",
            " 148199/150000: episode: 21580, duration: 0.079s, episode steps:   7, steps per second:  89, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.442517, mae: 13.730481, mean_q: 19.773571\n",
            " 148204/150000: episode: 21581, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.800 [0.000, 5.000],  loss: 0.227493, mae: 13.879428, mean_q: 20.027218\n",
            " 148213/150000: episode: 21582, duration: 0.099s, episode steps:   9, steps per second:  91, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.484665, mae: 13.762347, mean_q: 19.918074\n",
            " 148219/150000: episode: 21583, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 7.000],  loss: 0.172739, mae: 14.198054, mean_q: 19.906477\n",
            " 148227/150000: episode: 21584, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.283915, mae: 14.247942, mean_q: 19.968903\n",
            " 148229/150000: episode: 21585, duration: 0.023s, episode steps:   2, steps per second:  86, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 8.000 [8.000, 8.000],  loss: 0.115584, mae: 13.735603, mean_q: 19.758930\n",
            " 148234/150000: episode: 21586, duration: 0.045s, episode steps:   5, steps per second: 111, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.190221, mae: 14.220076, mean_q: 20.026562\n",
            " 148239/150000: episode: 21587, duration: 0.046s, episode steps:   5, steps per second: 108, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.800 [1.000, 5.000],  loss: 0.286553, mae: 13.937727, mean_q: 20.051832\n",
            " 148248/150000: episode: 21588, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.857038, mae: 13.714113, mean_q: 19.681755\n",
            " 148257/150000: episode: 21589, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.295304, mae: 14.025587, mean_q: 20.162266\n",
            " 148263/150000: episode: 21590, duration: 0.056s, episode steps:   6, steps per second: 108, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [1.000, 7.000],  loss: 0.315681, mae: 14.016369, mean_q: 19.685249\n",
            " 148269/150000: episode: 21591, duration: 0.053s, episode steps:   6, steps per second: 112, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 6.000],  loss: 0.173153, mae: 14.111026, mean_q: 20.127159\n",
            " 148276/150000: episode: 21592, duration: 0.078s, episode steps:   7, steps per second:  90, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.296444, mae: 13.921532, mean_q: 19.959431\n",
            " 148282/150000: episode: 21593, duration: 0.055s, episode steps:   6, steps per second: 110, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 8.000],  loss: 0.289050, mae: 13.951249, mean_q: 20.003149\n",
            " 148287/150000: episode: 21594, duration: 0.046s, episode steps:   5, steps per second: 109, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.800 [0.000, 8.000],  loss: 0.292118, mae: 13.721937, mean_q: 19.893860\n",
            " 148292/150000: episode: 21595, duration: 0.045s, episode steps:   5, steps per second: 111, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.476599, mae: 13.627965, mean_q: 19.913517\n",
            " 148299/150000: episode: 21596, duration: 0.073s, episode steps:   7, steps per second:  96, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.143 [0.000, 6.000],  loss: 0.530068, mae: 13.711889, mean_q: 19.730316\n",
            " 148306/150000: episode: 21597, duration: 0.072s, episode steps:   7, steps per second:  97, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.143 [0.000, 8.000],  loss: 0.173891, mae: 14.218511, mean_q: 20.250324\n",
            " 148314/150000: episode: 21598, duration: 0.081s, episode steps:   8, steps per second:  98, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 2.875 [0.000, 7.000],  loss: 0.572978, mae: 13.865906, mean_q: 19.772240\n",
            " 148322/150000: episode: 21599, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.201687, mae: 14.181333, mean_q: 19.865002\n",
            " 148328/150000: episode: 21600, duration: 0.063s, episode steps:   6, steps per second:  96, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.000 [0.000, 7.000],  loss: 0.190427, mae: 13.810704, mean_q: 19.990528\n",
            " 148337/150000: episode: 21601, duration: 0.076s, episode steps:   9, steps per second: 119, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.272690, mae: 13.895870, mean_q: 19.813698\n",
            " 148346/150000: episode: 21602, duration: 0.090s, episode steps:   9, steps per second: 100, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.403322, mae: 13.833895, mean_q: 19.799917\n",
            " 148351/150000: episode: 21603, duration: 0.047s, episode steps:   5, steps per second: 105, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.200 [0.000, 5.000],  loss: 0.185108, mae: 13.868843, mean_q: 19.909149\n",
            " 148359/150000: episode: 21604, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.267602, mae: 13.817369, mean_q: 19.830109\n",
            " 148367/150000: episode: 21605, duration: 0.071s, episode steps:   8, steps per second: 112, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.299072, mae: 14.215888, mean_q: 19.796007\n",
            " 148376/150000: episode: 21606, duration: 0.091s, episode steps:   9, steps per second:  98, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.268753, mae: 13.850110, mean_q: 19.781460\n",
            " 148384/150000: episode: 21607, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.265654, mae: 13.825345, mean_q: 19.798376\n",
            " 148392/150000: episode: 21608, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 3.875 [0.000, 8.000],  loss: 0.497969, mae: 13.756020, mean_q: 19.978090\n",
            " 148398/150000: episode: 21609, duration: 0.062s, episode steps:   6, steps per second:  97, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.285418, mae: 13.932770, mean_q: 19.864075\n",
            " 148407/150000: episode: 21610, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.362126, mae: 13.815577, mean_q: 19.875877\n",
            " 148415/150000: episode: 21611, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.489685, mae: 14.185102, mean_q: 19.997478\n",
            " 148424/150000: episode: 21612, duration: 0.104s, episode steps:   9, steps per second:  86, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 4.222 [0.000, 8.000],  loss: 1.126124, mae: 13.955709, mean_q: 19.772686\n",
            " 148428/150000: episode: 21613, duration: 0.045s, episode steps:   4, steps per second:  89, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 2.000 [1.000, 4.000],  loss: 0.551360, mae: 13.966939, mean_q: 19.717415\n",
            " 148435/150000: episode: 21614, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 5.000 [0.000, 8.000],  loss: 0.562136, mae: 13.923244, mean_q: 19.978525\n",
            " 148443/150000: episode: 21615, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.710108, mae: 14.038971, mean_q: 20.045090\n",
            " 148450/150000: episode: 21616, duration: 0.078s, episode steps:   7, steps per second:  89, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.491596, mae: 13.775867, mean_q: 19.919888\n",
            " 148456/150000: episode: 21617, duration: 0.058s, episode steps:   6, steps per second: 103, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 7.000],  loss: 0.343274, mae: 13.638546, mean_q: 20.152519\n",
            " 148462/150000: episode: 21618, duration: 0.061s, episode steps:   6, steps per second:  99, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.234018, mae: 13.618007, mean_q: 19.964979\n",
            " 148467/150000: episode: 21619, duration: 0.054s, episode steps:   5, steps per second:  93, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 8.000],  loss: 0.293138, mae: 13.803116, mean_q: 20.143764\n",
            " 148473/150000: episode: 21620, duration: 0.074s, episode steps:   6, steps per second:  82, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 5.500 [1.000, 8.000],  loss: 0.316070, mae: 13.788190, mean_q: 19.907225\n",
            " 148478/150000: episode: 21621, duration: 0.047s, episode steps:   5, steps per second: 106, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.800 [1.000, 6.000],  loss: 0.291548, mae: 13.924849, mean_q: 19.903683\n",
            " 148487/150000: episode: 21622, duration: 0.077s, episode steps:   9, steps per second: 117, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.194192, mae: 13.943102, mean_q: 20.027925\n",
            " 148492/150000: episode: 21623, duration: 0.045s, episode steps:   5, steps per second: 111, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.800 [0.000, 8.000],  loss: 0.348155, mae: 13.726339, mean_q: 19.819151\n",
            " 148500/150000: episode: 21624, duration: 0.080s, episode steps:   8, steps per second: 101, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.251216, mae: 13.666386, mean_q: 20.050617\n",
            " 148502/150000: episode: 21625, duration: 0.026s, episode steps:   2, steps per second:  76, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 2.000 [2.000, 2.000],  loss: 0.203143, mae: 13.594180, mean_q: 20.109627\n",
            " 148511/150000: episode: 21626, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.459481, mae: 14.011234, mean_q: 19.699217\n",
            " 148516/150000: episode: 21627, duration: 0.048s, episode steps:   5, steps per second: 105, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.218608, mae: 14.293730, mean_q: 19.981106\n",
            " 148522/150000: episode: 21628, duration: 0.068s, episode steps:   6, steps per second:  88, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.642678, mae: 13.709610, mean_q: 19.875351\n",
            " 148528/150000: episode: 21629, duration: 0.080s, episode steps:   6, steps per second:  75, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [0.000, 7.000],  loss: 0.212468, mae: 14.277352, mean_q: 20.099298\n",
            " 148537/150000: episode: 21630, duration: 0.081s, episode steps:   9, steps per second: 112, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.778 [0.000, 7.000],  loss: 0.301696, mae: 13.669923, mean_q: 20.035530\n",
            " 148543/150000: episode: 21631, duration: 0.067s, episode steps:   6, steps per second:  90, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.167 [0.000, 8.000],  loss: 0.771081, mae: 13.935407, mean_q: 19.689102\n",
            " 148550/150000: episode: 21632, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.000 [0.000, 8.000],  loss: 0.380383, mae: 13.906690, mean_q: 20.079302\n",
            " 148557/150000: episode: 21633, duration: 0.067s, episode steps:   7, steps per second: 105, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.296796, mae: 13.913098, mean_q: 20.084656\n",
            " 148564/150000: episode: 21634, duration: 0.067s, episode steps:   7, steps per second: 104, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 4.429 [1.000, 8.000],  loss: 0.393453, mae: 13.802019, mean_q: 19.906084\n",
            " 148573/150000: episode: 21635, duration: 0.104s, episode steps:   9, steps per second:  86, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.367024, mae: 14.008345, mean_q: 20.150755\n",
            " 148582/150000: episode: 21636, duration: 0.083s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.194225, mae: 13.956864, mean_q: 19.990593\n",
            " 148589/150000: episode: 21637, duration: 0.065s, episode steps:   7, steps per second: 107, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.216088, mae: 14.175016, mean_q: 20.165585\n",
            " 148597/150000: episode: 21638, duration: 0.092s, episode steps:   8, steps per second:  87, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.292181, mae: 13.832026, mean_q: 19.857712\n",
            " 148604/150000: episode: 21639, duration: 0.062s, episode steps:   7, steps per second: 114, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.183204, mae: 13.811277, mean_q: 20.150351\n",
            " 148610/150000: episode: 21640, duration: 0.055s, episode steps:   6, steps per second: 108, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [0.000, 7.000],  loss: 0.141364, mae: 14.138184, mean_q: 20.014099\n",
            " 148617/150000: episode: 21641, duration: 0.061s, episode steps:   7, steps per second: 114, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [1.000, 8.000],  loss: 0.351994, mae: 14.025833, mean_q: 20.039404\n",
            " 148622/150000: episode: 21642, duration: 0.060s, episode steps:   5, steps per second:  83, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.200 [0.000, 6.000],  loss: 0.311618, mae: 13.912287, mean_q: 20.008764\n",
            " 148629/150000: episode: 21643, duration: 0.080s, episode steps:   7, steps per second:  88, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.270962, mae: 13.753360, mean_q: 19.993620\n",
            " 148638/150000: episode: 21644, duration: 0.077s, episode steps:   9, steps per second: 117, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.186354, mae: 14.074045, mean_q: 20.110641\n",
            " 148645/150000: episode: 21645, duration: 0.077s, episode steps:   7, steps per second:  91, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [1.000, 8.000],  loss: 0.219435, mae: 13.784225, mean_q: 19.774137\n",
            " 148654/150000: episode: 21646, duration: 0.077s, episode steps:   9, steps per second: 116, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.349421, mae: 13.724787, mean_q: 19.788815\n",
            " 148662/150000: episode: 21647, duration: 0.074s, episode steps:   8, steps per second: 108, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.193404, mae: 14.111676, mean_q: 20.016809\n",
            " 148671/150000: episode: 21648, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.275988, mae: 13.769388, mean_q: 20.055622\n",
            " 148676/150000: episode: 21649, duration: 0.049s, episode steps:   5, steps per second: 102, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.600 [2.000, 8.000],  loss: 0.498539, mae: 14.112760, mean_q: 19.707663\n",
            " 148681/150000: episode: 21650, duration: 0.046s, episode steps:   5, steps per second: 108, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.327077, mae: 14.061356, mean_q: 20.099857\n",
            " 148688/150000: episode: 21651, duration: 0.063s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.143 [0.000, 7.000],  loss: 0.165412, mae: 13.928452, mean_q: 20.189589\n",
            " 148696/150000: episode: 21652, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.431193, mae: 13.665981, mean_q: 19.910038\n",
            " 148704/150000: episode: 21653, duration: 0.070s, episode steps:   8, steps per second: 114, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.171621, mae: 13.896284, mean_q: 19.910496\n",
            " 148710/150000: episode: 21654, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 8.000],  loss: 0.188669, mae: 13.805156, mean_q: 19.924667\n",
            " 148715/150000: episode: 21655, duration: 0.048s, episode steps:   5, steps per second: 105, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.000 [0.000, 5.000],  loss: 0.206950, mae: 14.025665, mean_q: 20.168579\n",
            " 148721/150000: episode: 21656, duration: 0.075s, episode steps:   6, steps per second:  80, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.167 [2.000, 8.000],  loss: 0.199100, mae: 13.749702, mean_q: 20.042997\n",
            " 148728/150000: episode: 21657, duration: 0.068s, episode steps:   7, steps per second: 102, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [1.000, 8.000],  loss: 0.215579, mae: 13.998650, mean_q: 19.999899\n",
            " 148733/150000: episode: 21658, duration: 0.056s, episode steps:   5, steps per second:  89, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.200 [1.000, 8.000],  loss: 0.295052, mae: 14.086166, mean_q: 20.037266\n",
            " 148739/150000: episode: 21659, duration: 0.054s, episode steps:   6, steps per second: 112, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [1.000, 8.000],  loss: 0.364043, mae: 13.888944, mean_q: 19.900175\n",
            " 148745/150000: episode: 21660, duration: 0.067s, episode steps:   6, steps per second:  89, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.667 [0.000, 7.000],  loss: 0.553925, mae: 14.078650, mean_q: 19.955584\n",
            " 148751/150000: episode: 21661, duration: 0.059s, episode steps:   6, steps per second: 101, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.833 [0.000, 8.000],  loss: 0.623924, mae: 13.973690, mean_q: 19.869198\n",
            " 148759/150000: episode: 21662, duration: 0.070s, episode steps:   8, steps per second: 114, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.318682, mae: 13.938338, mean_q: 19.974791\n",
            " 148766/150000: episode: 21663, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 7.000],  loss: 0.347295, mae: 14.216546, mean_q: 20.053644\n",
            " 148769/150000: episode: 21664, duration: 0.042s, episode steps:   3, steps per second:  72, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 2.667 [2.000, 3.000],  loss: 0.235592, mae: 13.693374, mean_q: 20.016836\n",
            " 148775/150000: episode: 21665, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 7.000],  loss: 0.229169, mae: 14.385699, mean_q: 20.099365\n",
            " 148784/150000: episode: 21666, duration: 0.080s, episode steps:   9, steps per second: 112, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.346981, mae: 13.768561, mean_q: 19.941267\n",
            " 148791/150000: episode: 21667, duration: 0.061s, episode steps:   7, steps per second: 115, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.243335, mae: 13.583072, mean_q: 20.237347\n",
            " 148799/150000: episode: 21668, duration: 0.079s, episode steps:   8, steps per second: 102, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.250 [0.000, 8.000],  loss: 0.241235, mae: 14.172267, mean_q: 19.952408\n",
            " 148808/150000: episode: 21669, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.349600, mae: 14.236008, mean_q: 20.084259\n",
            " 148815/150000: episode: 21670, duration: 0.071s, episode steps:   7, steps per second:  99, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.416128, mae: 14.260135, mean_q: 19.834991\n",
            " 148821/150000: episode: 21671, duration: 0.063s, episode steps:   6, steps per second:  95, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.667 [2.000, 7.000],  loss: 0.322530, mae: 13.817918, mean_q: 19.937292\n",
            " 148830/150000: episode: 21672, duration: 0.094s, episode steps:   9, steps per second:  95, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.421175, mae: 14.127876, mean_q: 19.891533\n",
            " 148837/150000: episode: 21673, duration: 0.076s, episode steps:   7, steps per second:  93, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.288377, mae: 13.786086, mean_q: 20.187784\n",
            " 148840/150000: episode: 21674, duration: 0.037s, episode steps:   3, steps per second:  81, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 4.000 [2.000, 8.000],  loss: 0.229427, mae: 13.650125, mean_q: 19.920500\n",
            " 148847/150000: episode: 21675, duration: 0.076s, episode steps:   7, steps per second:  92, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [0.000, 8.000],  loss: 0.603458, mae: 13.944495, mean_q: 19.726492\n",
            " 148853/150000: episode: 21676, duration: 0.056s, episode steps:   6, steps per second: 107, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.833 [0.000, 8.000],  loss: 0.341803, mae: 14.066701, mean_q: 20.073393\n",
            " 148860/150000: episode: 21677, duration: 0.063s, episode steps:   7, steps per second: 112, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.379572, mae: 13.592987, mean_q: 20.067616\n",
            " 148863/150000: episode: 21678, duration: 0.047s, episode steps:   3, steps per second:  64, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 6.333 [5.000, 7.000],  loss: 1.093369, mae: 13.123317, mean_q: 20.012024\n",
            " 148871/150000: episode: 21679, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.315522, mae: 14.086552, mean_q: 20.170464\n",
            " 148879/150000: episode: 21680, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.778386, mae: 13.737822, mean_q: 19.949329\n",
            " 148887/150000: episode: 21681, duration: 0.076s, episode steps:   8, steps per second: 106, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.750 [0.000, 8.000],  loss: 0.530361, mae: 13.951775, mean_q: 20.399948\n",
            " 148895/150000: episode: 21682, duration: 0.070s, episode steps:   8, steps per second: 114, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.396490, mae: 13.669754, mean_q: 19.700699\n",
            " 148902/150000: episode: 21683, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.286 [0.000, 8.000],  loss: 0.344541, mae: 13.737187, mean_q: 20.376654\n",
            " 148907/150000: episode: 21684, duration: 0.073s, episode steps:   5, steps per second:  68, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 2.400 [0.000, 4.000],  loss: 0.338584, mae: 13.501376, mean_q: 20.081049\n",
            " 148916/150000: episode: 21685, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.397333, mae: 13.811743, mean_q: 20.077705\n",
            " 148920/150000: episode: 21686, duration: 0.068s, episode steps:   4, steps per second:  59, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.000 [2.000, 6.000],  loss: 0.296506, mae: 14.000549, mean_q: 19.899515\n",
            " 148928/150000: episode: 21687, duration: 0.125s, episode steps:   8, steps per second:  64, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.296561, mae: 13.891430, mean_q: 20.072353\n",
            " 148934/150000: episode: 21688, duration: 0.085s, episode steps:   6, steps per second:  71, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 6.000],  loss: 0.303991, mae: 13.610927, mean_q: 19.940870\n",
            " 148941/150000: episode: 21689, duration: 0.087s, episode steps:   7, steps per second:  80, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.402462, mae: 13.891520, mean_q: 20.096079\n",
            " 148947/150000: episode: 21690, duration: 0.089s, episode steps:   6, steps per second:  67, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.775101, mae: 14.055343, mean_q: 20.078140\n",
            " 148954/150000: episode: 21691, duration: 0.089s, episode steps:   7, steps per second:  79, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.216823, mae: 13.778098, mean_q: 20.204365\n",
            " 148963/150000: episode: 21692, duration: 0.113s, episode steps:   9, steps per second:  80, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.508393, mae: 14.051720, mean_q: 19.990814\n",
            " 148971/150000: episode: 21693, duration: 0.117s, episode steps:   8, steps per second:  69, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.630281, mae: 13.714132, mean_q: 19.974726\n",
            " 148978/150000: episode: 21694, duration: 0.103s, episode steps:   7, steps per second:  68, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.143 [0.000, 8.000],  loss: 0.293437, mae: 13.882665, mean_q: 19.987371\n",
            " 148987/150000: episode: 21695, duration: 0.117s, episode steps:   9, steps per second:  77, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.425747, mae: 14.119463, mean_q: 19.916811\n",
            " 148993/150000: episode: 21696, duration: 0.098s, episode steps:   6, steps per second:  61, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.500 [0.000, 8.000],  loss: 0.359201, mae: 14.003343, mean_q: 19.761703\n",
            " 149001/150000: episode: 21697, duration: 0.124s, episode steps:   8, steps per second:  65, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.277283, mae: 13.754732, mean_q: 20.182941\n",
            " 149006/150000: episode: 21698, duration: 0.071s, episode steps:   5, steps per second:  70, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.200 [0.000, 8.000],  loss: 0.229201, mae: 14.006945, mean_q: 19.855587\n",
            " 149013/150000: episode: 21699, duration: 0.095s, episode steps:   7, steps per second:  74, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.306761, mae: 14.264975, mean_q: 20.005953\n",
            " 149017/150000: episode: 21700, duration: 0.055s, episode steps:   4, steps per second:  73, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.750 [0.000, 7.000],  loss: 0.368782, mae: 14.208358, mean_q: 20.045971\n",
            " 149022/150000: episode: 21701, duration: 0.069s, episode steps:   5, steps per second:  73, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.200 [1.000, 8.000],  loss: 0.418443, mae: 13.480217, mean_q: 19.962070\n",
            " 149028/150000: episode: 21702, duration: 0.092s, episode steps:   6, steps per second:  65, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.813201, mae: 13.793536, mean_q: 19.979248\n",
            " 149034/150000: episode: 21703, duration: 0.088s, episode steps:   6, steps per second:  68, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.333 [2.000, 8.000],  loss: 0.529409, mae: 13.919289, mean_q: 19.894648\n",
            " 149042/150000: episode: 21704, duration: 0.101s, episode steps:   8, steps per second:  79, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.376286, mae: 13.942396, mean_q: 20.011475\n",
            " 149047/150000: episode: 21705, duration: 0.085s, episode steps:   5, steps per second:  59, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.600 [1.000, 8.000],  loss: 0.241451, mae: 14.073601, mean_q: 20.277409\n",
            " 149054/150000: episode: 21706, duration: 0.103s, episode steps:   7, steps per second:  68, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.429 [0.000, 7.000],  loss: 0.241038, mae: 13.870241, mean_q: 20.050913\n",
            " 149059/150000: episode: 21707, duration: 0.070s, episode steps:   5, steps per second:  71, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 2.800 [0.000, 5.000],  loss: 0.224289, mae: 13.885885, mean_q: 20.050358\n",
            " 149064/150000: episode: 21708, duration: 0.095s, episode steps:   5, steps per second:  53, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.400 [2.000, 8.000],  loss: 0.267945, mae: 14.408144, mean_q: 19.756689\n",
            " 149071/150000: episode: 21709, duration: 0.092s, episode steps:   7, steps per second:  76, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.143 [0.000, 7.000],  loss: 0.384626, mae: 13.834830, mean_q: 20.040327\n",
            " 149079/150000: episode: 21710, duration: 0.143s, episode steps:   8, steps per second:  56, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.579371, mae: 13.895462, mean_q: 19.977310\n",
            " 149081/150000: episode: 21711, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 7.000 [7.000, 7.000],  loss: 0.384025, mae: 13.904383, mean_q: 19.949387\n",
            " 149084/150000: episode: 21712, duration: 0.046s, episode steps:   3, steps per second:  66, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 6.667 [4.000, 8.000],  loss: 0.244952, mae: 13.830005, mean_q: 20.094261\n",
            " 149089/150000: episode: 21713, duration: 0.072s, episode steps:   5, steps per second:  70, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.345285, mae: 14.057866, mean_q: 20.036249\n",
            " 149098/150000: episode: 21714, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.423262, mae: 13.763158, mean_q: 19.876150\n",
            " 149102/150000: episode: 21715, duration: 0.057s, episode steps:   4, steps per second:  70, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 2.750 [1.000, 4.000],  loss: 0.391353, mae: 14.159653, mean_q: 20.072186\n",
            " 149107/150000: episode: 21716, duration: 0.079s, episode steps:   5, steps per second:  63, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.382331, mae: 13.885608, mean_q: 19.956659\n",
            " 149114/150000: episode: 21717, duration: 0.125s, episode steps:   7, steps per second:  56, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 2.714 [0.000, 8.000],  loss: 0.207597, mae: 13.945668, mean_q: 20.104053\n",
            " 149122/150000: episode: 21718, duration: 0.075s, episode steps:   8, steps per second: 107, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.479413, mae: 13.824738, mean_q: 19.879330\n",
            " 149124/150000: episode: 21719, duration: 0.026s, episode steps:   2, steps per second:  78, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 4.000 [4.000, 4.000],  loss: 0.297250, mae: 13.826345, mean_q: 19.762665\n",
            " 149133/150000: episode: 21720, duration: 0.077s, episode steps:   9, steps per second: 118, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.373029, mae: 13.852371, mean_q: 20.076046\n",
            " 149142/150000: episode: 21721, duration: 0.089s, episode steps:   9, steps per second: 101, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.711090, mae: 13.814366, mean_q: 19.850233\n",
            " 149146/150000: episode: 21722, duration: 0.040s, episode steps:   4, steps per second: 101, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 4.750 [3.000, 8.000],  loss: 0.517973, mae: 13.970822, mean_q: 20.310652\n",
            " 149152/150000: episode: 21723, duration: 0.054s, episode steps:   6, steps per second: 110, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 7.000],  loss: 0.504036, mae: 13.621100, mean_q: 20.213949\n",
            " 149160/150000: episode: 21724, duration: 0.080s, episode steps:   8, steps per second: 100, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.282565, mae: 14.243799, mean_q: 20.106579\n",
            " 149166/150000: episode: 21725, duration: 0.074s, episode steps:   6, steps per second:  82, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.333 [0.000, 6.000],  loss: 0.269887, mae: 13.926140, mean_q: 19.945860\n",
            " 149173/150000: episode: 21726, duration: 0.063s, episode steps:   7, steps per second: 111, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.302737, mae: 14.043406, mean_q: 19.885530\n",
            " 149178/150000: episode: 21727, duration: 0.046s, episode steps:   5, steps per second: 108, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 3.000 [0.000, 8.000],  loss: 0.568786, mae: 13.752444, mean_q: 20.144409\n",
            " 149183/150000: episode: 21728, duration: 0.047s, episode steps:   5, steps per second: 107, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.200 [3.000, 8.000],  loss: 0.348494, mae: 13.626857, mean_q: 19.968979\n",
            " 149188/150000: episode: 21729, duration: 0.050s, episode steps:   5, steps per second: 100, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.600 [2.000, 8.000],  loss: 0.436345, mae: 13.861338, mean_q: 20.012552\n",
            " 149193/150000: episode: 21730, duration: 0.061s, episode steps:   5, steps per second:  81, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [2.000, 6.000],  loss: 0.202801, mae: 14.028421, mean_q: 20.218830\n",
            " 149200/150000: episode: 21731, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [1.000, 7.000],  loss: 0.358734, mae: 13.924530, mean_q: 19.946241\n",
            " 149209/150000: episode: 21732, duration: 0.078s, episode steps:   9, steps per second: 116, episode reward:  1.000, mean reward:  0.111 [ 0.000,  1.000], mean action: 4.000 [0.000, 8.000],  loss: 0.771436, mae: 13.881099, mean_q: 20.074806\n",
            " 149218/150000: episode: 21733, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.556 [0.000, 8.000],  loss: 0.306478, mae: 14.089219, mean_q: 20.126905\n",
            " 149225/150000: episode: 21734, duration: 0.072s, episode steps:   7, steps per second:  97, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.510900, mae: 13.754621, mean_q: 19.990017\n",
            " 149234/150000: episode: 21735, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.452420, mae: 14.106838, mean_q: 19.950781\n",
            " 149242/150000: episode: 21736, duration: 0.078s, episode steps:   8, steps per second: 102, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.556653, mae: 13.777395, mean_q: 20.047752\n",
            " 149247/150000: episode: 21737, duration: 0.047s, episode steps:   5, steps per second: 105, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.399839, mae: 13.574138, mean_q: 19.723888\n",
            " 149254/150000: episode: 21738, duration: 0.063s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.480440, mae: 13.746833, mean_q: 20.077646\n",
            " 149260/150000: episode: 21739, duration: 0.054s, episode steps:   6, steps per second: 110, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.352965, mae: 13.658951, mean_q: 19.856169\n",
            " 149269/150000: episode: 21740, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.380423, mae: 13.945545, mean_q: 19.995201\n",
            " 149276/150000: episode: 21741, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.286 [0.000, 8.000],  loss: 0.366302, mae: 14.005200, mean_q: 20.046860\n",
            " 149283/150000: episode: 21742, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.426085, mae: 14.043990, mean_q: 20.078125\n",
            " 149288/150000: episode: 21743, duration: 0.047s, episode steps:   5, steps per second: 107, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [0.000, 8.000],  loss: 0.357466, mae: 13.632571, mean_q: 19.979956\n",
            " 149293/150000: episode: 21744, duration: 0.057s, episode steps:   5, steps per second:  87, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [0.000, 8.000],  loss: 0.641181, mae: 13.972569, mean_q: 20.054874\n",
            " 149300/150000: episode: 21745, duration: 0.062s, episode steps:   7, steps per second: 114, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 7.000],  loss: 0.324494, mae: 14.234611, mean_q: 19.902536\n",
            " 149307/150000: episode: 21746, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.357741, mae: 13.823445, mean_q: 19.923418\n",
            " 149312/150000: episode: 21747, duration: 0.047s, episode steps:   5, steps per second: 105, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.400 [0.000, 8.000],  loss: 0.444009, mae: 13.726199, mean_q: 19.689896\n",
            " 149321/150000: episode: 21748, duration: 0.093s, episode steps:   9, steps per second:  96, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.274861, mae: 14.002972, mean_q: 20.033113\n",
            " 149327/150000: episode: 21749, duration: 0.061s, episode steps:   6, steps per second:  98, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 3.167 [0.000, 8.000],  loss: 0.189451, mae: 14.196044, mean_q: 20.038952\n",
            " 149331/150000: episode: 21750, duration: 0.039s, episode steps:   4, steps per second: 102, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.500 [1.000, 5.000],  loss: 0.257379, mae: 13.758261, mean_q: 20.007210\n",
            " 149337/150000: episode: 21751, duration: 0.056s, episode steps:   6, steps per second: 108, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 6.000],  loss: 0.299578, mae: 13.926402, mean_q: 19.991892\n",
            " 149346/150000: episode: 21752, duration: 0.098s, episode steps:   9, steps per second:  92, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.525867, mae: 13.924776, mean_q: 20.065563\n",
            " 149354/150000: episode: 21753, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.692918, mae: 14.053537, mean_q: 19.864166\n",
            " 149362/150000: episode: 21754, duration: 0.069s, episode steps:   8, steps per second: 115, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.311721, mae: 14.179516, mean_q: 20.284369\n",
            " 149367/150000: episode: 21755, duration: 0.050s, episode steps:   5, steps per second:  99, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 4.400 [2.000, 6.000],  loss: 0.166030, mae: 14.059537, mean_q: 20.033531\n",
            " 149374/150000: episode: 21756, duration: 0.080s, episode steps:   7, steps per second:  87, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.571 [0.000, 8.000],  loss: 0.245028, mae: 13.637903, mean_q: 19.803061\n",
            " 149381/150000: episode: 21757, duration: 0.066s, episode steps:   7, steps per second: 106, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.571 [0.000, 7.000],  loss: 0.234275, mae: 14.364535, mean_q: 19.954275\n",
            " 149388/150000: episode: 21758, duration: 0.063s, episode steps:   7, steps per second: 111, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.196737, mae: 13.867918, mean_q: 20.101574\n",
            " 149395/150000: episode: 21759, duration: 0.062s, episode steps:   7, steps per second: 114, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.230858, mae: 13.748338, mean_q: 20.076910\n",
            " 149401/150000: episode: 21760, duration: 0.070s, episode steps:   6, steps per second:  86, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.286778, mae: 13.965684, mean_q: 20.038683\n",
            " 149407/150000: episode: 21761, duration: 0.057s, episode steps:   6, steps per second: 105, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [1.000, 8.000],  loss: 0.206804, mae: 13.918506, mean_q: 20.042566\n",
            " 149414/150000: episode: 21762, duration: 0.062s, episode steps:   7, steps per second: 112, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 7.000],  loss: 0.310297, mae: 13.834013, mean_q: 20.012440\n",
            " 149421/150000: episode: 21763, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.286 [0.000, 6.000],  loss: 0.428138, mae: 13.985779, mean_q: 20.042982\n",
            " 149430/150000: episode: 21764, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.440661, mae: 14.103964, mean_q: 20.114426\n",
            " 149437/150000: episode: 21765, duration: 0.064s, episode steps:   7, steps per second: 109, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.429 [0.000, 6.000],  loss: 0.704973, mae: 14.058145, mean_q: 20.092653\n",
            " 149446/150000: episode: 21766, duration: 0.078s, episode steps:   9, steps per second: 116, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.279423, mae: 13.835861, mean_q: 20.098763\n",
            " 149451/150000: episode: 21767, duration: 0.048s, episode steps:   5, steps per second: 105, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.000 [2.000, 6.000],  loss: 0.255714, mae: 13.920832, mean_q: 19.924997\n",
            " 149457/150000: episode: 21768, duration: 0.068s, episode steps:   6, steps per second:  88, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.167 [0.000, 6.000],  loss: 0.294682, mae: 13.781856, mean_q: 20.070467\n",
            " 149465/150000: episode: 21769, duration: 0.076s, episode steps:   8, steps per second: 105, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.246109, mae: 13.866573, mean_q: 20.018875\n",
            " 149470/150000: episode: 21770, duration: 0.048s, episode steps:   5, steps per second: 104, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.400 [2.000, 7.000],  loss: 0.258659, mae: 14.060695, mean_q: 20.001644\n",
            " 149478/150000: episode: 21771, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.284915, mae: 13.978708, mean_q: 20.029076\n",
            " 149485/150000: episode: 21772, duration: 0.072s, episode steps:   7, steps per second:  97, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.280210, mae: 13.819009, mean_q: 19.896656\n",
            " 149493/150000: episode: 21773, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.463778, mae: 14.035032, mean_q: 20.159328\n",
            " 149499/150000: episode: 21774, duration: 0.060s, episode steps:   6, steps per second: 100, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 5.167 [2.000, 8.000],  loss: 0.689172, mae: 13.949864, mean_q: 19.981558\n",
            " 149508/150000: episode: 21775, duration: 0.095s, episode steps:   9, steps per second:  94, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.293266, mae: 13.839961, mean_q: 20.109833\n",
            " 149510/150000: episode: 21776, duration: 0.024s, episode steps:   2, steps per second:  82, episode reward: -10.000, mean reward: -5.000 [-10.000,  0.000], mean action: 6.000 [6.000, 6.000],  loss: 0.293583, mae: 13.322137, mean_q: 20.139219\n",
            " 149515/150000: episode: 21777, duration: 0.051s, episode steps:   5, steps per second:  99, episode reward: -10.000, mean reward: -2.000 [-10.000,  0.000], mean action: 5.200 [2.000, 7.000],  loss: 0.601034, mae: 13.678436, mean_q: 19.686441\n",
            " 149522/150000: episode: 21778, duration: 0.085s, episode steps:   7, steps per second:  83, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.714 [0.000, 8.000],  loss: 0.229420, mae: 13.739288, mean_q: 19.939157\n",
            " 149526/150000: episode: 21779, duration: 0.053s, episode steps:   4, steps per second:  76, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 2.750 [0.000, 8.000],  loss: 0.296060, mae: 14.110747, mean_q: 20.061481\n",
            " 149530/150000: episode: 21780, duration: 0.040s, episode steps:   4, steps per second: 101, episode reward: -10.000, mean reward: -2.500 [-10.000,  0.000], mean action: 3.000 [1.000, 4.000],  loss: 0.237949, mae: 13.888235, mean_q: 20.086119\n",
            " 149536/150000: episode: 21781, duration: 0.055s, episode steps:   6, steps per second: 110, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [1.000, 7.000],  loss: 0.530830, mae: 13.862346, mean_q: 19.888071\n",
            " 149545/150000: episode: 21782, duration: 0.096s, episode steps:   9, steps per second:  94, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.215740, mae: 14.289227, mean_q: 20.164112\n",
            " 149553/150000: episode: 21783, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 4.625 [0.000, 8.000],  loss: 0.237057, mae: 14.049970, mean_q: 20.065056\n",
            " 149560/150000: episode: 21784, duration: 0.072s, episode steps:   7, steps per second:  97, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.358653, mae: 13.942223, mean_q: 19.934109\n",
            " 149569/150000: episode: 21785, duration: 0.095s, episode steps:   9, steps per second:  94, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.216450, mae: 13.843632, mean_q: 20.086935\n",
            " 149576/150000: episode: 21786, duration: 0.069s, episode steps:   7, steps per second: 102, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.336032, mae: 13.758304, mean_q: 20.048523\n",
            " 149584/150000: episode: 21787, duration: 0.073s, episode steps:   8, steps per second: 110, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.162370, mae: 13.887234, mean_q: 19.976379\n",
            " 149591/150000: episode: 21788, duration: 0.076s, episode steps:   7, steps per second:  92, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.333203, mae: 13.924173, mean_q: 19.859022\n",
            " 149598/150000: episode: 21789, duration: 0.069s, episode steps:   7, steps per second: 101, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.571 [0.000, 8.000],  loss: 0.286685, mae: 13.770210, mean_q: 19.924786\n",
            " 149606/150000: episode: 21790, duration: 0.069s, episode steps:   8, steps per second: 115, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.442995, mae: 14.088762, mean_q: 20.223537\n",
            " 149613/150000: episode: 21791, duration: 0.060s, episode steps:   7, steps per second: 116, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.470075, mae: 13.877604, mean_q: 19.871525\n",
            " 149616/150000: episode: 21792, duration: 0.049s, episode steps:   3, steps per second:  62, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 6.333 [5.000, 7.000],  loss: 0.270653, mae: 14.069514, mean_q: 20.437632\n",
            " 149625/150000: episode: 21793, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.242774, mae: 14.127445, mean_q: 20.051384\n",
            " 149632/150000: episode: 21794, duration: 0.060s, episode steps:   7, steps per second: 117, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.857 [0.000, 8.000],  loss: 0.340003, mae: 13.642836, mean_q: 20.043283\n",
            " 149641/150000: episode: 21795, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.513165, mae: 13.927650, mean_q: 19.923611\n",
            " 149650/150000: episode: 21796, duration: 0.082s, episode steps:   9, steps per second: 109, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.205513, mae: 14.048580, mean_q: 20.128147\n",
            " 149658/150000: episode: 21797, duration: 0.070s, episode steps:   8, steps per second: 115, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.483057, mae: 13.906275, mean_q: 19.886295\n",
            " 149663/150000: episode: 21798, duration: 0.046s, episode steps:   5, steps per second: 109, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 5.000 [2.000, 8.000],  loss: 0.748084, mae: 14.157927, mean_q: 19.823017\n",
            " 149671/150000: episode: 21799, duration: 0.084s, episode steps:   8, steps per second:  95, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.405292, mae: 13.941908, mean_q: 19.828323\n",
            " 149676/150000: episode: 21800, duration: 0.053s, episode steps:   5, steps per second:  94, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.526129, mae: 13.694409, mean_q: 19.827431\n",
            " 149685/150000: episode: 21801, duration: 0.081s, episode steps:   9, steps per second: 111, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.348475, mae: 14.017790, mean_q: 20.234795\n",
            " 149692/150000: episode: 21802, duration: 0.062s, episode steps:   7, steps per second: 113, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 8.000],  loss: 0.378037, mae: 13.907438, mean_q: 19.786987\n",
            " 149699/150000: episode: 21803, duration: 0.071s, episode steps:   7, steps per second:  99, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [0.000, 8.000],  loss: 0.343370, mae: 14.511897, mean_q: 20.113134\n",
            " 149708/150000: episode: 21804, duration: 0.076s, episode steps:   9, steps per second: 119, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.338735, mae: 13.960123, mean_q: 19.870441\n",
            " 149715/150000: episode: 21805, duration: 0.061s, episode steps:   7, steps per second: 115, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [1.000, 8.000],  loss: 0.644037, mae: 13.769982, mean_q: 20.010532\n",
            " 149724/150000: episode: 21806, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.293263, mae: 14.021992, mean_q: 19.906002\n",
            " 149729/150000: episode: 21807, duration: 0.055s, episode steps:   5, steps per second:  91, episode reward: 20.000, mean reward:  4.000 [ 0.000, 20.000], mean action: 4.800 [2.000, 8.000],  loss: 0.397173, mae: 14.170070, mean_q: 20.516293\n",
            " 149737/150000: episode: 21808, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.625 [0.000, 8.000],  loss: 0.424219, mae: 13.524282, mean_q: 19.888041\n",
            " 149745/150000: episode: 21809, duration: 0.070s, episode steps:   8, steps per second: 114, episode reward: -10.000, mean reward: -1.250 [-10.000,  0.000], mean action: 5.000 [0.000, 8.000],  loss: 0.251959, mae: 13.617832, mean_q: 20.251970\n",
            " 149752/150000: episode: 21810, duration: 0.082s, episode steps:   7, steps per second:  85, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.301674, mae: 13.915109, mean_q: 19.877825\n",
            " 149760/150000: episode: 21811, duration: 0.071s, episode steps:   8, steps per second: 113, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.304547, mae: 14.015868, mean_q: 19.943600\n",
            " 149767/150000: episode: 21812, duration: 0.063s, episode steps:   7, steps per second: 111, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.571 [1.000, 8.000],  loss: 0.675836, mae: 13.661421, mean_q: 19.903530\n",
            " 149774/150000: episode: 21813, duration: 0.062s, episode steps:   7, steps per second: 112, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.297831, mae: 13.643094, mean_q: 19.842953\n",
            " 149777/150000: episode: 21814, duration: 0.049s, episode steps:   3, steps per second:  61, episode reward: -10.000, mean reward: -3.333 [-10.000,  0.000], mean action: 1.667 [1.000, 3.000],  loss: 0.478993, mae: 14.206044, mean_q: 20.067417\n",
            " 149783/150000: episode: 21815, duration: 0.068s, episode steps:   6, steps per second:  88, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.374001, mae: 13.910434, mean_q: 19.970116\n",
            " 149790/150000: episode: 21816, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.605977, mae: 13.835366, mean_q: 19.907322\n",
            " 149796/150000: episode: 21817, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [1.000, 7.000],  loss: 0.729170, mae: 13.697234, mean_q: 19.731199\n",
            " 149802/150000: episode: 21818, duration: 0.065s, episode steps:   6, steps per second:  92, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.667 [0.000, 8.000],  loss: 0.322556, mae: 13.843651, mean_q: 20.078917\n",
            " 149808/150000: episode: 21819, duration: 0.058s, episode steps:   6, steps per second: 104, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.333 [1.000, 8.000],  loss: 0.327726, mae: 13.953345, mean_q: 20.113741\n",
            " 149814/150000: episode: 21820, duration: 0.053s, episode steps:   6, steps per second: 112, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.376854, mae: 13.830304, mean_q: 20.092932\n",
            " 149821/150000: episode: 21821, duration: 0.062s, episode steps:   7, steps per second: 113, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 4.429 [0.000, 8.000],  loss: 0.551353, mae: 13.938474, mean_q: 19.965540\n",
            " 149829/150000: episode: 21822, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.211386, mae: 13.876699, mean_q: 20.198235\n",
            " 149838/150000: episode: 21823, duration: 0.079s, episode steps:   9, steps per second: 114, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.293956, mae: 13.825078, mean_q: 19.859285\n",
            " 149845/150000: episode: 21824, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 7.000],  loss: 0.439914, mae: 13.990769, mean_q: 20.009333\n",
            " 149853/150000: episode: 21825, duration: 0.082s, episode steps:   8, steps per second:  97, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.375 [0.000, 8.000],  loss: 0.409505, mae: 13.919130, mean_q: 19.967928\n",
            " 149862/150000: episode: 21826, duration: 0.086s, episode steps:   9, steps per second: 105, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.189249, mae: 13.828131, mean_q: 20.060661\n",
            " 149869/150000: episode: 21827, duration: 0.070s, episode steps:   7, steps per second: 101, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.714 [1.000, 7.000],  loss: 0.367426, mae: 13.752635, mean_q: 19.927675\n",
            " 149877/150000: episode: 21828, duration: 0.086s, episode steps:   8, steps per second:  93, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.125 [0.000, 8.000],  loss: 0.407726, mae: 13.916147, mean_q: 19.980223\n",
            " 149884/150000: episode: 21829, duration: 0.064s, episode steps:   7, steps per second: 110, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 7.000],  loss: 0.282119, mae: 13.691749, mean_q: 19.978176\n",
            " 149890/150000: episode: 21830, duration: 0.065s, episode steps:   6, steps per second:  92, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 4.167 [0.000, 7.000],  loss: 0.222839, mae: 14.154864, mean_q: 20.142351\n",
            " 149896/150000: episode: 21831, duration: 0.054s, episode steps:   6, steps per second: 112, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.000 [0.000, 7.000],  loss: 0.190305, mae: 14.120725, mean_q: 19.971794\n",
            " 149902/150000: episode: 21832, duration: 0.068s, episode steps:   6, steps per second:  88, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [0.000, 8.000],  loss: 0.264311, mae: 14.091796, mean_q: 19.825060\n",
            " 149909/150000: episode: 21833, duration: 0.065s, episode steps:   7, steps per second: 108, episode reward: -10.000, mean reward: -1.429 [-10.000,  0.000], mean action: 3.714 [0.000, 8.000],  loss: 0.440103, mae: 13.803230, mean_q: 19.890560\n",
            " 149917/150000: episode: 21834, duration: 0.070s, episode steps:   8, steps per second: 114, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 4.500 [1.000, 8.000],  loss: 0.507585, mae: 13.783613, mean_q: 19.638088\n",
            " 149925/150000: episode: 21835, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward: 20.000, mean reward:  2.500 [ 0.000, 20.000], mean action: 3.875 [0.000, 8.000],  loss: 0.461754, mae: 13.817984, mean_q: 20.173830\n",
            " 149931/150000: episode: 21836, duration: 0.065s, episode steps:   6, steps per second:  92, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.333 [1.000, 8.000],  loss: 0.352299, mae: 13.803126, mean_q: 19.689838\n",
            " 149937/150000: episode: 21837, duration: 0.054s, episode steps:   6, steps per second: 112, episode reward: -10.000, mean reward: -1.667 [-10.000,  0.000], mean action: 2.333 [0.000, 5.000],  loss: 0.707141, mae: 13.702865, mean_q: 19.941437\n",
            " 149946/150000: episode: 21838, duration: 0.077s, episode steps:   9, steps per second: 116, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.443644, mae: 13.916472, mean_q: 20.074699\n",
            " 149952/150000: episode: 21839, duration: 0.068s, episode steps:   6, steps per second:  88, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 4.667 [0.000, 8.000],  loss: 0.388455, mae: 14.094579, mean_q: 20.080465\n",
            " 149961/150000: episode: 21840, duration: 0.081s, episode steps:   9, steps per second: 112, episode reward: -10.000, mean reward: -1.111 [-10.000,  0.000], mean action: 3.667 [0.000, 8.000],  loss: 0.278012, mae: 13.865129, mean_q: 20.056099\n",
            " 149968/150000: episode: 21841, duration: 0.070s, episode steps:   7, steps per second: 101, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.714 [0.000, 7.000],  loss: 0.156369, mae: 14.211779, mean_q: 20.023779\n",
            " 149977/150000: episode: 21842, duration: 0.094s, episode steps:   9, steps per second:  96, episode reward: 20.000, mean reward:  2.222 [ 0.000, 20.000], mean action: 4.000 [0.000, 8.000],  loss: 0.291390, mae: 13.973444, mean_q: 19.892834\n",
            " 149983/150000: episode: 21843, duration: 0.062s, episode steps:   6, steps per second:  96, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.500 [0.000, 7.000],  loss: 0.215661, mae: 13.794436, mean_q: 19.933350\n",
            " 149989/150000: episode: 21844, duration: 0.055s, episode steps:   6, steps per second: 109, episode reward: 20.000, mean reward:  3.333 [ 0.000, 20.000], mean action: 3.000 [0.000, 6.000],  loss: 0.153545, mae: 14.000253, mean_q: 20.072130\n",
            " 149996/150000: episode: 21845, duration: 0.071s, episode steps:   7, steps per second:  98, episode reward: 20.000, mean reward:  2.857 [ 0.000, 20.000], mean action: 3.571 [0.000, 8.000],  loss: 0.210459, mae: 14.062631, mean_q: 20.012314\n",
            "done, took 1643.080 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7bdc1d9db5b0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testar o agente\n",
        "#dqn.test(env, nb_episodes=3, visualize=True)"
      ],
      "metadata": {
        "id": "59zvW5JL43Yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model.save('my_model_1')\n",
        "# Carregamento dos pesos do modelo (não suporta mais load_model)\n",
        "model.load_weights( \"/content/drive/MyDrive/mod\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXT39el71CPQ",
        "outputId": "62230b03-75ad-4b97-e1e7-aaf98a3bc08c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x78f1972244c0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para jogar contra o modelo DQN treinado\n",
        "def play_game(model):\n",
        "    env = TicTacToeEnv()\n",
        "\n",
        "    # Usando o modelo carregado diretamente\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Jogador humano (Jogador 1)\n",
        "          # Jogada do agente (Jogador 2) usando o modelo treinado\n",
        "        print(\"AI play/Jogada da IA:\")\n",
        "        state_flat = state.reshape(1, 1, 9)  # Agora o estado tem 3 dimensões: (1, 1, 9)\n",
        "        action = np.argmax(model.predict(state_flat))  # Usando o modelo diretamente para escolher a ação\n",
        "        print(f\"AI chose the position / IA escolheu a posição {action}\")\n",
        "        state, reward, done, info = env.step(action)\n",
        "        if done:\n",
        "            env.render()\n",
        "            print(\"AI win / IA ganhou!\" if reward == 10 else \"Draw/Empate!\")\n",
        "            break\n",
        "        env.render()\n",
        "\n",
        "        action = int(input(\"Choose your move / Escolha sua jogada (0-8): \"))  # Jogada do jogador humano\n",
        "        if state[action] != 0:\n",
        "            print(\"Jogada inválida! Tente novamente.\")\n",
        "            continue\n",
        "        state, reward, done, info = env.step(action)\n",
        "        if done:\n",
        "            env.render()\n",
        "            print(\"you win / Você ganhou!\" if reward == 10 else \"Draw / Empate!\")\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "# Supondo que o modelo já esteja treinado e você tenha o modelo disponível\n",
        "# Chame a função passando o modelo já carregado:\n",
        "play_game(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "id": "BStyg_CY9o3F",
        "outputId": "31f80d95-7d65-4382-daeb-f55a0c79b8d5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI play/Jogada da IA:\n",
            "AI chose the position / IA escolheu a posição 0\n",
            "1 0 0\n",
            "0 0 0\n",
            "0 0 0\n",
            "\n",
            "Choose your move / Escolha sua jogada (0-8): 3\n",
            "AI play/Jogada da IA:\n",
            "AI chose the position / IA escolheu a posição 4\n",
            "1 0 0\n",
            "2 1 0\n",
            "0 0 0\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-41991f5fff67>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Supondo que o modelo já esteja treinado e você tenha o modelo disponível\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Chame a função passando o modelo já carregado:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-41991f5fff67>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Choose your move / Escolha sua jogada (0-8): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Jogada do jogador humano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Jogada inválida! Tente novamente.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutilr\n",
        "shutil.make_archive('modelo_1', 'zip', '/content/my_model_1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "TQrph-Ls2Uxe",
        "outputId": "6fa14d1a-494c-4d64-ca8f-7f4a9033f358"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/minha_pasta.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}