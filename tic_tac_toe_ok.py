# -*- coding: utf-8 -*-
"""Tic Tac toe ok.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fl2cwdneBfN2oTpW3mmdxefktWYhcAOo
"""

!pip install keras
!pip install keras-rl2
!pip install tensorflow==2.14.0

from keras.models import Sequential
from keras.layers import Dense, Flatten


from rl.agents.dqn import DQNAgent
from rl.policy import EpsGreedyQPolicy
from rl.memory import SequentialMemory
from tensorflow.keras.optimizers.legacy import Adam

import numpy as np
import random
from gym import Env, spaces

class TicTacToeEnv(Env):
    def __init__(self):
        super().__init__()
        self.action_space = spaces.Discrete(9)  # 9 células do tabuleiro
        self.observation_space = spaces.Box(0, 2, (9,), dtype=int)  # Tabuleiro 3x3 achatado com algumas configurações legais, recomendo a documentação
        self.reset()

    def reset(self):
        self.board = np.zeros(9, dtype=int)  # Tabuleiro 1D (array)
        self.done = False
        self.current_player = 1
        return self.board

    def step(self, action):
        if self.done or self.board[action] != 0:
            return self.board, -10, True, {}  # Penalidade por jogada inválida

        self.board[action] = self.current_player # vai na posição escolhina e coloca o número
        if self.check_winner(self.current_player):
            return self.board, 10, True, {}  # Vitória
        if not (self.board == 0).any():
            return self.board, 1, True, {}  # Empate

        # Alternando o jogador
        self.current_player = 3 - self.current_player #alterna entre 1 e 2
        return self.board, 0, False, {}

    def render(self, mode='human'):
        board = self.board.reshape(3, 3)
        print("\n".join([" ".join(map(str, row)) for row in board]))
        print()

    def check_winner(self, player):
        board = self.board.reshape(3, 3)
        return any(
            all(board[row, :] == player) or
            all(board[:, col] == player) or
            all(np.diag(board) == player) or
            all(np.diag(np.fliplr(board)) == player)
            for row in range(3) for col in range(3)
        )

# Criar o ambiente
env = TicTacToeEnv()

# Testando o agente em 10 episódios
for episode in range(10):
    state = env.reset()  # Reiniciar o ambiente a cada episódio
    done = False
    while not done:
        action = env.action_space.sample()  # Ação aleatória para o jogador
        next_state, reward, done, info = env.step(action)  # Passo no ambiente
        env.render()  # Mostrar estado
        if done:
            print(f"Fim do episódio {episode + 1}")

# Construção do modelo
def build_model(input_shape, action_space):
    model = Sequential([
        Flatten(input_shape=(1,) + input_shape),  # Entrada achatada, para o ESP, necessário corrigir isso
        Dense(64, activation='relu'),
        Dense(64, activation='relu'),
        Dense(action_space, activation='linear')  # Saída Q(s, a)
    ])
    return model

model = build_model(env.observation_space.shape, env.action_space.n)
model.summary()

# Configurar memória e política
memory = SequentialMemory(limit=50000, window_length=1)
policy = EpsGreedyQPolicy()

# Criar o agente DQN
dqn = DQNAgent(model=model, nb_actions=env.action_space.n, memory=memory,
               nb_steps_warmup=10, target_model_update=1e-2, policy=policy)
dqn.compile(optimizer=Adam(), metrics=['mae'])

# Treinar o agente
dqn.fit(env, nb_steps=150000, visualize=False, verbose=2)

# Testar o agente
#dqn.test(env, nb_episodes=3, visualize=True)

#model.save('my_model_1')
# Carregamento dos pesos do modelo (não suporta mais load_model)
model.load_weights( "/content/drive/MyDrive/mod")

# Função para jogar contra o modelo DQN treinado
def play_game(model):
    env = TicTacToeEnv()

    # Usando o modelo carregado diretamente
    state = env.reset()
    done = False

    while not done:
        # Jogador humano (Jogador 1)
          # Jogada do agente (Jogador 2) usando o modelo treinado
        print("AI play/Jogada da IA:")
        state_flat = state.reshape(1, 1, 9)  # Agora o estado tem 3 dimensões: (1, 1, 9)
        action = np.argmax(model.predict(state_flat))  # Usando o modelo diretamente para escolher a ação
        print(f"AI chose the position / IA escolheu a posição {action}")
        state, reward, done, info = env.step(action)
        if done:
            env.render()
            print("AI win / IA ganhou!" if reward == 10 else "Draw/Empate!")
            break
        env.render()

        action = int(input("Choose your move / Escolha sua jogada (0-8): "))  # Jogada do jogador humano
        if state[action] != 0:
            print("Jogada inválida! Tente novamente.")
            continue
        state, reward, done, info = env.step(action)
        if done:
            env.render()
            print("you win / Você ganhou!" if reward == 10 else "Draw / Empate!")
            break



# Supondo que o modelo já esteja treinado e você tenha o modelo disponível
# Chame a função passando o modelo já carregado:
play_game(model)

import shutilr
shutil.make_archive('modelo_1', 'zip', '/content/my_model_1')